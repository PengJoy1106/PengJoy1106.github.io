[{"title":"立夏｜当时只道是寻常","url":"/2023/05/06/立夏：一夜雨声凉到梦/","content":"\n新年快乐。\n\n此刻窗外是无尽的夜，我在生活，且还要将生活继续下去。\n\n我拥有的唯一支撑就是尽力让自己清醒地明白一切都是短暂的。无论亲人、朋友、恋人、师长，总有离开的一天，与人的情谊总会在某一天戛然而止，甚至没有任何征兆。而孤独是生活的赠礼，我愈发爱我的自由。我一直都不是一个那么快乐的人，不过还好，我也不太需要。\n  \n有人说，亲人的离世不是一场暴雨，而是一生的潮湿。我想我还是不够成熟，曾经很多次，我以为我能够接受的。在这几个月里，我一直在想我们可能更需要一些死亡教育，也许人没有必要执着于活得那么久，见过黄昏和大海，放肆地大笑过，在生命最好的年岁里，勇敢地去爱想要爱的人。人生好短啊，但是足够了。我终于接受了一些遗憾和局限，生活里的偶然其实很庞大，我们向前走，就这样向前走。\n\n在我爷爷最后的那段时间里，看着这个行将就木的老人，我感到莫大的绝望。我不知道怎么减轻他身体的痛苦，不知道怎么愈合他溃烂的皮肤，更不知道怎么慰藉他临终之前腐朽的灵魂。也许，他早就已经失了魂了。\n\n那些个夜晚，他不停地在哀嚎，这也许是他唯一能做的与死神的抗争了。我不曾想到这刺耳的哀嚎声竟会让我觉得心安，以至于他走后的很长一段时间，我总能听到这种声音。这声音一停下我立马心跳加快，像是接受最终审判的罪犯。我不再相信「一切都会好的」，我不停地告诉自己「明天总会来的」。明天，总会来的。\n\n我终于还是崩溃了。有天晚上，我失声痛哭，那一刻我希望死神能立马把我带走，我无法接受破壁残垣的现状，无法接受衰老和病痛对陪伴了我这么多年的人的折磨，如果可以，我愿意用我十年的寿命换他临终之前没有伤痛，但我做不到。这种无能为力让痛苦变成了愤怒，我开始恨自己的无能为力，我发了疯般地砸烂板凳，然后蜷缩在地上，企图通过这种方式释放自己的情绪。终究是徒劳的。我很希望眼前的这个老人能理解我的痛苦，但他不知道，就像他已经认不出我是谁了一样，他已经足够痛苦了。\n\n那段时间我坐在他床边费力地咀嚼林棹的《潮汐图》，如同看不懂这本书一样，我也无法理解生活的苦难以及所有歌颂苦难的人。这本书讲述的是生命与尊严的故事，我几乎每读一段话都要查阅资料去理解陌生的粤语词汇和大量的民俗，并强迫自己适应作者叙述时诡异的节奏变化。\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20230506165142.png)\n故事的主人公，那只雌性巨蛙说「我活过的世界都死尽了。」\n我不想讨论理解生活的苦难这件事，我根本无法理解每个人的苦难或是每种生活的苦难，因为那不是两三件事情，而是无数个条件叠加重合的产物。至于人生意义、生老病死这种问题，还是顺其自然的好。\n\n我再次重温了那个看了不下五遍的电影《一一》，长的就像是整个人生。我的内心还住着好奇的简洋洋，但其实已经告别了躁动的简婷婷，然后变成书读的多，多思且内心敏感的N J，上有老，下有小，事业平平，没啥动力，在生活中磨噬掉最后的主体性。我觉得我也老了。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20230506164756.png)\n我想起2018年的夏天，那时候我也坐在他旁边看书，而他总是望着天边残存的夕阳。那时候我写下「我开始敬佩那些能够并且愿意活到七八十岁甚至更久的人 光是活着就已经足够吃力 至于何时会离开 我都不再觉得奇怪」。那年他已经七十八岁，也许那时他就明白日薄西山，但和现在的我相比，却显得那么有活力，仿佛从来没有失去过对生的渴望和希望。\n一切都像是安排好的，爷爷生前喜闹，于是选择在除夕这天离开我们，在一声声鞭炮声中他的身体彻底冰冷，我才理解死亡是多么残酷却又无解的现实。那些万家灯火啊。\n\n「赌书消得泼茶香，当时只道是寻常。」\n我丧失了些许关于曾经的美好记忆，在某一刻，我终于开始释怀过去和现在，开始认真感受自我毁灭的过程，我知道即使把悲伤写成悲剧，也无法增加苦难的美感，无法美化那些痛苦的、不完美的、有孔隙的。泪没有流尽又干掉，为了什么结果呢？\n\n就把人生当成一场游戏吧，不管是沉浸其中，还是冷眼旁观，不管是某个思想的载体，还是某个理念的开拓者，不管是有所作为，还是有所不为，死亡都会一直忠心耿耿的守在最终。世界上所有的东西都是漂浮着的，只有死亡是着地的​。\n\n立夏已经到了，夏天也快要来了吧。你知道你的人生还有很多个夏天，所以不必在记忆的长河里刻舟求剑。新年快乐，在夏天来临之前准备好开始新的生命。","tags":["随笔"],"categories":["像梦一场"]},{"title":"Paper Reading 9|Sea surface target detection based on complex ARMA-GARCH processes","url":"/2022/09/26/文献阅读9/","content":"\n# 摘要\n\n在金融应用中，常用带有广义自回归条件异方差(GARCH)误差的自回归移动平均(ARMA)模型拟合收益序列。在本文中，我们开发了一个复值ARMA-GARCH模型用于海杂波建模应用。与AR-GARCH模型相比，新引入的MA项使模型能够将相邻回波测量值的条件方差依赖性作为模型系数，利用相邻测量值之间的强相关性提高了建模精度。基于海杂波建模的复值ARMA-GARCH过程，进一步发展了一种海面目标检测算法。通过对大量实际海杂波数据的分析，我们对其性能进行了评估，结果表明，与最先进的AR-GARCH探测器相比，所提出的海面目标探测器的探测概率有明显提高。\n\n# 引言\n\n海杂波的精确建模在遥感和雷达应用中具有重要意义，有利于优化探测算法设计和性能预测[1-9]。为了全面研究不同环境条件下实际杂波的统计特性，开展了各种实验，并向国际研究界提供了数据库，如麦克马斯特智能像元处理雷达(IPIX)数据库[10]和科学与工业研究理事会(CSIR)数据库[11]。然而，有时统计描述杂波振幅和功率所需的数据要么无法获得，要么缺乏各种海况、掠射角、极化和风向的数据。另外，计算机仿真程序提供了一个廉价可靠的环境来获取各种频段的合成杂波数据[12,13]。\n\n对于低掠射角，海杂波一般具有威布尔[14]、K[15]、对数正态[16]、帕累托[17]、复合高斯(CG)[18]概率密度函数(pdf)等特征。对于大掠射角海杂波[19]，Pareto分布也是一个很好的模型。然而，当海杂波与目标的模型不符合实际复杂环境[20]时，传统的基于统计的检测方法无法获得预期的检测结果。此外，这些传统分布不能精确地模拟返回序列中可能存在的时间依赖性。\n\n 该产品模型描述了雷达系统相干处理区间(CPI)量级的观测时间间隔的散射机理。因此，在雷达应用中，CG模型被广泛用于表征重尾杂波分布，特别是海杂波建模[22]。CG过程的织构控制着复合过程的方差，通常被假设遵循一定的分布，例如Gamma[23]，逆Gamma[23,24]，逆高斯[25]。然而，在实际应用中，很难确定哪种纹理分布是最优的。\n\n引入广义自回归条件异方差(GARCH)过程对海杂波进行建模，为将杂波回波振幅作为时间序列建模提供了一种新的方法。在这里，“异方差”这个术语意味着方差不是恒定的，而是时变的。在[28]中，作者提出了一个复杂的非线性ARCH模型和相应的检测器，与线性GARCH检测器相比，该检测器实现了更高的检测概率。然而，线性和非线性garch过程都不能考虑相邻杂波回波之间的强相关性。因此，为了有定量的概念，有必要首先估计数据自相关函数的时间自相关函数[27]的平均值。然后，根据估计的自相关函数，可以预测分离一定区间的两个脉冲的样本是不相关的。例如，如果经过六个脉冲后，自相关函数近似为零，那么我们从六个连续的样本中提取一个样本。这种数据提取方法明显效率低下，浪费了大部分雷达资源。因此，研究了一系列GARCH型过程用于海杂波建模，并在[29]中提出了一个与自回归(AR)过程相结合的广义非线性-非对称GARCH模型。由于AR工艺的引入，可以有效地缩短提取间隔时间。\n\n值得注意的是，GARCH过程也是一个产品模型，正如在下一节中观察到的那样。因此，它自然继承了CG模型的优点。GARCH过程与CG过程的主要区别在于GARCH过程的条件方差是时变的，依赖于历史信息。GARCH过程的两个主要特征是lep- tokurity和波动性聚类(即，大变化往往在低大变化之后，小变化往往在小变化之后)，这在海杂波测量中得到了准确的显示。历史信息被用来改进模型当前的描述和未来的预测。虽然相邻杂波测量之间的相关性很强，但被时间间隔隔开的两个脉冲的样本变得不相关。这意味着在使用GARCH过程(或非线性GARCH过程)进行杂波建模时，需要首先分析杂波的自相关函数来确定适当的时间区间。显然，当雷达停留时间较短时，该方法效率较低，甚至无法使用，导致数据长度不足，无法进行精确的参数估计。\n\n随后，Pascual等人[30]提出了一种用于海杂波建模的复杂二维自回归GARCH (AR-GARCH-2D)过程。通过引入AR项，将相邻测量值的相关性建模为条件均值的系数。然而，由于条件方差是由包括杂波测量在内的过去信息组成的，因此不仅杂波测量结果是依赖的，而且它们的条件方差也是依赖的。因此，在模型的条件均值表达式中加入移动平均(MA)项，以获得对实际海杂波更精确的建模是自然合理的。新的杂波模型具有两个优点:1)不再需要数据提取，大大减少了所需的雷达停留时间;2)实际杂波的条件均值建模更精确，条件方差更小，这与检测器的决策阈值密切相关，如4.3.1节所示。\n\n本文通过实际海杂波数据，证明了本文提出的ARMA-GARCH探测器与AR-GARCH探测器相比，在探测概率上有明显提高。值得一提的是，我们主要讨论了一维情况，以阐述在条件均值表达式中添加MA项的优点。换句话说，脉冲维(慢时间维)杂波被建模为一个复杂的ARMA-GARCH过程。事实上，ARMA-GARCH过程和相应的检测算法可以通过在当前的条件方差表达式中加入距离维(快速时间维)测量值和距离维的条件方差，方便地扩展到二维情况。同时，参数估计所需的数据量也增加了。此外，本文使用的雷达数据的距离分辨率为30 m，导致距离维数系数可以忽略不计。因此，二维和一维过程，以及相应的探测器，本质上是相同的。这一观点在第4节的数值模拟中得到了证明。由此推断，随着雷达分辨率的提高，二维探测器可能获得更好的性能。\n\n本文的其余部分组织如下。第二节简要介绍了复杂的一维和二维ARMA-GARCH过程，以及参数估计算法和模型顺序选择准则。基于海杂波的一维模型，在第三节中提出了一种二元假设检验检测算法。在第4节中，进行了数值模拟来验证所提出的检测器的优点。\n\n# 基于复杂ARMA-GARCH过程的新型杂波模型\n\n## 一种新的杂波模型\n\n## 参数估计\n\n由于只有α0和α1两个系数的GARCH过程通常足以捕获海杂波[27]的大部分统计行为，我们将条件方差部分的模型阶数限制为(1,1)。此外，我们将AR和MA项的最大阶数限制为2，这能够建模相邻杂波样本之间的相关性，如下文所示。本文认为，传统的信息准则，如赤池信息准则(AIC)[33]和贝叶斯信息准则(BIC)[34]，不能用于条件异方差模型[35]的阶数确定。本文利用基于Kullback-Leibler信息准则[36]估计的AIC和BIC准则的修正版本。\n\n# 基于新杂波模型的检测算法\n\n基于上述新的杂波模型，我们开发了一种判断雷达回波是无目标还是有目标的检测算法。对于一个给定的范围单元，N个复样本从慢时间维{yt}tN 1可以组装成一个n维向量y =[y1 y2··=yN]T。\n\n在虚假设条件下(H0),假设杂乱的数据只能由c。在备择假设下(H1),相反,它是假定的数据由c x和杂波信号的总和。目标信号向量x是建模为x =β,β是未知的目标复振幅,s是已知的复杂转向向量的元素是由圣= exp (j2πfd Tst), fd是目标多普勒频率和Ts雷达脉冲重复间隔(PRI)。\n\n# 数值结果\n\n## 实际的雷达数据\n\n为了在实际环境中评估所提议的探测器的性能，我们获得了麦克马斯特IPIX雷达[10]在1993年冬天收集的杂波测量。IPIX雷达是一种双偏振高分辨率x波段雷达，工作频率为9.39 GHz。IPIX数据库的详细分析可在[22]和[37]中找到。特别地，从“starea”和“stareC0000”数据集中选取14组数据来评估所提出的检测器的拟合优度和性能。图1显示了以17号数据文件为例的VV线偏振通道的大小。缩写VV表示垂直发射偏振和垂直接收偏振。从图1(b)可以清楚地观察到波动率的聚类。\n\n表1给出了不同型号阶次的HAIC和HBIC结果。可以看出，当p = 2和q = 2时，对应的ARMA(2,2)-GARCH(1,1)模型达到最低的HAIC和HBIC。乍一看，增加模型阶数似乎可以提高建模精度和检测性能。然而，在实际应用中，这有时是不现实的，因为高阶模型需要更大的训练数据量，这受雷达停留时间和PRF的影响。此外，参数估计较费时。这是在实际应用中需要考虑的权衡，特别是在实时雷达任务中。\n\n## 估计精度与数据长度的关系\n\n首先，我们进行了仿真，证明二维ARMA-GARCH模型在应用于IPIX雷达数据时，与一维模型相比没有优势。我们最初将二维ARMA-GARCH模型的条件方差限制为仅依赖于相邻脉冲维数和距离维数\n\n其原因有两方面:1)条件方差系数随m1、m2、n1、n2阶的增加而急剧增加，严重阻碍了过程系数的准确估计;此外，在实际场景中，即使杂波数据集足够大，可以对高阶模型进行精确估计，估计过程也更加耗时，可能会影响所提检测器的实时性。2)表2给出了三组实际杂波数据的二维ARMA-GARCH过程的QMLE结果。当m1 = m2 = n1 = n2 = 1时，除α01外，α10、β01和β10的估计系数几乎都小于0.1，说明电流条件方差主要受相邻脉冲维数回归的影响。因此，本文主要研究一维ARMA-GARCH模型。然后，我们通过一个仿真表明，增加n1并不能提高建模精度。我们随机选择几组数据(#19，#25，#26和#280,IPIX文件，第一个范围单元，VV极化，数据长度为5000)，并使用ARMA(2,2)-GARCH(2,1)模型估计系数。估计结果如表3所示。可以看出，α口令口令的口令要么为0，要么非常小(小于0.01)，这意味着杂波回波分离了多个脉冲，几乎不会影响电流条件方差的性质。因此，我们将GARCH部分的阶数限制为(1,1)。\n\n与GARCH和AR-GARCH模型相比，ARMA-GARCH模型相对复杂。因此，有必要分析估计误差与数据长度的关系，以指导实际杂波数据的估计质量。\n\n预先设定的参数近似等于17号杂波数据集的QMLE。对于每个数据长度，我们重复估计过程200次。参数的初始值随机选取，满足第2.1节介绍的约束条件。表4显示了作为数据长度函数的估计的平均值和标准差值。可以看出，估计的平均值具有较小的偏差，该偏差随着估计过程中使用的样本数量的增加而减小。此外，估计的标准差随着样本数量的增加而减小，这表明QMLE类似于渐近无偏一致估计的行为。\n\n## 性能比较\n\n### 拟合优度\n\n### 检测性能指标\n\n# 结论\n\n本文将海杂波建模为具有GARCH误差的复杂ARMA过程。该模型精确捕获了实际杂波数据在不同极化下的重尾pdf，拟合优度可与AR-GARCH和CG过程相媲美。同时考虑雷达回波测量的相关性及其条件方差的相关性作为模型系数，从而对实际杂波进行更精确的相关建模。结合SQP算法和Hessian矩阵的BFGS更新方法，推导了ARMA-GARCH过程系数的QMLE过程，并通过数值模拟研究了估计量的渐近性质。为了保证qmle收敛于真值，推导了模型系数的约束条件。此外，在考虑异方差特性的情况下，采用改进的AIC和BIC来确定合适的模型阶数，这是复杂性和拟合效果之间的权衡。\n\n此外，我们通过几个例子说明ARMA-GARCH过程的二维扩展在应用于IPIX雷达数据时没有优势。距离维系数α10和β10的估计结果表明，当前条件方差与邻近距离维测量之间的相关性，以及当前条件方差与邻近距离维条件方差之间的相关性均无统计学意义。\n\n我们还提出了一种基于杂波模型的检测算法。由于引入了MA项，ARMA-GARCH模型的条件方差比AR-GARCH模型的条件方差小得多，这在检测中是必不可少的。利用合成的和实际的杂波数据，对该检测器的性能进行了评价，并与AR-GARCH检测器和高斯检测器进行了比较。如预期的那样，模拟ROC曲线显示，在虚警概率相同的情况下，所提检测器的检测概率远远优于其他两种检测器，AR-GARCH检测器在大多数情况下是第二好的检测器。原因可以解释如下。与AR-GARCH检测器相比，ARMA-GARCH过程更准确地模拟了时间相关性，从而减少了条件方差，从而降低了决策阈值，提高了检测概率。与高斯检测器相比，AR-GARCH和ARMA-GARCH检测器的阈值是时变的，依赖于先验信息，可以自适应调整。值得注意的是，在HH偏振下，AR-GARCH检测器比高斯检测器表现出更明显的改进。因为HH偏振数据通常比VV偏振数据要高得多。杂波的PDF具有较重的尾部，此时杂波的高斯假设不再有效，garch型模型的优势更加明显。\n\n在实际的雷达应用中，需要考虑探测器的实时性。可以看出，与GARCH、AR-GARCH和一些传统的杂波模型相比，所提出的复合ARMA-GARCH模型具有更多的系数。因此，系数的估计计算量较大，但需要在海况不剧烈变化时经常更新系数值。\n\ngarch型探测器的性能可以通过以下两种方式进一步提高。首先，GARCH模型解释了波动率的平滑变化，但不能解释杂波中罕见的大离散峰值。因此，引入泊松分布(经济学文献中又称“跳跃过程”[41-43])来控制时间区间内事件的数量是一种很有前途的方法。跳跃概率要么是常数，要么随时间变化。其次，利用多元GARCH模型[44]，将模型扩展到极化信息。\n\n ","tags":["surface object detection","Radar"],"categories":["文献阅读"]},{"title":"Paper Reading 8|Sea surface target detection and recognition algorithm based on local and global salient region detection","url":"/2022/09/26/文献阅读8/","content":"\n# 摘要\n\n提出了一种新的显著区域检测算法，用于在抖动视场中对海上船舶进行检测和识别。基于这种情况，本文采用的解决方法是分别检测每一帧的显著区域，而不是使用跟踪算法，显著区域检测基于局部和全局对比。结果表明，该方法能够有效地抑制干扰，准确地检测出目标的形状。证明本文算法是一种高效的船舶检测目标检测算法。\n\n# 引言\n\n随着科学技术的出现，对人类视觉系统的研究表明，人类的视觉系统总是能够将注意力集中在最显著的物体上，这就是所谓的视觉选择性注意。有关区域就是所谓的突出对象。视觉注意机制可以帮助人们将注意力锁定在最突出的区域。该机制可以消除来自背景的干扰。借助显著性检测，机器视觉可以高效地完成图像分割[1-2]、目标识别[3]、目标检索[4]和目标跟踪。\n\n# 显著性检测的主要方法\n\n在显著性检测领域，1998年，Itti[7]基于Treisman[5-6]的整合理论和Koch的生物学架构，提出了经典的视觉注意模型。所谓Itti模型在实际测量中得到了接近人类视觉感知的结果。2006年，Jonathan Hare在图论的基础上提出了GBVS[8]算法，它遵循Itti模型的结构。该结构包含三个步骤，分别是特征提取、中心周边竞争和重组。此外，本文还利用马尔可夫链计算了中心-周边竞争的相互作用。2007年，Hou和Zhang提出了基于自然图像[9]的傅里叶变换频谱分析的残差谱模型。这是第一个在频域上处理的显著性检测模型。2008年，Achanta提出了基于多尺度周边区域对比的交流算法[10]。2009年，Achanta提出了一种频率调谐算法[11]。通过不同高斯带通滤波器与均值的叠加得到全分辨率显著图。2013年，Imamoglu[12]提出了一种基于小波变换的底层特征显著性检测模型。该模型利用小波变换建立多尺度特征图进行局部检测。计算出的显著图可以突出突出区域，但分辨率较低。\n\n由于研究现状和针对海面目标的检测情况。本文提出了一种基于空间域局部检测与全局检测融合的快速显著性检测算法，以获得更好的海面检测情况下的检测效果。该算法还应具有较高的执行效率。\n\n# 方法\n\n## 局部检测\n\n直观意义上的突出物是指它们不同于其周边地区所具有的某些特征。本文基于[10]理论，提取每个像素与其周围像素的差值。显然，当它小于过滤器大小时，对象将被完全检测到。另一方面，较小的过滤器尺寸只能部分检测到目标。较小的物体可以被较小的滤波器很好地检测到，也可以被所有的滤波器检测到。大的物体只能用大的滤波器来检测。因此，为了得到综合的结果，我们将最终显著图定义为三个显著图的加权平均值。\n\n[10]中的卷积核是一个均值滤波器的核，显著图中像素的值是周围像素的平均值。但是，这个内核有两个缺点。首先，当物体较大时，物体的中心区域像素都是显著的物体，并且彼此相似。他们的歧视是非常小的。它会在突出物体的中心形成一个“洞”。第二，在背景中物体的边界附近会有一个“晕”，因为物体的像素在计算时也包含在内核中。光晕在显著图中是一个虚假的显著区域。\n\n在[10]中，计算在Lab颜色空间中。事实上，在其他颜色空间中还有更多的特征。本文将图像转换为Lab颜色空间和HSV颜色空间。然后将图像分割成单个通道。将0到255的范围归一化，以观察每个通道的性能。通道如图2所示。\n\n在对大量的海面图像进行验证后，总结出以下结论。因为“a”代表红色和绿色之间的位置。在真实的海面场景中，大海、天空和船舶涂层的位置过于接近，难以区分。“b”空间表示黄色和蓝色之间的位置。然而，大海、天空的色调和船的涂层是近似蓝色的。在b空间的差异仍然很小。“H”空间代表色调。事实上，“a”和“b”空间的相似性相当于“H”空间的相似性。对于其他空间，“L”空间表示颜色的明度。“S”空间表示颜色的饱和度。“V”空间表示亮度。这三个空间的区别是明显的。因此，本文从这三个空间计算显著性映射。\n\n然后用式(2)中定义的卷积核对L、S、V通道进行滤波，根据实际情况，目标船舶的尺寸会相对较小。因此本文采用不同大小核的显著图的加权平均。由于目标尺寸相对较小，内核较小的显著图可以显示更多的细节，\n\n## 全局检测\n\n相对于局部检测，基于心理学研究的全局检测。[13]推断，人类的感知系统对颜色、强度和纹理等视觉信号的对比非常敏感。在此基础上，提出了一种利用图像颜色统计量计算空间显著性图的有效方法。[13]中的算法的计算复杂度与图像像素的数量成线性关系。图像的显著性图是建立在图像像素之间的颜色对比之上的。算法定义为\n\n突出的物体与周围地区截然不同。但是在数值变化平缓的区域会有一个相对较高的显著区域，如图4所示。本文将全局显著图视为基于一定特征的单通道图像。通过局部检测的方法进行过滤。结果如图4所示。\n\n## 局部检测与全局检测的融合\n\n[14]和[15]表明，神经网络或任何相关领域的研究都不能证明在颜色空间中存在线性依赖性。因此，本文不考虑色彩空间的相互作用。目前，大多数权重都是通过机器学习或经验来定义的。提出了一种具有特征独立性和特征连接性的特征融合方法。该方法保留了各特征间的独立性，提取了特征间的线性关系同时组合影响。\n\n在融合显著图后，显著区域应该是地图中最突出显示的区域。然而，背景中的一些区域也可能被突出显示，但不是很亮。压制背景区域，强调真正的突出区域。由于sigmoid函数可以模拟神经细胞的非线性脉冲，本文提出了一种利用sigmoid函数曲线的变体进行校正的曲线。\n\n## 实验结果和分析\n\n本文最后对几种代表性的显著性检测算法进行了对比实验。选择的方法有AC、FT、HC和LC。AC、HC、LC是基于空间域的。FT是基于频域的。AC为局部检测，其他为全局检测。它们易于实现，得到了广泛的应用。\n\n图10中的图像是通过不同算法得到的不同输入图像的显著性图，所有算法都是在c++中用OpenCV 2.4.10实现的。开发环境是Windows 7上的Visual Studio 2013。计算机的主频为3.20GHz，内存大小为8GB。\n\n如图6所示，本文在高速下比其他四种算法都能得到更好的显著性图。640×480分辨率图像的平均处理时间约为150ms。\n\n## 主成分分析\n\n为了得到目标(船)的方向，将从显著图中经过形态学处理提取出船的轮廓。考虑到不可能只有一个目标，需要对每个区域进行分析，计算方向。\n\n主成分分析是统计学中的一种降维方法，其目的是提取最显著的元素和结构。同时，还能减少噪声和冗余。更简单的结构将被揭示。实际上，它是通过线性分式变换将数据变换成一个新的坐标系。投影数据在第一个轴(第一个主成分)的方差最大，在另一个轴(第二个主成分)的方差第二大。\n\n对于目标区域内的像素点，PCA的目标是将维数从2降至1。计算出的一维将具有最大的方差(方向)。首先，求出矩阵X的协方差矩阵。X是一个零均值归一化矩阵[17]。C的特征向量对应于最大的特征值也就是第一主成分。\n\n# 结论\n\n提出了一种新的局部和全局显著性检测相结合的显著性检测算法。本文定义了一种新的卷积核，克服了原有卷积核的缺点。本文还对不同通道在Lab和HSV颜色空间中的性能进行了检验。根据每个信道的性能选择合适的信道和融合方法。最后，本文设计了一种新的融合方法，将局部和全局显著图融合在一起，突出重叠部分，抑制其他部分。同时，在不使用GPU的情况下，处理时间约为150ms, FPS可达7。该算法可以是一种基于多线程或GPU的海面检测实时检测跟踪算法。\n\n","tags":["surface object detection","target tracking"],"categories":["文献阅读"]},{"title":"Paper Reading 7|An Image-Based Benchmark Dataset and a Novel Object Detector for Water Surface Object Detection","url":"/2022/09/26/文献阅读7/","content":"\n# 摘要\n\n水面物体检测是自动驾驶和水面视觉应用的重要课题之一。迄今为止，从网站收集的现有公共大规模数据集并不关注具体的场景。作为这些数据集的一个特点，图像和实例的数量也仍然处于较低的水平。为加快水面自动驾驶技术的发展，本文提出了一个大规模、高质量的标注基准数据集——水面物体检测数据集(WSODD)，用于对不同的水面物体检测算法进行基准测试。该数据集包含7467张不同水环境、气候条件和拍摄时间的水面图像。此外，该数据集包括14个公共对象类别和21,911个实例。同时，在WSODD中还关注更具体的场景。为了找到在WSODD上提供良好性能的简单的体系结构，提出了一种名为CRB-Net的新的对象检测器作为基线。在实验中，CRB-Net与16种最先进的物体检测方法进行了比较，在检测精度上都优于它们。在本文中，我们进一步讨论了数据集多样性(例如，实例大小、光照条件)、训练集大小和数据集细节(例如，分类方法)的影响。跨数据集验证表明，WSODD的性能明显优于其他相关数据集，CRB-Net具有良好的适应性。\n\n# 引言\n\n水面物体检测在无人驾驶水面车辆(usv)和水面视觉应用等自动驾驶领域发挥着越来越重要的作用。为了更准确地检测视觉对象，使用带注释的基准数据集(Everingham等人，2010)来验证不同的对象检测方法，可以避免建立自己的数据集的耗时过程。根据不同的对象检测方法，可以基于相同的标注基准数据集给出有说服力的性能比较。然而，关注水面物体检测应用的基于图像的数据集缺乏。此外，目前的水面数据集还存在一些缺陷。例如，船型识别数据集(Clorichel, 2018)中存在的主要问题是数据规模小，水面物体类别数量有限，而且只有一种气候类型。此外，对于大型的基于图像的通用数据集，如MS COCO (Lin et al.， 2014)、ImageNet (Krizhevsky et al.， 2012)和Places 2 (Zhou et al.， 2015)，水面视觉对象的图像来自网站，没有足够的图像来训练不同的神经网络。因此，当水面探测器对这些类型的数据集进行训练时，性能是一个问题。为了解决这些问题，有必要建立一个新的水面数据集，包含广泛的水环境、完整的常见障碍类别、多种气候条件和不同的拍摄时间。\n\n本文提出了一个新的基准数据集WSODD，它具有更多的实例和类别，用于水面常见障碍的检测。它由7467张由海康威视工业相机拍摄的水面图像组成，每张图像的分辨率是1920 × 1080。WSODD中包含了各种各样的环境，如海洋、湖泊和河流。WSODD中的图像是在三种不同的拍摄时段(白天、黄昏和夜晚)和三种不同的气候条件(晴天、多云和雾天)下获得的。在提议的全注释数据集中，有14个类别和21,911个实例，每个实例都用轴对称边界框标记。所有的注释和原始图片都将公开，并将建立一个在线基准。\n\n为了深入研究WSODD，建议将CRB-Net作为基线。水面物体检测数据集(WSODD)包含许多小的物体以及不容易被检测到的物体，因此检测器提取更深层次的语义特征，并使用SPP (He et al.， 2015)来增强接受野。虽然融合了跨尺度的特征，但大多数以前的结构只是简单的堆叠输入，没有区别。然而，这些特征具有不同的分辨率，它们对融合特征的贡献往往是不相等的。为了解决这一问题，我们引入了一种改进的BIFPN (Tan等人，2020)，它可以在特征融合过程中通过注意机制和Mish激活进行自适应权重调整(Misra, 2019)。此外，CRB-Net基于K-means算法优化锚帧初值，使锚帧符合障碍物的形状特征。本文的主要贡献是：\n\n(1) 水面物体检测数据集是一种基于图像的新型水面物体检测基准数据集，具有最多的常见障碍类别、最广泛的水环境和天气条件。WSODD中的图像能更准确地反映真实的视觉对象。\n\n(2) 提出了一种新的目标检测方法(CRB-Net)，并与16种最先进的目标检测方法进行了性能比较。结果表明，CRB-Net在检测精度方面优于其他方法。此外，我们还探索了各种探测器对WSODD中不同大小物体的检测性能。\n\n(3) 选择船型识别数据集进行跨数据集泛化，因为该数据集是唯一公开的基于图像的水面数据集。结果表明，WSODD比船型识别具有更多的模式和属性，CRB-Net具有较好的泛化能力。\n\n除了推进水面视觉中的物体检测研究外，WSODD还将提出机器视觉领域中值得探索的方法的新问题。\n\n# 相关工作\n\n## 数据集\n\n目前，用于水面物体检测的数据集并不多。船型识别数据集是该领域唯一的基于公共图像的数据集。它包含1462幅水面图像，有三类常见的物体:船(贡多拉、充气船、皮划艇、纸船、帆船)、船(游轮、渡船、货船)和浮标。虽然该数据集的水环境和拍摄时间非常丰富，但在数据集中没有提供对象检测的注释。\n\n基于图像的通用数据集也可用于水面检测。例如，MS COCO是一个大型通用数据集，包括91类对象，总共328,000张图像。然而，与水面检测相关的只有一个类别(船)，包含3146张图像。显然，该数据集中的障碍和图像数量不足以保证深度学习神经网络的有效训练。另一个名为ImageNet的数据集提供了大规模的注解，但与水面物体检测相关的类别只有四种:双体船、三体船、集装箱船和航空母舰，这些图像与真实的水面情况有很大的差异。此外，Places2是一个泛型数据集，包含365个类别，但只有5个类别与水面有关，分别是港口、湖泊、装卸区、水和河流。一般来说，由于缺乏水面障碍物，这些图像大多无法用于水面目标检测任务。表1显示了WSODD与其他基于图像的WSODD的比较。\n\n此外，也有一些基于视频的wsodd，如新加坡-海事数据集(Prasad et al.， 2017)、MODD数据集(Kristan et al.， 2016)和Visual-Inertial-Canoe数据集(Miller et al.， 2018)，但大多数都存在障碍类别少、环境相对简单的问题，难以实现较好的目标检测性能。\n\n## 方法\n\n众所周知，对于早期的通用对象检测方法[例如，LBP (Ojala等人，2002)，DPM (Felzenszwalb等人，2010)]，很难从图像中提取特征。此外，目标检测的精度和速度也不能令人满意。2012年以后，随着深度学习的发展，出现了许多基于cnn的高效检测器，主要分为两类:两阶段目标检测方法和一级目标检测方法(Liang et al.， 2020)。最著名的两阶段目标检测方法是R-CNN (Girshick等人，2014)系列[例如，Faster R-CNN (Ren等人，2016)，Mask R-CNN (He等人，2017)和Cascade R-CNN (Cai和Vasconcelos, 2018)]。在单阶段目标检测方法方面，Yolo (Redmon and Farhadi, 2018)和SSD (Liu et al.， 2016)是最显著的方法。此外，一级检测器还可以转化为CenterNet等无锚检测器(Duan et al.， 2019)。\n\n水面目标检测作为计算机视觉的重要组成部分，受到了广泛关注。在深度学习方法出现之前，小波变换与图像形态学相结合的方法(Yang et al.， 2004)是实现水面目标检测的主导方法。提出了一种基于HOG (Dalal and Triggs, 2005)的目标检测系统(Wijnhoven et al.， 2010)，用于在海上视频中寻找船舶。Matsumoto (Matsumoto, 2013)提出了一种HOG-SVM方法，用于在舰船摄像机图像上检测舰船。Kaido等在2016年将支持向量机和边缘检测用于舰船的检测。此外，通过使用两个摄像头和对通过港口的各种船舶的识别，提出了船舶号牌识别(Kaido等人，2016年)。\n\n深度学习技术极大地推动了该领域的发展。由于尺寸、外观和干扰的变化，无监督方法(Liu et al.， 2014)受到严重限制。因此，使用监督方法更为常见(Mizuho等人，2021年)。Yang (Yang et al.， 2017)提出了一种使用Fast R-CNN实现船舶识别和分类的体系结构。此外，提出了一种集成了深度学习方法的混合舰船检测方法(Yao et al.， 2017)。具体来说，他们利用深度神经网络(DNNs)和区域提议网络(RPNs)获取目标船只的2D边界框。此外，设计了一种基于ResNet的地表物体快速检测方法(Chae et al.， 2017)，目标检测速度可达32.4帧/秒(FPS)。此外，Qin (Qin and Zhang, 2018)采用FCN进行表面障碍检测，具有良好的鲁棒性。2019年，提出了一种改进的基于rbox的水面目标检测框架(an et al.， 2019)，以获得检测的精度召回率和精度。Sr等人提出了一种利用改进的YOLO和多特征舰船检测方法对舰船进行检测的舰船算法。该方法通过MDS(多维尺度)对SIFT特征进行降维，并使用RANSAC(随机样本一致性)优化SIFT特征匹配，有效消除不匹配(Sr等，2019)。此外，提出了一种基于改进的Faster R-CNN的实时水面物体检测方法(Zhang et al.， 2019)，该方法包括两个模块，将低层特征与高层特征相结合，提高检测精度。利用该方法对北京北运河的水面浮子进行了3天的视频监控，验证了该方法的有效性。此外，采用深度残差网络和跨层跳转连接策略(Liu et al.， 2019)提取先进的船舶特征，有助于提高目标识别性能。2020年，基于yolov2 (Chen et al.， 2020b)提出了一种检测小船的方法，该方法还可以用于识别水面上的各种障碍物。H-Yolo (Tang et al.， 2020)提出了基于兴趣区域预选网络的船舶检测。该方法的原理是根据船舶与背景之间的色相、饱和度、值(HSV)差异从图像中区分可疑区域。然后，提出了一种名为Yolov3-2SMA的水面检测方法(Li et al.， 2020)，允许在动态水中环境中实时、高精度的物体检测。此外，Jie等人(2021)改进了yolov3用于检测内河水道中的船舶，改进方法的mAP和FPS分别提高了约5%和2%。最近，造船厂(Han et al.， 2021)被引入来解决小型船舶的漏检问题。该算法设计了一种新的扩展卷积和最大池化放大接受场模块，提高了模型对空间信息的获取能力和目标空间位移的鲁棒性。然而，上述大多数方法都是基于用于港口管理的静态摄像机的自主船舶上不可行，因此与移动自主船舶上的舰载监视系统不匹配(Jie等人，2021年)。此外，即使所有提出的算法都存在效率和准确性方面的缺陷。\n\n# 水面物体检测的基准数据集\n\n大多数研究人员认为，一个数据集应该涵盖尽可能多的真实图像，在注释过程中尽可能少的个人偏见。与以往的数据集相比，本文提出的数据集包含了更多的实例、类别、环境、拍摄时间和天气条件。\n\n## 图像采集\n\nWSODD中的所有图像都是由一台工业4G高清相机在2020年7月16日至9月10日期间拍摄的。温度范围为20-35◦C。\n\n为了丰富环境，尽可能准确地反映现实世界，我们选择了五个水域，包括三种类型的水面环境。这些是渤海(中国辽宁省的大连;海洋)，黄海(中国山东省烟台市;海洋)，玄武湖(江苏省南京市，中国;南海子湖(北京，中国;湖泊)和长江(中国江苏省南京市;河)。\n\n为了丰富气候类别，每一个水环境都在不同的天气条件下拍摄，如阳光、云层和雾。\n\n同时，在不同的光照条件下，包括正午(强光)、黄昏(弱光)和晚上(极弱光)对障碍物进行拍摄，以便为数据集收集足够的研究材料。\n\n图1显示了WSODD的一些典型环境。可见，这些图像不仅显示了大量的水面障碍物信息，还包含了周边海域、陆地和港口的相关信息，更接近于实际的水面物体检测应用(Kristan et al.， 2014)。\n\n## 选择的类别\n\n选取水面物体检测数据集，对水面上常见的14个物体进行标注，分别是船、船、球、桥、岩石、人、垃圾、桅杆、浮标、平台、港口、树、草、动物。图2显示了每个类别的两个图像。\n\n选择对象的核心标准是其在真实水环境中的共性。水面物体检测数据集的对象类别划分比较广泛。例如，船舶类别包括大型军舰和客轮;与此同时，其他研究人员可以直接基于这个数据集测试方法，或者对现有的类别进行更详细的分类。表2列出了WSODD的每个类别的映像和实例的数量。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220926105301.png)\n\n## 图像标注\n\n水面物体检测数据集有两种注释方式，与PASCAL VOC (Everingham等人，2010)和MS COCO (Lin等人，2014)相同。注释文件以XML格式保存。\n\n考虑到很多研究者基于COCO格式注释文件进行实验，我们将提供将VOC文件转换为COCO文件的代码。当其他研究人员想要使用COCO格式注释文件时，他们可以使用此代码轻松地转换格式。\n\n值得注意的是，这项研究关注于注解水面数据集，而不包括陆地对象。所有的注释，包括省略的对象，都由工程师检查，以确保更详细的注释。\n\n## 数据集的统计数据\n\n不同水面环境的统计结果如图3所示。海洋环境图像1771张，湖泊环境图像4114张，河流环境图像1581张，分别占WSODD的24%、55%和21% (Zhang et al.， 2020)。需要注意的是，WSODD中的船舶类别只包括海洋和河流，因为玄武湖和南海子湖是小湖，不能容纳大型船舶。平台类只存在于海中。在拍摄过程中，我们发现在近海水域有很多这样的平台，用于海洋养殖和海水质量检测，但在河流和湖泊中没有发现这样的物体。相反，草的分类只存在于河流和湖泊中，而不存在于海中，那里从未发现过大面积的草。一个可能的原因是海浪的冲击会破坏草的生长。\n\n图4描述了不同气候条件下的图像数量。大部分图像(4918张，占WSODD的66%)是在晴天拍摄的，而最少的图像(589张，占数据集的8%)是在雾天拍摄的。\n\n不同拍摄时间的数据如图5所示。大部分照片，6354张，约占总数的85%，是在白天收集的。此外，每张照片平均在白天拍摄3.15个实例。黄昏时的每张图像都拍摄了类似数量的实例，为3.24个(Alessandro et al.， 2018)。然而，每张在夜间拍摄的照片的平均实例数是1.19。造成这种巨大差异的主要原因有两个。一是在夜间继续移动的物体数量很少，比如船只。二是夜间光线太暗，很多已有的物体无法被发现，特别是距离拍摄地点较远的物体或较小的物体。\n\n一些实例可能只有图像的0.01%，而另一些实例可能超过40%。实例之间的显著差异使检测任务更加具有挑战性，因为模型必须足够灵活，以便处理极小和极大的对象(Li et al.， 2018)。图6描述了映像中实例规模的统计信息。\n\n# 水面物体的新型探测器\n\n为了构建WSODD的基线方法，我们提出了CRB-Net，这是一种基于CSPResNet的增强目标检测器(Wang et al.， 2019\n\n## 网络结构\n\n图7显示了CRB-Net的体系结构。CRB-Net的主干使用ResBlock_building块获得5个输出特征层，每个特征层中的特征点设置为3个锚点。\n\n此外，每个检测层的每个检测帧基于不同的锚帧进行偏移。每个锚的宽度和高度值需要根据被检测物体的形状特征来获得。我们使用K-means聚类算法对锚帧的初始值进行优化，使锚帧更适合水面场景，同时显著减少了训练时间。\n\n接下来，使用两个SPPNets (He et al.， 2014)来增加F4和F5的接受域，这两个域可以分离出最重要的上下文特征。\n\n融合不同分辨率功能的常用方法是，在将它们相加之前，将它们的分辨率调整为相同的大小。然而，不同的输入对融合过程的贡献是不一样的。为了解决这个问题，我们设计了一个改进的BIFPN，它包含了一个注意机制。\n\n最后，将语义融合后的特征层分别送入5个Yolo头部，得到预测结果。\n\n## 网络模块详细信息\n\n### ResBlock_Body\n\n这实际上是一个CSPResNet，其结构如图8所示。剩余块堆叠在主干部分。另一部分，残边，经过一些处理后直接连接到末端。这种结构缓解了DNN深度增加引起的梯度消失问题。\n\n### K-Means Algorithm\n\n为了寻找最优的聚类效果，我们选取了多组不同数量的聚类进行实验比较。我们发现，当聚类数量达到15个时，平均欠条的增长几乎停止(平均欠条的计算方法是计算每个训练集标签的欠条和聚类得到的中心，以最大的欠条值作为该标签的值，最后将所有标签值平均得到)。考虑到模型过拟合的风险会随着聚类数量的增加而增加，最终选取了15个聚类中心。\n\n### 改进的BIFPN\n\n它集成了双向跨尺度连接和快速归一化融合。选择最佳值1.35作为BiFPN宽度缩放因子。为了更好的说明融合的过程，选择P2作为一个例子来描述融合的特征。\n\n此外，CRB-Net使用以下方案:CutMix (Yun等人，2019)、DropBlock正则化(Ghiasi等人，2018)、ciou损失(Zheng等人，2020)、CmBN (Yao等人，2020)和NMS (Bodla等人，2017)。我们尝试使用马赛克数据增强(Bochkovskiy等人，2020年)、类标签平滑(Szegedy等人，2016年)和余弦退火调度器(Loshchilov和Hutter, 2016年)，但这些方案都没有很好地工作。\n\n# 实验和讨论\n\n目标检测的目的是准确识别图像中各种目标的类别和位置信息。\n\n","tags":["surface object detection","dataset"],"categories":["文献阅读"]},{"title":"Paper Translation 5|Object detection method for ship safety plans using deep","url":"/2022/09/14/文献翻译5/","content":"\n# Ship detection for visual maritime surveillance from non-stationary platforms☆\n## Abstract\n\n> This paper presents a new ship target detection algorithm to achieve eﬃcient visual maritime surveillance from non-stationary surface platforms, e.g., buoys and ships, equipped with CCD cameras. In the proposed detector, the three main steps including horizon detection, background modeling and background subtraction, are all based on Discrete Cosine Transform (DCT). By exploiting the characteristics of DCT blocks, we simply extract the horizon line providing an important cue for sea-surface modeling. The DCT-based feature vectors are calculated as the sample input to a Gaussian mixture model which is eﬀective in representing dynamic ocean textures, such as waves, wakes and foams. Having modeled sea regions, we perform the ship detection using background subtraction followed by foreground segmentation. Experimental results with various maritime images demonstrate that the proposed ship detection algorithm outperforms the traditional techniques in terms of both detection accuracy and real-time performance, especially for complex sea-surface background with large waves.\n\n本文提出了一种新的船舶目标检测算法，以实现从非静止水面平台(如浮标和船舶)上高效的视觉海上监视。在该检测器中，地平线检测、背景建模和背景减去三个主要步骤都是基于离散余弦变换(DCT)的。通过利用DCT块体的特征，我们简单地提取了水平线，为海面建模提供了重要线索。**将基于dct的特征向量作为样本输入计算到高斯混合模型中，该模型能够有效地表示波浪、尾迹和泡沫等动态海洋纹理**。在建立了海域模型的基础上，采用背景减法和前景分割法对船舶进行检测。对各种海洋图像的实验结果表明，该算法在检测精度和实时性方面都优于传统的舰船检测算法，特别是在具有大波的复杂海面背景下。\n\n## 1. Introduction\n\nSea-surface platforms are commonly deployed for a wide variety of tasks and operations, such as open ocean exploration, supervisory control of Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs), oil and gas drilling, ecological monitoring and sampling, and homeland security surveillance (Y. Zhang et al., 2016; Li et al., 2014). For non-stationary platforms, e.g., buoys and ships, it is of great importance to develop an automated maritime surveillance system, especially in wide open waters. Such a visual monitoring tool in the military applications enables signiﬁcant cap-abilities for safeguarding maritime rights and interests, strengthening supervision and management of sensitive areas, resolving maritime disputes, and detecting illegal activities. It adds a great deal of convenience to civil applications as well, e.g., port traﬃc management, maritime search and rescue.\n\n海面平台通常用于各种各样的任务和操作，如开放海洋勘探、自主水下航行器(auv)和遥控航行器(rov)的监督控制、石油和天然气钻探、生态监测和采样以及国土安全监视(Y. Zhang等人，2016;Li et al.， 2014)。对于非固定平台，例如浮标和船只，开发一种自动化海上监视系统是非常重要的，特别是在宽阔的开放水域。这种可视化监控工具在军事上的应用，为维护海洋权益、加强对敏感地区的监督管理、解决海洋纠纷、侦查非法活动等提供了巨大的能力。它也为民用应用带来了极大的便利，如港口交通管理、海上搜救等。\n\nThe current maritime surveillance systems are mainly based on air-/space-borne Synthetic Aperture Radar (SAR), High Frequency Surface Wave Radar (HFSWR), regular ship-based radars, and air-/space-borne optical sensors (Tello et al., 2005; Sciotti et al., 2002; Zhu et al., 2010; Künzner et al). The SAR equipment can cover an ultra-wide range and operate continuously under all-weather conditions at the expense of limited image resolution. For optical sensors, the infrared camera provides longer view distance relative to typical cameras, especially at night or in low visibility (Withagen et al., 1999; Broek et al., 2000). However, the low-resolution imagery and high power consumption of infrared cameras limit access to the deployment of autonomous surveillance systems. In comparison, visible-light images captured by optical cameras generally contain rich color and texture information, which enable people to interpret and recognize the scene more readily. Additionally, the visible-light camera has the advantages of low-cost, easy installation and low power consumption. More importantly, such a camera can be employed for military security, since the passive imaging modality does not expose the location of the surveillance system. This has motivated the development and improvement of optical systems over the past decades, to meet the critical need of maritime scene monitoring at improved imaging resolution by integrating with other sensors (Feﬁlatyev et al., 2012).\n\n目前的海上监视系统主要基于空/星载合成孔径雷达(SAR)、高频表面波雷达(HFSWR)、常规舰载雷达和空/星载光学传感器(Tello等人，2005年;Sciotti等人，2002年;朱等，2010;Kunzner等)。SAR设备可以覆盖超宽的范围，并在全天候条件下连续工作，但以有限的图像分辨率为代价。对于光学传感器，红外相机相对于典型相机提供了更长的视距，特别是在夜间或能见度低的情况下(Withagen et al.， 1999;Broek et al.， 2000)。然而，红外摄像机的低分辨率图像和高功耗限制了自主监控系统的部署。相比之下，光学相机捕捉到的可见光图像通常包含丰富的颜色和纹理信息，使人们更容易对场景进行解读和识别。此外，该可见光相机具有成本低、安装方便、功耗低等优点。更重要的是，这种相机可以用于军事安全，因为被动成像方式不会暴露监视系统的位置。这推动了光学系统在过去几十年的发展和改进，通过与其他传感器集成，以满足提高成像分辨率的海上场景监测的关键需求(Fefilatyev等人，2012)。\n\nBased on the platform structure, the visual maritime surveillance system can be divided into two categories: video surveillance with stationary/non-stationary cameras. The stationary surveillance sys-tems are generally employed in harbor, port, and coast applications where the background remains basically unchanged. On the other hand, the non-stationary surveillance equipment usually works in open waters far away from the coastline using cameras mounted on moving ships or swaying buoys. In this case, the captured scene keeps changing due to the platform movement. This study is aimed at devising a solution for ship target detection from buoy-/ship-based visual sur-veillance, which can motivate more maritime applications for the realization of intelligent visual sensor networks with on-board video processing and real-time bi-directional communication.\n\n根据平台结构，可视海上监视系统可分为两类:固定式/非固定式摄像机视频监视。固定式监视系统一般应用于背景基本不变的港口、港口和海岸应用。另一方面，非固定式监视设备通常在远离海岸线的开放水域工作，使用安装在移动船只或摇摆浮标上的摄像机。在这种情况下，由于平台的移动，捕获的场景不断变化。本研究旨在设计一种基于浮标/船基视觉监视的船舶目标检测解决方案，为实现具有舰载视频处理和实时双向通信的智能视觉传感器网络提供更多的海上应用。\n\nSome earlier works on automatic detection techniques for ships or surface objects have been proposed based on video imagery informa-tion. These methods generally fall into three categories. The ﬁrst class is based on the background modeling and subtraction (Wang et al., 2005; Moreira et al., 2014; Prasad et al., 2017). Hu et al. (2011) proposed a vessel detection method in which the ocean background is simply modeled by median values of n video frames. Some authors (Bloisi and Iocchi, 2009; Frost and Tapamo, 2013; Grupta et al., 2009; Wei et al., 2009; Szpak and Tapamo, 2011; Robert-Inácio et al., 2007; Pires et al., 2010) used Gaussian functions to model the water surface followed by background subtraction. The Gaussian mixture model (GMM) statistically exploits the fact that each pixel belongs to the sea surface or to the ship (Moreira et al., 2014). Zhang and Zheng (2011) and Borghgraef et al. (2010) modiﬁed the conventional GMM to break down the challenging problem of fast moving maritime targets. However, these methods, primarily designed for ﬁxed cameras, do not generally oﬀer good detection / surveillance performance for non-sta-tionary platforms with a high degree of variability.\n\n一些基于视频图像信息的船舶或水面物体自动检测技术的早期工作已经被提出。这些方法通常分为三类。第一类是基于背景建模和减法(Wang et al.， 2005;莫雷拉等人，2014;Prasad等人，2017)。Hu et al.(2011)提出了一种船舶检测方法，该方法简单地用n个视频帧的中值来模拟海洋背景。一些作者(Bloisi和Iocchi, 2009;弗罗斯特和塔帕莫，2013;Grupta等，2009;魏等，2009;斯帕克和塔帕莫，2011年;Robert-Inácio等，2007;Pires等人，2010)使用高斯函数对水面进行建模，然后进行背景减法。高斯混合模型(GMM)在统计上利用了每个像素属于海面或船舶的事实(Moreira et al.， 2014)。Zhang和Zheng(2011)和Borghgraef等人(2010)修改了传统的GMM，以分解快速移动海上目标的挑战性问题。然而，这些方法主要是为固定摄像机设计的，通常不能为具有高度可变性的非静止平台提供良好的探测/监视性能。\n\nThe second category of ship detection methods is based on human visual attention model (Prasad et al., 2017). Itti et al. (1998) utilized a visual saliency map to analyze complex natural scenes. It segregates the regions of interest (high saliency) according to the local contrast which is consistently present at various length scales. However, it is not eﬀective in dealing with wakes corresponding to the moving ships, since they introduce a high contrast over the surrounding pixels (Prasad et al., 2017). To achieve real-time detection, Hou and Zhang (2007) constructed the saliency map in spatial domain by extracting the corresponding spectral residual in spectral domain. Agraﬁotis et al.(2014) designed a maritime tracking system by combining visual attention map with GMM. The tracking results are further reﬁned using an adaptable online neural network tracker. Additional enhance-ment on visual attention model was realized by a two-scale detection scheme (Liu et al., 2013). At the larger scale, sea background is removed by a mean-shift smoothing algorithm. At the smaller scale, objects of interest are coarsely labelled using salient edge region extraction; post-processing for chrominance components provides more useful cues to select the output targets. Albrecht et al. (2011) modeled the visual maritime attention using multiple low-level image features in combination with a Bayes classiﬁer. We conclude that these approaches based on visual attention model expect to reduce the noise of sea background at a larger scale as well as enhance the salient features of object regions. However, such methods usually do not perform well when large surface waves are involved in the scene. This is because the visual saliency map will probably become inaccurate if the salience of waves has the same or even higher order of magnitude compared to the original targets.\n\n第二类船舶检测方法是基于人类视觉注意模型(Prasad et al.， 2017)。Itti等人(1998)利用视觉显著图来分析复杂的自然场景。它根据在不同长度尺度上一致存在的局部对比来分离感兴趣的区域(高显著性)。然而，它在处理与移动船只相对应的尾迹时并不有效，因为它们比周围像素引入了高对比度(Prasad et al.， 2017)。为了实现实时检测，Hou和Zhang(2007)通过提取光谱域中对应的光谱残差，构建了空间域的显著性图。Agrafiotis等(2014)将视觉注意地图与GMM相结合，设计了一种海上跟踪系统。利用可适应的在线神经网络跟踪器对跟踪结果进行进一步优化。通过双尺度检测方案实现了对视觉注意模型的额外增强(Liu et al.， 2013)。在大尺度上，采用均值漂移平滑算法去除海洋背景。在较小的尺度上，使用显著边缘区域提取对感兴趣的对象进行粗标记;色度成分的后处理为选择输出目标提供了更有用的线索。Albrecht等人(2011)使用多个低级图像特征结合贝叶斯分类器对海洋视觉注意力进行建模。这些基于视觉注意模型的方法有望在更大范围内降低海洋背景的噪声，并增强目标区域的显著特征。然而，当场景中涉及到较大的表面波时，这种方法通常不能很好地执行。这是因为，如果波的显著性与原始目标具有相同甚至更高的数量级，则可视显著性图可能会变得不准确。\n\nThe other techniques apply edge and texture features to detect ship targets. For buoy-based visual surveillance, Feﬁlatyev et al. (2012) and Feﬁlatyev (2012) proposed a marine vehicle detection algorithm by exploiting the gradient information. After extracting the horizon by Hough transform, a global thresholding algorithm segments ship targets eﬀectively from the background region above the estimated horizon. However, this method cannot work once the targets appear below the horizon. Although the edge and contour features (e.g., using Hough transform) are widely used in ship detection (Arshad et al., 2011; Yan et al., 2012; Xu et al., 2011), these methods generally do not achieve good performance for complex background. To make full use of various texture information in the sea-surface background, the detec-tion accuracy can be signiﬁcantly improved by incorporating fractal feature (Liang et al., 2012). Using both color and texture components,Kumar and Selvi (2011) and Selvi and Kumar (2011) introduced an object classiﬁcation algorithm based on Local Binary Pattern (LBP) for ship detection. The Histogram of Gradients (HOG) (Wijnhoven et al., 2010) is a commonly used feature representation. Loomans et al.(2013) devised a combination of a multi-scale HOG detector and a hierarchical KLT feature point tracker to track ships in harbors. This tracking system also incorporates an active camera to improve the tracking results under challenging conditions. In the Pascal Visual Object Classes (VOC) challenge, the Deformable Part Model (DPM) based on HOG achieved the best detection performance for twenty classes, including boats (Everingham et al., 2015). In Sullivan and Shah (2008), an enhanced Maximum Average Correlation Height (MACH) ﬁlter was applied for vessel detection by matching appearance tem-plates with testing sequences via Fast Fourier transform (FFT). The advantage of these appearance-based methods is that they do not rely on background modeling. In more recent works (Everingham et al., 2015; R. Zhang et al., 2016; Zou and Shi, 2016), the Convolutional Neural Network (CNN) features have achieved a substantial improve-ment in detection performance. While the above feature-based detec-tion schemes beneﬁt from the texture consistency within sea-surface background, their computational complexities pose a rather serious challenge to real-time video communication over visual sensor net-works.\n\n另一种方法是利用边缘和纹理特征来检测船舶目标。对于基于浮标的视觉监控，Fefilatyev等(2012)和Fefilatyev(2012)提出了一种利用梯度信息的海洋车辆检测算法。在Hough变换提取水平面后，全局阈值算法将船舶目标有效地从估计水平面以上的背景区域中分割出来。然而，一旦目标出现在地平线以下，这种方法就无法工作。尽管边缘和轮廓特征(例如，使用霍夫变换)被广泛应用于船舶检测(Arshad等人，2011;严等人，2012;Xu et al.， 2011)，对于复杂背景，这些方法通常不能取得良好的性能。为了充分利用海面背景中的各种纹理信息，结合分形特征可以显著提高检测精度(Liang et al.， 2012)。Kumar和Selvi(2011)和Selvi和Kumar(2011)同时使用颜色和纹理成分，提出了一种基于局部二值模式(LBP)的目标分类算法用于舰船检测。梯度直方图(HOG) (Wijnhoven et al.， 2010)是一种常用的特征表示。Loomans等人(2013)设计了一种多尺度HOG探测器和分层KLT特征点跟踪器的组合，用于跟踪港口中的船舶。该跟踪系统还包括一个主动摄像头，以改善在具有挑战性的条件下的跟踪结果。在Pascal可视对象类(VOC)挑战中，基于HOG的可变形部件模型(DPM)在包括船只在内的20个类中获得了最佳检测性能(Everingham等人，2015年)。在Sullivan和Shah(2008)中，通过快速傅立叶变换(FFT)将外观模板与检测序列进行匹配，采用一种增强的最大平均相关高度(MACH)滤波器进行船舶检测。这些基于外观的方法的优点是它们不依赖于背景建模。在最近的作品中(Everingham等人，2015;张锐等，2016;邹和Shi, 2016)，卷积神经网络(CNN)特征在检测性能上取得了实质性的提高。上述基于特征的检测方案虽然得益于海面背景的纹理一致性，但其计算复杂性给视觉传感器网络上的实时视频通信带来了相当严重的挑战。\n\nAiming at visual maritime surveillance from non-stationary plat-forms, we propose an eﬃcient ship target detection algorithm to achieve both high detection accuracy and real-time performance. Here, we ﬁrst develop an eﬀective learning strategy including simple horizon segmentation and complex sea-surface background modeling. In maritime scenario, ships usually appear around the position of the horizon line, occupying both sky and ocean regions. The horizon line can be used as a reference to limit the regions of interest and reduce the execution time of detection. After horizon detection, we can simply extract the sea-surface background regions below the horizon and only use these regions for background modeling. This lowers the probability of detection mistakes caused by the presence of background motion, e.g., waves, wakes, and foams. More importantly, such independent detectors for sky and sea regions increase the detection sensitivity to small objects around the horizon line. Therefore, an initial detector of horizon line is required before background modeling and object detection. In the proposed scheme, we can simply detect the horizon line by exploiting the characteristics of Discrete Cosine Transform (DCT) blocks. At the step of sea-surface background modeling, we present a novel DCT-based texture Gaussian mixture model to further separate ship targets from the complex sea-surface background below the horizon. Having detected the sea-surface background, we remove it and ﬁnally obtain ship targets according to the texture consistency. The main contribution of the proposed algorithm is to provide more accurate detection results within complex sea-surface background, which is of vital importance for ship-/buoy-based surveillance applica-tions in the presence of large waves. Experiments with real images are presented to assess the eﬀectiveness of the proposed ship detection approach, in comparison to traditional techniques.\n\n针对非平稳平台上的海上视觉监视，提出了一种高效的船舶目标检测算法，以达到较高的检测精度和实时性。在此，我们首先开发了一个有效的学习策略，包括简单的地平线分割和复杂的海面背景建模。在海上场景中，船舶通常出现在地平线的位置附近，既占据了天空，也占据了海洋。地平线可以作为参考，限制感兴趣的区域，减少检测的执行时间。地平线检测后，我们可以简单提取地平线以下的海面背景区域，只使用这些区域进行背景建模。这降低了由背景运动(如波浪、尾迹和泡沫)引起的探测错误的概率。更重要的是，这种针对天空和海洋区域的独立探测器增加了对地平线周围小物体的探测灵敏度。因此，在背景建模和目标检测之前，需要先对地平线进行初始探测。在该方法中，我们可以利用离散余弦变换(DCT)块的特性简单地检测出水平线。在海面背景建模阶段，提出了一种新的基于dct的纹理高斯混合模型，进一步将船舶目标从复杂的海面背景中分离出来。在探测到海面背景后，对其进行去除，最后根据纹理一致性得到舰船目标。该算法的主要贡献是在复杂的海面背景下提供更准确的探测结果，这对船舶/浮标在大浪环境下的监视应用至关重要。用真实图像进行了实验，以评估所提出的船舶检测方法的有效性，并与传统技术进行了比较。\n\nIn the remainder, we describe the implementation details of the proposed ship detection algorithm in Section 2, and compare its performance with previous techniques in Section 3. We present a summary and conclusions from this investigation in Section 4.\n\n在剩下的部分中，我们将在第2节中描述所提出的船舶检测算法的实现细节，并在第3节中比较其与以前的技术的性能。我们在第4节中给出了这个调查的总结和结论。\n\n## 2. The proposed ship detection algorithm\n\nThe maritime images acquired from a non-stationary platform typically contain the foreground of ship targets and the background of sea surface as well as sky. The main challenge for background subtraction and detection of foreground objects is the diﬃculty in modeling the dynamics of water, including waves, wakes and foams (Prasad et al., 2017). In order to improve the detection accuracy of maritime surveillance systems in open sea, we present a novel ship detection algorithm using DCT-based Gaussian mixture model (GMM). According to the characteristics of DCT coeﬃcients, the proposed algorithm ﬁrst detects the horizon line to extract the sea-surface regions for background modeling.  In the procedure of sea-surface background modeling with GMM, we calculate the coeﬃcient energies of three regions in each DCT block as the feature vectors, to feed the learning process of GMM. Once all the ocean regions are modeled , the vessels in these regions can be detected by classifying each image block into background or foreground.\n\n从非静止平台上获取的海洋图像通常包含船舶目标的前景和海面背景以及天空。背景减去和探测前景物体的主要挑战是难以建模水的动力学，包括波浪、尾迹和泡沫(Prasad et al.， 2017)。为了提高公海海上监视系统的检测精度，提出了一种基于dct的高斯混合模型(GMM)的船舶检测算法。该算法根据DCT系数的特点，首先检测水平线，提取海面区域进行背景建模。在基于GMM的海面背景建模过程中，我们计算每个DCT块中三个区域的系数能量作为特征向量，为GMM的学习过程提供支持。一旦建立了所有海洋区域的模型，就可以通过将每个图像块分为背景或前景来检测这些区域中的船只。\n\nFig. 1 outlines the procedure of the proposed ship detection scheme, including the detection of horizon line, the sea-surface back-ground modeling using GMM, and the ship detection using background subtraction. Next, we give the detailed description of each part in the proposed algorithm.\n\n所提出的船舶检测方案的流程如图1所示，包括水平线的检测、GMM的海面背景建模和背景减去的船舶检测。接下来，我们对算法的各个部分进行了详细的描述。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914155613.png)\n\n### 2.1. DCT-based horizon detection\n\nThe initial detector of horizon line is required before background modeling and object detection. According to the characteristics of DCT coeﬃcients, the proposed horizon detection algorithm involves the following steps:\n\n1. Decompose the luminance component of an input image into 8×8 non-overlapped blocks, and then apply DCT to these blocks:\n\n   ![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914110214.png)\n\n2. Label a DCT block as sky/sea-surface region using the following measure:\n\n   ![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914110252.png)\n\n   ![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914110317.png)\n\n3. Draw a horizon line approximately using the central points of all bottommost blocks in the sky region. In some cases, the ship targets may occupy a large fraction of sky background above the horizon line. In order to avoid the misclassiﬁcation caused by ship targets, we only select the bottommost blocks with smooth changes in vertical direction (y-coordinates) for horizon detection.\n\n在背景建模和目标检测之前，需要先对水平线进行初始检测。根据DCT系数的特点，提出的地平线检测算法包括以下步骤:\n1. 将输入图像的亮度分量分解为8×8不重叠的块，然后对这些块应用DCT:\n2. 用以下方法将DCT块标记为天空/海面区域:\n3. 用天空区域中所有最底部方块的中心点画一条水平线。在某些情况下，船舶目标可能占据地平线以上天空背景的很大一部分。为了避免船舶目标造成的分类错误，我们只选择垂直方向(y坐标)变化平缓的最底部块进行水平检测。\n\n## 2.2. Sea-surface background modeling using GMM（使用GMM进行海面背景建模）\n\nThe background subtraction technique is an eﬀective tool for moving object detection with stationary cameras, where a pixel-wise statistical background model is used to classify the input video stream into foreground and background regions. However, such pixel-based background modeling algorithm is mainly suitable for visual maritime surveillance from stationary platforms. Our goal is to address the issue of object detection from non-stationary platforms, such as buoys or ships. From a large range of video content types, we observe that the texture feature of sea surface is generally uniform and consistent regardless of whether the camera is stable or not. Considering the texture consistency of sea-surface background, we present a DCT-based background modeling method using texture features. According to the input video contents, this model should be realized by a learning mechanism which can quickly provide texture-based features for dynamic scene analysis of sea surface.\n\n背景减法技术是静止摄像机运动目标检测的一种有效工具，该技术使用像素统计背景模型将输入视频流划分为前景和背景区域。但这种基于像素的背景建模算法主要适用于静止平台上的海上视觉监视。我们的目标是解决从非静止平台(如浮标或船只)上探测物体的问题。从大量的视频内容类型中，我们观察到，无论摄像机是否稳定，海面的纹理特征基本上是一致的。考虑到海面背景的纹理一致性，提出了一种基于dct的基于纹理特征的背景建模方法。根据输入的视频内容，该模型需要通过一种学习机制来实现，该学习机制能够快速地为海面动态场景分析提供基于纹理的特征。\n\n### 2.2.1. DCT-based feature selection of background texture\n\nThe texture-based features in sea-surface background can be considered as the spatial distribution of intensity variations. Here, we deﬁne the texture feature as a three dimensional vector in DCT domain, as shown in Fig. 2. The DCT coeﬃcients depicted by diﬀerent colors account for the spectrum component in the corresponding direction. To elaborate, the white region R0 in the upper left corner denotes the direct-current component; R1 (green), R2 (yellow) and R3 (gray) regions represent the vertical, diagonal and horizontal frequency variation, i.e., horizontal, diagonal and vertical texture information. We calculate the energies E1, E2 and E3 of regions R1, R2 and R3 respectively to generate the texture-based feature vector X:\n\n海表背景中的纹理特征可以看作是强度变化的空间分布。这里，我们将纹理特征定义为DCT域中的三维向量，如图2所示。用不同颜色表示的DCT系数表示对应方向的频谱分量。为了详细说明，左上角的白色区域R0表示直流分量;R1(绿色)、R2(黄色)和R3(灰色)区域分别表示垂直、对角和水平的频率变化，即水平、对角和垂直纹理信息。我们分别计算R1、R2和R3区域的能量E1、E2和E3，生成基于纹理的特征向量X:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914160612.png)\n\nAccordingly, each block in the sea-surface background yields a corresponding texture feature vector denoted by i (Xi= 1,2, …,N); N is the total number of blocks or feature vectors within the sea-surface background region. Using these texture-based feature vectors, we can model the texture background of sea surface in the following section.\n\nN为海面背景区域内的块或特征向量的总数。利用这些基于纹理的特征向量，我们可以在下一节中对海面的纹理背景进行建模。\n\n### 2.2.2. Learning of Gaussian mixture model\nAfter the horizon detection described in Section 2.1, we can segment the sea-surface background from the sky background. We then categorize the sea-surface background regions into K clusters using Gaussian mixture model (GMM) (Wang et al., 2005). The GMM is based on the feature vectors described in Section 2.2.1. Let D = { ,XX12, …, tX} account for a sample set of 3-dimensional feature vectors X deﬁned in Eq. (3). Each sample corresponds to a DCT block from K clusters with a certain probability. To quantify, the probability of Xt based on Gaussian distribution is written as (Wang et al., 2005):\n\n在2.1节中描述的地平线检测之后，我们可以从天空背景中分割出海面背景。然后我们使用高斯混合模型(GMM)将海面背景区域划分为K个簇(Wang et al.， 2005)。GMM基于2.2.1节中描述的特征向量。设D = {，XX12，…，tX}表示公式(3)中定义的三维特征向量X的样本集。每个样本对应K个聚类中的一个具有一定概率的DCT块。为了量化，基于高斯分布的Xt概率写为(Wang et al.， 2005):\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914160737.png)\n\nSince the DCT is an orthogonal transform, coeﬃcients in a DCT block are independent of each other (Wang et al., 2005). Accordingly, the three components E1, E2 and E3 (coeﬃcient energies) of X are mutually independent. As the covariance of independent variables is zero, the covariance matrix Σi,t is a diagonal matrix:\n\n由于DCT是一个正交变换，DCT块中的系数是相互独立的(Wang et al.， 2005)。因此，X的三个分量E1、E2和E3(系数能量)是相互独立的。由于自变量的协方差为零，因此协方差矩阵Σi,t为对角矩阵:\n\n![image-20220914160810185](C:\\Users\\Pengyk\\AppData\\Roaming\\Typora\\typora-user-images\\image-20220914160810185.png)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914160841.png)","tags":["ship detection","visual maritime surveillance","object detection"],"categories":["文献阅读"]},{"title":"Paper Translation 4|Object detection method for ship safety plans using deep learning","url":"/2022/09/13/文献翻译4/","content":"\n# Object detection method for ship safety plans using deep learning\n\n## 0 Abstract\n\n> During the safety inspection of a ship, there is a stage conﬁrming whether the safety plan is designed in accordance with the regulations. In this process, an inspector checks whether the location and number of various objects (safety equipment, signs, etc.) included in the safety plan meet the regulations. Manually converting the information of objects existing in the ship safety plan into digital data requires signiﬁcant effort and time. To overcome this problem, a technique is required for automatically extracting the location and information of the object in the plan. However, owing to the characteristics of the ship safety plan, there are frequent cases in which the detection target overlaps with noise (ﬁgure, text, etc.), which lowers the detection accuracy. In this study, an object detection method that can effectively extract the object quantity and location within the ship safety plan was proposed. Among various deep learning models, suitable models for object detection in ship safety plans were compared and analyzed. In addition, an algorithm to generate the data necessary for training the object detection model was proposed and adopted the feature parameters, which showed the best performance. Subsequently, a specialized object detection method to rapidly process a large ship safety plan was proposed. The method proposed in this study was applied to 15 ship safety plans. Consequently, an average recall of 0.85 was achieved, conﬁrming the effectiveness of the proposed method.   \n\n船舶安全检查过程中，有一个确认安全方案是否按规定设计的阶段。在此过程中，检查员检查安全计划中所包含的各种物体(安全设备、标志等)的位置和数量是否符合规定。将船舶安全计划中存在的物体信息手动转换为数字数据需要大量的精力和时间。为了解决这一问题，需要一种自动提取规划目标位置和信息的技术。但由于舰船安全方案的特点，检测目标与噪声(图像、文字等)重叠的情况时有发生，降低了检测精度。本文提出了一种能够有效提取**船舶安全计划**中目标数量和位置的目标检测方法。在各种深度学习模型中，对适合舰船安全计划目标检测的模型进行了比较和分析。此外，提出了**一种生成训练目标检测模型所需数据的算法**，该算法采用的特征参数表现出最佳性能。。随后，提出了一种**用于快速处理大型船舶安全方案的目标检测方法**。本研究提出的方法已应用于15个船舶安全计划。平均召回率为0.85，验证了该方法的有效性。\n\n## 1 Introduction\n\n### 1.1 Research background\n\n>To examine whether safety regulations of ships are met, the register of shipping should check the installation location and quantity of various safety and lifesaving devices. A plan containing this information is called a ship safety plan (thereafter referred to as a safety plan). Shipbuilding companies produce safety plans according to the regulations of the International Maritime Organization (IMO) (2003; 2017). For example, they should represent the installed devices in the same format as deﬁned in Table 1. Subsequently, the ship owner or ship-building company provides a safety plan to the register of shipping for inspection. This plan is usually delivered in a non-editable document in various formats such as Portable Document Format (PDF), Joint Photograph Experts Group (JPEG), or Portable Network Graphics (PNG), because of conﬁdentiality issues. Aforementioned formats are unable to provide detailed information (location, quantity, type, etc.) of the inserted devices. Therefore, the inspector needs to inspect procedures manually because of insufﬁcient information. \n\n船舶登记机关为检查船舶是否符合安全规定，应检查各种安全救生装置的安装位置和数量。包含这些信息的计划称为船舶安全计划(以后称为安全计划)。造船企业根据国际海事组织(IMO)(2003年;2017)。例如，它们应该以表1中定义的相同格式表示已安装的设备。随后，船东或造船公司向船舶注册机构提供一份安全计划，以供检查。由于保密问题，该计划通常以各种格式(如便携式文件格式(PDF)、联合摄影专家组(JPEG)或便携式网络图形(PNG))等不可编辑的文件形式交付。上述格式无法提供插入设备的详细信息(位置、数量、类型等)。因此，由于资料不足，检验员需要手工检查程序。\n\n>Owing to the characteristics of the shipbuilding industry, safety plans are extensive and complex, requiring an inordinately large amount of labor for relatively simple work. In particular, a device (hereafter referred to as an object) in a safety plan is small, less than 0.003% of the total safety plan size on average, and the number of objects is mostly 800 or above. Consequently, several inspectors must manually inspect the location and quantity of each object over several days. In addition, there might be a difference in the results according to the proﬁciency levels of the inspectors. Therefore, various additional tasks may be necessary, such as collecting the results of each inspector for the ﬁnal revision. \n\n由于造船业的特点，安全计划广泛而复杂，相对简单的工作需要大量的劳动力。具体而言，安全计划中的设备(以下简称对象)较小，平均小于总安全计划尺寸的0.003%，对象数量多为800个及以上。因此，若干视察员必须在几天内人工检查每个物体的位置和数量。此外，根据检查员的熟练程度，结果也可能有所不同。因此，各种额外的任务可能是必要的，例如为最终的修订收集每个检查人员的结果。\n\n> In this study, an object detection model was applied to automate the aforementioned inspection procedure for a safety plan. Object detection refers to a task that requires an algorithm to localize all objects in the image (Russakovsky et al., 2015). The proposed model requires a safety plan as input and detects the objects; subsequently, the location and quantity of objects are organized. As a result, the time needed for the inspection procedure can be saved. \n\n在本研究中，本研究应用目标检测模型，实现上述安全计划检测过程的自动化。对象检测是指需要一种算法来定位图像中所有对象的任务(Russakovsky et al.， 2015)。该模型以安全计划为输入，对目标进行检测;随后，组织对象的位置和数量。因此，可以节省检验程序所需的时间。\n\n### 1.2 Related works\n\n> Research on detecting target objects within input images has been actively conducted in various ﬁelds. The algorithms for detecting objects can be divided into two types, as follows:  \n>\n>- Decomposing objects into primary elements (such as lines, arcs, and circles) and ﬁnding them in input images\n>- Matching objects to a region of input images directly\n\n对输入图像中目标物体的检测在各个领域都有积极的研究。检测对象的算法可以分为两类:\n\n- 将对象分解为主要元素(如线、弧和圆)，并在输入图像中找到它们\n\n- 将对象直接匹配到输入图像的区域\n\n> To execute the former algorithm, the structural characteristics of the target object should be determined. The target object is decomposed into primary elements, and geometric relationships between them are determined. If a region of the primary elements has a similar part of the geometric relationships of the target object, the region can be considered as the target object; this method is known as a case-based algorithm. Luo and Liu (2003) proposed ten geometric relationships to deﬁne a target object consisting of a circle and two horizontal straight lines. This al-gorithm can achieve approximately 100% detection accuracy when the target object and input image are simple. However, as the shape of the target object becomes more complex, the number of geometric re-lationships increases exponentially. Moreover, the detection accuracy rapidly decreases in overlapping objects. Another limitation is that the detection accuracy is signiﬁcantly affected by the quality and angle of the target object and input image. \n\n要执行前一种算法，需要确定目标对象的结构特征。将目标对象分解为基本元素，确定它们之间的几何关系。如果一个区域的初级元素与目标对象有相似的部分几何关系，可以认为该区域是目标对象;这种方法被称为基于案例的算法。Luo和Liu(2003)提出了十种几何关系来定义一个由圆和两条水平直线组成的目标对象。在目标对象和输入图像比较简单的情况下，该算法可以达到约100%的检测精度。然而，随着目标对象的形状变得更加复杂，几何关系的数量呈指数增长。此外，在目标重叠的情况下，检测精度会迅速下降。另一个局限性是检测精度受目标物体和输入图像的质量和角度影响较大。\n\n> However, in the latter case, the algorithm directly compares whether there is a region similar to the target object in the input image. Subsequently, it detects the region with the highest correlation as the target object. Before the advent of deep learning technology, the sliding- window algorithm was primarily used to detect objects in images (Gualdi et al., 2012). The sliding-window algorithm detects the highest correlation position as the target object by moving around the detection window, as shown in Fig. 1. However, this method has the disadvantage of lowering the detection accuracy when the quality of the object to be searched in the image is not good or when rotation, deformation, and overlapping occur. Therefore, it is essential to rearrange images or remove unnecessary elements through image preprocessing before detection to prevent these problems. The method of moving around the detection window and performing a speciﬁc calculation within the window is known as the sliding-window algorithm. \n\n而对于后一种情况，算法直接比较输入图像中是否存在与目标对象相似的区域。然后检测相关度最高的区域作为目标对象。在深度学习技术出现之前，滑动窗算法主要用于检测图像中的目标(Gualdi et al.， 2012)。滑动窗口算法通过在检测窗口周围移动，将相关度最高的位置检测为目标对象，如图1所示。但是，当图像中待搜索对象的质量不佳或发生旋转、变形和重叠时，该方法的缺点是会降低检测精度。因此，在检测前必须对图像进行重新排列或者通过图像预处理去除不必要的元素，以防止这些问题的发生。在检测窗口周围移动并在窗口内执行特定计算的方法被称为滑动窗口算法。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220913103733.png)\n\n> In the 2000s, deep learning technology developed signiﬁcantly, and emerged as an alternative solution to overcome excessive reliance on the quality of the input image. Generally, the deep learning method is deﬁned as a set of machine learning algorithms that train a model through a large amount of data and attempt high-level abstraction using the trained model (Bengio et al., 2013). Speciﬁcally, it extracts critical features from a large amount of data or complex data through the training process. Therefore, a deep learning method can build a model that summarizes the characteristics of a target object, and can detect the target object by locating features in an input image. \n\n进入21世纪后，深度学习技术得到了显著的发展，并成为克服对输入图像质量过度依赖的替代解决方案。通常，深度学习方法被定义为一组机器学习算法，通过大量数据训练模型，并尝试使用训练后的模型进行高层抽象(Bengio et al.， 2013)。具体来说，它通过训练过程从大量数据或复杂数据中提取关键特征。因此，深度学习方法可以建立一个总结目标对象特征的模型，并通过在输入图像中定位特征来检测目标对象。\n\n> In the ﬁeld of object detection in a plan, most of the relevant research focuses on piping and instrumentation diagram (P&ID). Yu et al. (2019) applied the modiﬁed sliding-window algorithm with AlexNet (Krizhev-sky et al., 2017), which is a deep learning model used for classiﬁcation. Through this model, an accuracy of over 90% was achieved for four types of objects considering rotation. The sliding-window algorithm requires high computation time because it searches the entire image. Therefore, beyond the classiﬁcation of an input image, object detection that directly speciﬁes the location of an object and categorizes a class has recently become popular. Rahul et al. (2019) used fully convolutional networks (FCN) (Shelhamer et al., 2017) to recognize 11 types of objects in P&ID and showed high accuracy with an average F1-score of 0.94. Similarly, Yun et al. (2020) assigned ten types of objects in P&IDs as the target to be detected. They used the regions with CNN features (R–CNN) model (Girshick et al., 2014) modiﬁed for the environment of P&ID and achieved an average F1-score of 0.92. \n\n在目标检测领域，大多数相关的研究都集中在管道和仪表图(P&ID)上。Yu等人(2019)将改进的滑动窗口算法与AlexNet (Krizhev-sky等人，2017)结合使用，AlexNet是用于分类的深度学习模型。通过该模型，对四种考虑旋转的物体，精度均达到90%以上。滑动窗口算法需要对整个图像进行搜索，计算时间较长。因此，除了对输入图像进行分类之外，直接指定对象位置并对类进行分类的对象检测最近变得很流行。Rahul等人(2019)使用完全卷积网络(FCN) (Shelhamer等人，2017)在P&ID中识别了11种类型的对象，并显示出较高的准确性，f1平均得分为0.94。同样，Yun et al.(2020)在p&id中指定了10种类型的物体作为待检测的目标。他们使用了带有CNN特征的区域(R-CNN)模型(Girshick et al.， 2014)，该模型针对P&ID的环境进行了改进，f1平均得分为0.92。\n\n> In the ﬁeld of naval architecture and ocean engineering, several machine learning models have been proposed. Lee et al. (2020) detected ships in images using the Faster R–CNN model and tracked them using a Kalman ﬁlter. Kim et al. (2020) trained the existing ocean weather data using the convolutional long short-term memory (LSTM) model (Shi et al., 2015), which is effective in processing time-series data, and used the model for ocean weather prediction. Furthermore, using these advantages, Lee et al. (2021) estimated the required horsepower for the ship’s operation using the LSTM model and predicted the fuel oil consumption. Choi et al. (2020) proposed a model that predicts the wave height from an image of the sea using convolutional LSTM. In addition to deep learning models, in the ﬁeld of machine learning, there is a model called reinforcement learning that takes actions and learns the consequences to make decisions autonomously in speciﬁc situations. Zhao and Roh (2019) proposed a collision avoidance method that considers equations of the motion of a ship by applying a deep reinforcement learning (DRL) model (Mnih et al., 2013). By expanding this, Chun et al. (2021) proposed a DRL-based collision avoidance model for autonomous ships to decide their avoidance routes and navigate autonomously. However, research on the detection of plans in naval architecture and ocean engineering is rare. \n\n在造船和海洋工程领域，已经提出了几种机器学习模型。Lee等人(2020)使用Faster R-CNN模型检测图像中的船只，并使用卡尔曼滤波器跟踪它们。Kim等人(2020)利用卷积长短时记忆(LSTM)模型(Shi et al.， 2015)对已有的海洋天气数据进行训练，该模型对时间序列数据处理有效，并用于海洋天气预报。此外，利用这些优势，Lee等人(2021年)利用LSTM模型估计了船舶运行所需的马力，并预测了燃油消耗。Choi等人(2020)提出了一种利用卷积LSTM从海洋图像预测海浪高度的模型。除了深度学习模型，在机器学习领域，还有一种叫做强化学习的模型，它采取行动并学习结果，在特定的情况下自主做出决定。Zhao和Roh(2019)提出了一种通过应用深度强化学习(DRL)模型来考虑船舶运动方程的避碰方法(Mnih et al.， 2013)。在此基础上，Chun等人(2021)提出了一种基于drl的船舶避碰模型，用于自主船舶决定避碰路线和自主航行。然而，在船舶工程和海洋工程中，对平面图检测的研究还很少。\n\n> In this study, the input image was the safety plan, in which the target objects frequently overlapped with other objects, lines, or texts. Furthermore, if these objects are classiﬁed in the same class, there might be a difference in the expression method depending on the safety plan designer. In addition, because the target objects are signiﬁcantly small compared with the size of the safety plan, a specialized object detection algorithm is required. Therefore, this study proposes an object detection method that effectively founds objects in an enormous safety plan. Furthermore, a data generation model is proposed that makes training data automatically without a time-consuming labeling process. \n\n在本研究中，输入图像为安全计划，目标对象经常与其他对象、线条或文本重叠。此外，如果这些对象属于同一类，则表达式方法可能会有差异，这取决于安全计划设计器。此外，由于目标对象相对于安全计划的规模而言明显较小，因此需要专门的目标检测算法。因此，本研究提出了一种在庞大的安全计划中有效发现目标的目标检测方法。此外，提出了一种数据生成模型，使训练数据自动生成，而无需费时的标记过程。\n\n# 2 Theoretical background\n\n### 2.1 Overall process of the object detection in the safety plan（安全方案中物体检测的整体过程）\n\n> The overall process for detecting an object in the safety plan of this study is illustrated in Fig. 2. This study mainly comprises an object detection model, a training data generation model, and an object detection application. \n> First, to detect objects in the safety plan, the Faster RCNN model was used as the object detection model. The sliding-window and non-maximum suppression (NMS) algorithms were used together as detection algorithms (Fig. 2-(a)). Because a large amount of training data was required to train the object detection model, a speciﬁc training data generation model was proposed, as shown in Fig. 2-(b), to cover the insufﬁcient data. Finally, the object detection model was applied to detect objects in the safety plan, as shown in Fig. 2-(c). \n> Detailed procedures for each part are described in forthcoming sections. \n\n本研究安全方案中物体检测的总体流程如图2所示。本研究主要包括目标检测模型、训练数据生成模型和目标检测应用。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914093206.png)\n\n首先，对安全计划中的对象进行检测，使用Faster RCNN模型作为对象检测模型。检测算法采用滑动窗口算法和非最大抑制算法(NMS)相结合(图2-(a))。由于训练目标检测模型需要大量的训练数据，为了弥补不足的数据，我们提出了如图2-(b)所示的训练数据生成模型。最后，利用对象检测模型对安全方案中的对象进行检测，如图2-(c)所示。\n\n每个部分的详细过程将在后面的小节中描述。\n\n### 2.2 Detection target of the safety plan （安全计划的检测目标）\n\n> The input safety plan can be divided into two major parts, as shown in Fig. 3. The safety plan was divided into a region showing ﬁreﬁghting and safety devices with speciﬁc symbols and a table summarizing the information of objects. In addition, the characteristics of an object were represented by a text of connotations around the object. The object table contained information on the expression method, name, and quantity of each object. In this study, the objects were classiﬁed by IMO classiﬁcations (2003; 2017), which are referred to as classes. \n\n输入安全方案可分为两大部分，如图3所示。安全计划被划分为一个区域，显示消防和安全设备，并有特定的符号和一个表格，总结物体的信息。此外，一个对象的特征是由一个文本的内涵围绕对象。对象表包含关于每个对象的表达式方法、名称和数量的信息。本研究采用IMO (2003;2017年)，被称为类。\n\n> Every object in the safety plan must meet the requirement of a speciﬁc location and quantity according to shipping regulations; therefore, the register of shipping must inspect this aspect to ensure safety. This study can support the inspection procedure by automatically detecting the location and quantity of objects in a safety plan. \n\n安全计划中的每一件物品都必须符合航运规定的特定地点和数量的要求;因此，船级社必须在这方面进行检查，以确保安全。该研究可通过自动检测安全计划中物体的位置和数量来支持检测过程。\n\n> Object detection models mostly require image formats such as JPEG and PNG for input values. However, most designers provide a safety plan in a PDF document in which size information, such as pixels, is not provided. Therefore, to match the input format, a PDF document must be converted into JPEG format. \n\n对象检测模型大多需要JPEG和PNG等图像格式作为输入值。然而，大多数设计师在PDF文档中提供了一个安全方案，其中没有提供像素等大小信息。因此，为了匹配输入格式，必须将PDF文档转换为JPEG格式。\n\n> The converted image size depends on the output dots per inch (DPI). DPI refers to the number of pixels included in a length of 1 inch; hence, it is used as a measure of resolution. As the DPI increases, there is a possibility that the objects in the safety plan become more explicit, leading to an increase in detection accuracy. However, conversion to a speciﬁc DPI requires a signiﬁcant amount of memory because of the enormous size of a safety plan. Therefore, it is necessary to set an appropriate DPI to prevent an out-of-memory problem. \n\n转换后的图像大小取决于每英寸输出点(DPI)。DPI是指长度为1英寸内包含的像素个数;因此，它被用作分辨率的衡量标准。随着DPI的增加，安全计划中的对象有可能变得更加明确，从而导致检测精度的提高。但是，由于安全计划的巨大规模，转换到特定的DPI需要大量内存。因此，有必要设置适当的DPI以防止内存不足问题。\n\n> Next, a procedure for cropping the safety plan is required before inputting it into the object detection model. Excluding the target area, the safety plan consists of a table that indicates the location and quantity of objects, and title area. This part has the problem of false detection of an object, leading to an increased detection time. Therefore, specifying the detection area of the drawing can minimize the detection time when using an object detection model. \n\n然后，在将安全计划输入到目标检测模型之前，需要对其进行裁剪。除目标区域外，安全计划由一个表组成，该表表示物体的位置和数量，以及标题区域。这部分存在误检物体的问题，导致检测时间增加。因此，在使用物体检测模型时，指定图纸的检测区域可以最大限度地减少检测时间。\n\n### 2.3 Generation of training data for object detection （生成用于目标检测的训练数据）\n\n> In general, training a deep learning model requires enormous training data. However, the input safety plan is very large, and there are hundreds of objects that need to be detected in the safety plan. In addition, manually labeling more than 100 classes of objects to use as training data is time consuming and labor intensive. Therefore, a method for creating training data by randomly generating images using automated labeling was proposed in this study. \n\n一般来说，训练一个深度学习模型需要大量的训练数据。但是，输入安全计划非常大，安全计划中需要检测的对象有数百个。此外，手动标记100多个类别的对象作为训练数据是耗时和劳动密集型的。因此，本研究提出一种利用自动标记随机生成图像来生成训练数据的方法。\n\n> There are various methods for generating virtual data. One of them is using generative adversarial networks (GAN) (Goodfellow et al., 2014), which has been studied a lot recently to generate virtual images. GAN is a method of generating virtual data close to real data using two models of generator and discriminator. It has the advantage of diversifying data because it directly creates a virtual image rather than a combination of existing images. However, it was not suitable in this study because it was difﬁcult to specify the object’s location in the generated image. \n\n生成虚拟数据的方法有很多种。其中一种就是生成对抗网络(GAN) (Goodfellow et al.， 2014)，它是近年来被大量研究的虚拟图像生成方法。GAN是一种利用生成器和鉴别器两种模型生成接近真实数据的虚拟数据的方法。它具有数据多样化的优点，因为它直接创建一个虚拟映像，而不是现有映像的组合。然而，由于在生成的图像中很难确定目标的位置，因此不适用于本研究。\n\n> Furthermore, GAN had to be learned through a large amount of training data. Radford et al. (2016) conducted a study to generate a human face with a small size of 64px × 64px through deep convolutional GAN (DCGAN). They obtained 3 M images from 10 K people and used 350,000 face boxes to train the GAN. However, since the training image size was 416px × 416px and the number of safety plan data we had was only 15, we could not guarantee good results for generating training data using GAN. \n\n此外，GAN需要通过大量的训练数据来学习。Radford et al.(2016)研究了通过深度卷积GAN (DCGAN)生成大小为64px × 64px的人脸。他们从10 K个人那里获得了3 M张图像，并使用了35万个脸盒来训练GAN。但是由于训练图像的尺寸为416px × 416px，而我们拥有的安全计划数据数量只有15个，因此我们不能保证使用GAN生成训练数据的良好结果。\n\n> **Therefore, this study used a simple method to place the object on the image randomly. This method is easy to devise and has the advantage that the labeling data can be obtained together because we know the object’s location. However, since there are limitations in the generation method, it is essential to increase the diversity of data by intentionally inserting noise. This method was selected because of the advantage of quickly generating a lot of virtual images and annotation ﬁles. Fig. 4 shows the steps used to generate the training data. In the ﬁgure, a ‘base’ means the background of the training image, and a ‘material’ means the object to be placed on the base.** \n\n**因此，本研究采用了一种简单的方法，将物体随机放置在图像上。该方法设计简单，优点是可以同时获得标记数据，因为我们知道物体的位置。然而，由于生成方法的局限性，有必要通过故意插入噪声来增加数据的多样性。该方法具有快速生成大量虚拟图像和注释文件的优点，因此选择了该方法。图4显示了生成训练数据的步骤。在图中，“基础”指的是训练图像的背景，“材料”指的是要放在基础上的物体。**\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220914101803.png)\n\n> To achieve a high detection accuracy by applying the generated data to train the deep learning model, generating data similar to the actual environment was necessary. Therefore, to collect reference data for generation, labeling was conducted for at least one safety plan for each type of ship. Hence, 15 safety plans were labeled. Table 2 shows detailed information on the safety plan used as reference data. As described later in Section 2.5, the detection window size used in this study is 416px × 416px, which is small compared to the overall drawing. Since the image that enters the detection window is a tiny part of the drawing, it is the expression method of the object, not the line type, that is important for detection. Accordingly, labeling at least one per ship type is for the reference of the class, number of objects, and image size according to the ship type. \n\n为了将生成的数据用于训练深度学习模型以达到较高的检测精度，需要生成与实际环境相似的数据。因此，为收集生成的参考数据，对每一类型船舶至少进行一个安全方案的标注。因此，15个安全计划被标记。表2显示了作为参考数据的安全计划的详细信息。如后面2.5节所述，本研究中使用的检测窗口大小为416px × 416px，相对于整个绘图来说较小。由于进入检测窗口的图像是图纸的一个很小的部分，所以对于检测来说，重要的是物体的表达方法，而不是线的类型。相应地，对每个船舶类型至少标记一个，以便根据船舶类型参考类、对象数量和图像大小。\n\n> The next step was to extract the base and material from the labeled safety plan to generate the training data. An example of generating training data using base and material is shown in Fig. 5. \n\n下一步是从标记的安全计划中提取基础和材料，生成训练数据。图5所示为使用基础和材料生成训练数据的示例。\n\n> To obtain a base to place the training objects, an empty plan image, in which the part containing the object was removed from the labeling data, was extracted. For a generation, an image (416 px × 416 px) was randomly cropped from the object-removed plan image and used as a base (Fig. 5-(a)). \n\n为了获得放置训练对象的基础，提取了一幅空的平面图像，其中包含目标的部分从标注数据中去除。对于一代，图像(416px × 416px)从对象删除的平面图像中随机裁剪并用作基础(图5-(a))。\n\n> Each safety plan has a different set of object classes.  Even in the same class, expression of an object could be different, depending on the safety plan designer, as shown in Fig. 6;  hence, this should be considered when classifying the class of the object.  First, the classes were classiﬁed according to IMO classiﬁcations.  Although the object expression method is different for each plan, objects belonging to the same category were treated as the same class.  Images of objects existing in the safety plan were extracted and reclassiﬁed according to the IMO classiﬁcation.  This process was repeated for all labeled safety plans and used as the material (see Fig. 5-(b)).\n\n每个安全计划都有一组不同的对象类。即使在同一个类中，根据安全方案设计者的不同，物体的表达也可能不同，如图6所示;因此，在对对象分类时应该考虑到这一点。首先，按照国际海事组织(IMO)的分类方法进行了分类。虽然每个计划的对象表达方法不同，但属于同一类别的对象被视为同一个类。提取安全计划中存在的物体图像，并根据IMO分类方法进行重新分类。对所有已标记的安全计划重复此过程，并用作材料(见图5-(b))。\n\n> The next step is the merging of the base and material. The larger the input size used for training, the more information can be provided, which helps to improve the accuracy. However, over a speciﬁc size, the accuracy increase rate converges, and the complexity of the deep learning model increases, which extends the training time and hinders the detection speed. Therefore, we trained the object detection model by ﬁxing the input size to 416 px × 416 px. The base was ﬁxed to the corresponding size, and random rotation was added to diversify the data. This process can minimize the overﬁtting problem of the deep learning model for the training data. While arranging the materials at the base, random scaling of approximately 20% was applied to the material to ensure the robustness of the deep learning model. Finally, after randomly selecting the type, quantity, and position, the material was placed at the base to create an image for training (Fig. 5(c)). By repeating the previously mentioned process, a large amount of training data (training images and an annotation ﬁle) can be generated based on each arranged history. As shown in Fig. 7, the created image has an environment similar to that of the real safety plan. \n\n下一步是基础和材料的合并。用于训练的输入尺寸越大，可以提供的信息越多，有助于提高准确性。但是，在特定的规模上，精度增长率收敛，深度学习模型的复杂性增加，延长了训练时间，阻碍了检测速度。因此，我们通过固定输入大小为416 px × 416 px来训练目标检测模型。将基数固定到相应的大小，并加入随机旋转，使数据多样化。该过程可以最大限度地减少深度学习模型对训练数据的过拟合问题。在对基底材料进行排列时，对材料进行约20%的随机缩放，以保证深度学习模型的鲁棒性。最后，随机选择材料的类型、数量和位置后，将材料放置在基底上，生成用于训练的图像(图5(c))。通过重复前面提到的过程，可以根据每个排列的历史生成大量的训练数据(训练图像和注释文件)。如图7所示，创建的图像环境与真实的安全方案环境相似。\n\n> In general, when training a deep learning model, the labeled data are divided into training and validation sets. The training set is related to model training, such as updating weights, whereas the validation set is not involved in training and only used to observe the accuracy of the deep learning model for each epoch. However, because the data generated in this study were not actual data, there was a negligible correlation between the training inclination and detection accuracy. The loss for the training set continues to decrease as training continues without affecting the detection accuracy for the real safety plan. Fig. 8 shows the difference between the data distributions commonly used in training and those used in this study. Each deep learning model was trained for various epochs, and epochs showing high accuracy for each model were ﬁxed and used. \n\n一般来说，在训练深度学习模型时，标记数据被分为训练集和验证集。训练集与模型训练有关，如更新权值，而验证集不参与训练，只用于观察深度学习模型在每个epoch的准确性。但是，由于本研究生成的数据不是实际数据，训练倾角与检测精度之间的相关性可以忽略不计。随着训练的继续，训练集的损失会继续减少，而不会影响真实安全计划的检测精度。图8显示了训练中常用的数据分布与本研究中使用的数据分布的差异。每个深度学习模型对不同的epoch进行训练，固定每个模型精度较高的epoch并使用。\n\n### 2.4  Deep learning model for object detection in the safety plan （深度学习模型用于安全计划中的目标检测） \n\n> Objects in the safety plan represent various equipment or devices on the ship and follow the expression method recommended by the IMO regulations. However, depending on the safety plan designer, several differences exist in the expression, as shown in Fig. 6. The human eye can perceive the difference between these expressions; however, it is difﬁcult for computers to distinguish between these features. \n\n安全计划中的物体代表船舶上的各种设备或装置，并遵循国际海事组织规则推荐的表达方法。但是，根据安全方案设计者的不同，表达式存在若干差异，如图6所示。人类的眼睛可以感知这些表情之间的区别;然而，计算机很难区分这些特征。\n\n> Deep learning models are specialized in solving these problems because features are extracted from objects through training. In particular, the CNN model is considered promising to extract these features from images (Albawi et al., 2018). Therefore, even if an object is slightly transformed, it can be detected in the same class. This study aimed to classify hundreds of objects; hence, an object detection model based on the CNN model was used. \n\n深度学习模型专门解决这些问题，因为通过训练从对象中提取特征。特别是，CNN模型被认为有望从图像中提取这些特征(alawi等人，2018年)。因此，即使对一个对象进行了轻微的转换，也可以在同一个类中检测到它。这项研究旨在对数百个物体进行分类;因此，我们使用了一个基于CNN模型的目标检测模型。\n\n> The object detection model performs regional proposal, localization, and classiﬁcation steps to predict the location and class of an object, respectively. In this case, the object detection model is divided into the one-stage and two-stage detection models according to the two-step procedure, as shown in Fig. 9. \n\n目标检测模型分别执行区域建议、定位和分类步骤来预测目标的位置和类别。在此情况下，按照两步流程，将目标检测模型分为单步检测模型和两步检测模型，如图9所示。\n\n> The one-stage detection model is characterized by performing the classiﬁcation and localization steps simultaneously and has beneﬁt of a fast detection speed. Representative one-stage detection models include you only look once (YOLO) and single-shot multibox detector (SSD) (Liu et al., 2016). \n\n该单阶段检测模型具有分类和定位步骤同时进行的特点，具有检测速度快的优点。具有代表性的单级检测模型包括you only look once (YOLO)和单次多盒检测器(SSD) (Liu et al.， 2016)。\n\n> YOLO v3 (Redmon and Farhadi, 2017) is a well-known third version of one-stage YOLO object detection model. It is characterized using a search technique that divides images into grids, and locates and classiﬁes objects in each grid simultaneously to calculate the results. In the two-stage detection model, the regional proposal step that generates proposal boxes and the localization and classiﬁcation step that predicts the location and class of an object in the boxes are sequentially per-formed. However, the YOLO model is advantageous owing to its faster detection than other models by performing these two processes simultaneously and is mainly used for real-time detection of objects in an image. Therefore, in the ﬁeld of naval architecture and ocean engineering, this model is often used to detect and track ships in real-time (J. B. Lee et al., 2021). Because the safety plan has a large input size, the time required for detection can be reduced using the YOLO model, which requires relatively less time to detect.\n\nYOLO v3 (Redmon and Farhadi, 2017)是一种著名的单阶段YOLO对象检测模型的第三版。该算法采用一种搜索技术，将图像划分为网格，同时对每个网格中的对象进行定位和分类，计算结果。在两阶段检测模型中，依次执行生成提议框的区域提议步骤和预测框中对象位置和类别的定位分类步骤。而YOLO模型的优点是它可以同时执行这两个过程，比其他模型的检测速度更快，主要用于图像中目标的实时检测。因此，在造船和海洋工程领域，该模型经常被用于实时检测和跟踪船舶(J. B. Lee et al.， 2021)。由于安全方案的输入量较大，使用YOLO模型可以减少检测所需的时间，而YOLO模型检测所需的时间相对较少。\n\nThe RetinaNet model (Lin et al., 2020) is a one-stage detection model, similar to the YOLO v3 model, and is characterized by improving training efﬁciency through a new loss function that plays an important role in training. In particular, this loss function shows good training results for cases where the number of objects is small, and the distinction is evident compared with the background. The safety plan occupies a larger part of the background than the object to be detected; thus, using this model may reﬂect this environment well. \n\nRetinaNet模型(Lin et al.， 2020)是一种单阶段检测模型，类似于YOLO v3模型，其特点是通过一种新的损失函数提高训练效率，该函数在训练中发挥重要作用。特别是在目标数量较少的情况下，该损失函数表现出良好的训练效果，与背景的区别明显。安全方案在背景中所占的面积比待检测对象大;因此，使用该模型可以很好地反映这种环境。\n\n> The two-stage detection model provides a relatively high accuracy by the regional proposal step that proposes where to look. However, there is a tendency that it is slower than the one-stage detection model. Also, even if the accuracy was high for a speciﬁc dataset, there is no evidence that the model showed promising results for safety plans. Therefore, in this study, the two-stage detection model was applied to detect objects in the safety plan, and the results were compared with those of the one- stage detection model. \n\n两阶段检测模型通过提出查找位置的区域建议步骤提供了相对较高的精度。但是，有一种趋势是，它比单阶段检测模型慢。此外，即使对特定数据集的精度很高，也没有证据表明该模型在安全计划方面显示出良好的结果。因此，本研究采用两阶段检测模型对安全计划中的目标进行检测，并将结果与单阶段检测模型进行比较。\n\n> The Faster R–CNN model is a two-stage detection model with the region proposal network (RPN), as shown in Fig. 10 (Ren et al., 2017). However, it consumes more time for detection than that consumed by the one-stage detection model due to one additional step. Still, it shows a relatively higher accuracy than that of other models. In contrast to the previous two object detection models, the proposed method shows a higher detection accuracy. \n\nFaster R-CNN模型是一个带有区域建议网络(RPN)的两阶段检测模型，如图10 (Ren et al.， 2017)所示。但是，由于多了一个步骤，它比单阶段检测模型消耗更多的检测时间。尽管如此，它仍然比其他模型显示了相对更高的准确性。与前两种目标检测模型相比，该方法具有更高的检测精度。\n\n> In this study, YOLO v3, RetinaNet, and Faster R–CNN models were applied and analyzed for the object detection model. In a study similar to ours, Zhao et al. (2019) used various deep learning models. The results of testing them on representative datasets (VOC, COCO) can be seen in each table. We can see that the training data could change the superiority and inferiority of all deep learning models. This study focused on developing object detection methods within the ship safety plan and a training data generation model. Therefore, all models in this study used the models of the original research without changing the learning pa-rameters or other environments for comparison of the applied detection algorithm. \n\n本研究采用YOLO v3、RetinaNet和Faster R-CNN模型对目标检测模型进行分析。在一项与我们类似的研究中，Zhao等人(2019)使用了各种深度学习模型。在代表性数据集(VOC, COCO)上的测试结果可以在每个表中看到。可以看出，训练数据可以改变所有深度学习模型的优劣势。本研究着重于开发船舶安全计划内的目标检测方法及训练数据生成模型。因此，本研究中所有的模型都使用了原研究的模型，在不改变学习参数或其他环境的情况下，对所应用的检测算法进行比较。\n\n### 2.5  Detection algorithm for the safety plan \n\n> The safety plan has an enormous size compared with the target ob-ject. In addition, most of the plans are in PDF and consist of ship drawings with various elements (tables, headings, etc.). In general, there are approximately 60 classes and 800 objects in the safety plan on average, and the ratio of the target object to safety plan is relatively small (less than approximately 0.003%). Therefore, it requires a long time to train the object detection model; thus, the time required for detection increases signiﬁcantly. Therefore, to pass the safety plan through the deep learning model, conversion and specialized detection processes are required. \n\n与目标物体相比，安全方案的尺寸是巨大的。此外，大多数计划是PDF格式的，由各种元素(表格、标题等)的船舶图纸组成。一般而言，安全计划中平均约有60个类，800个对象，目标对象与安全计划的比例相对较小(小于约0.003%)。因此，训练目标检测模型需要较长的时间;因此，检测所需的时间大大增加。因此，要使安全方案通过深度学习模型，需要进行转换和专门的检测过程。\n\n> The safety plan converted to a resolution of 200 DPI varied from approximately 6600 px × 4500 px to 24,000 px × 7400 px depending on the ship type. This image was then provided as an input to the object detection model, where it was readjusted to ﬁt the size for training (416 px × 416 px). Consequently, most of the information in the safety plan was lost; hence, for the detection accuracy, we needed a suitable input method. \n\n安全计划转换为分辨率为200 DPI，根据船舶类型从大约6600px × 4500px到24000px × 7400px不等。然后将该图像作为对象检测模型的输入，在该模型中它被重新调整以适应训练的大小(416px × 416px)。因此，安全计划中的大部分信息都丢失了;因此，为了检测的准确性，我们需要一个合适的输入法。\n\n> The ﬁrst method was to divide the image equally into a size of 416px × 416px, as shown in Fig. 11 (a). The algorithm in this method is simple and the detection speed is fast. However, if an object exists on the dividing edge, it may be detected as two objects or may not be detected. \n\n第一种方法是将图像平均分割为416 px × 416 px大小，如图11 (a)所示。该方法算法简单，检测速度快。但是，如果一个对象存在于分割边缘上，它可能被检测为两个对象，也可能不被检测到。\n\n> The second method was to introduce the sliding-window algorithm consisting of a detection window with 416px × 416px that slides over the image. As shown in Fig. 11 (b), this window passes by sweeping the image at regular intervals. The region that enters the window is sequentially inputted into the object detection model. This algorithm increases the detection accuracy because the target object is uncropped. In contrast, when passing at regular intervals, the same object can be detected as different objects; hence, a speciﬁc algorithm is required to ﬁlter them into a single object. \n\n第二种方法是引入滑动窗口算法，由一个416px × 416px的检测窗口在图像上滑动组成。如图11 (b)所示，该窗口以规则间隔扫描图像通过。将进入窗口的区域依次输入到目标检测模型中。该算法提高了检测精度，因为目标对象是未裁剪的。相反，当以固定的间隔传递时，可以将相同的对象检测为不同的对象;因此，需要一个特定的算法来将它们过滤成单个对象。\n\n> We used the NMS algorithm when using the sliding-window algorithm to solve the problem of duplicated detection. The NMS algorithm determines the degree of overlap between each object and removes the remaining object with low reliability of overlapping. Here, reliability refers to the probability that the object existing in the box belongs to the correct class. To use this algorithm, the concept of the intersection of union (IoU) was proposed. The IoU, as shown in Eq. (1), is a measure of the degree of overlap between the bounding boxes of each object. \n\n在使用滑动窗口算法的同时，我们使用了NMS算法来解决重复检测的问题。NMS算法通过确定每个对象之间的重叠程度，去除重叠可靠性较低的剩余对象。这里，可靠性指的是存在于盒子中的对象属于正确类别的概率。为了利用该算法，提出了并集的交集(IoU)的概念。IoU，如公式(1)所示，是每个对象的边界框之间重叠程度的度量。\n\n> For instance, if two bounding boxes overlap entirely, the IoU equals to 1, and if there is no overlap, the IoU equals to 0. In this study, the case where the IoU was greater than 0.2, i.e., the overlapped part exceeded 20% of the region occupied by the two boxes, was identiﬁed as dupli-cated results. Since there is almost no overlap between objects in the safety plan, we decided to give a low threshold value. \n\n例如，如果两个边框完全重叠，则IoU等于1，如果没有重叠，则IoU等于0。在本研究中，当IoU大于0.2，即重叠部分超过两个盒子所占区域的20%时，被识别为重复结果。由于安全方案中物体之间几乎不存在重叠，所以我们决定给出一个较低的阈值。\n\n> Fig. 12 shows the process of applying the NMS algorithm to the safety plan. The NMS algorithm computed the IoU between each other for all the boxes of detection results. If the IoU between the two bounding boxes was greater than 0.2, the algorithm compared the reliabilities of the two boxes. Subsequently, the box with lower reliability was deleted. This process was performed for all bounding boxes in the image to obtain the detection results from which the duplicated objects were removed. \n\n图12给出了NMS算法在安全计划中的应用过程。NMS算法计算所有检测结果框之间的IoU。如果两个边界框之间的IoU大于0.2，则算法比较两个框的可靠性。随后删除了可靠性较低的框。该过程对图像中的所有边界框执行，以获得检测结果，从中去除重复的对象。\n\n## 3  Applications\n\n### 3.1 Comparison of the object detection models\n\n> Before developing the detection algorithm, we assessed the accuracy of the object detection models by creating an example to select the model to be used. The indicator for comparing the detection accuracy, called Prototype-1, consisted of ten images with 58 objects (10 classes), as shown in Fig. 13. The accuracy rates for object detection models are deﬁned in Eqs. (2)–(4), and the terms are deﬁned in Table 3. Recall refers to the portion of actual correct answers found out of all actual correct answers. On the other side, precision means the ratio of the actual correct answers among those detected as correct answers. Since the two rates tend to be opposite to each other, the F1-score is the harmonic average of the two. \n\n在开发检测算法之前，我们通过创建一个示例来选择要使用的模型来评估目标检测模型的准确性。比较检测精度的指标Prototype-1由10幅图像58个物体(10类)组成，如图13所示。在方程中定义了目标检测模型的准确率。(2) -(4)，术语定义见表3。回忆是指在所有实际正确答案中找出的实际正确答案的部分。另一方面，精确度是指实际正确答案与被检测为正确答案的答案的比率。由于两种比率往往是相反的，f1分数是两者的调和平均数。\n\n> As shown in Table 4, trained with 10,000 training data equally, the detection accuracy of the RetinaNet model was better than that of the YOLO v3 model; hence, an object detection algorithm was developed using the RetinaNet model. The speciﬁcations of the computer used in this study are listed in Table 5. \n\n如表4所示，平均10000个训练数据训练后，RetinaNet模型的检测精度优于YOLOv3模型;因此，我们开发了一种基于RetinaNet模型的目标检测算法。本研究使用的计算机规格如表5所示。\n\n> Because the safety plan was tens of times larger than the size of the detection input conducted in other relevant studies, a unique detection algorithm to deal with a large input size was required. Therefore, a detection algorithm that utilized the sliding-window algorithm was applied to the RetinaNet model. To use the sliding-window algorithm, the NMS algorithm for removing duplicated objects was subsequently added. To efﬁciently track the detection accuracy, the object detection model was evaluated using Prototype-2 (see Fig. 14). Prototype-2 was a bulk carrier safety plan consisting of 723 objects (83 classes), and an accuracy index, called the F1-score, was used. To train the RetinaNet model, 200,000 training data were created and the model with 83 classes was trained. \n\n由于安全方案是其他相关研究检测输入规模的数十倍，因此需要一种独特的检测算法来处理较大的输入规模。因此，将一种利用滑动窗算法的检测算法应用于retina模型。为了使用滑动窗口算法，随后增加了用于移除重复对象的NMS算法。为了高效跟踪检测精度，使用Prototype-2对目标检测模型进行评估(见图14)。原型2号是由723个物体(83个等级)组成的散货船安全计划，使用了被称为f1评分的准确性指标。为了训练RetinaNet模型，创建了20万个训练数据，训练了83个类的模型。\n\n> Table 6 shows that when the sliding-window algorithm was applied to the RetinaNet model, the recall was high. However, the number of false or duplicated detection results was more than six times that of the actual number of correct answers, resulting in very low precision. To overcome this problem, the NMS algorithm was applied to every detection window where objects were detected while moving around the image. We could adjust the IoU threshold of the NMS algorithm that RetinaNet owns, but in this study, we used a method of reapplying the NMS algorithm to use the original detection model as it is. Then, the NMS algorithm was used again for the entire results to reduce false detections. Furthermore, by adding a method to designate the detection area before the start, the required time and the number of misdetections could be reduced. \n\n从表6可以看出，将滑动窗口算法应用于视网膜anet模型时，查全率较高。但是，错误或重复的检测结果是实际正确答案数的6倍以上，准确度很低。为了克服这一问题，将NMS算法应用到每个检测窗口，在图像周围移动时检测到物体。我们可以调整RetinaNet所拥有的NMS算法的IoU阈值，但是在本研究中，我们采用了一种重新应用NMS算法的方法来使用原来的检测模型。然后，对整个结果再次使用NMS算法，以减少误检。此外，通过增加在开始前指定检测区域的方法，可以减少所需的时间和误检次数。\n\n> There are various objects in the safety plan, and there are cases that do not belong to the classes assigned by IMO classiﬁcations or do not follow the expression deﬁned by IMO. To ensure the robustness of the object detection model, it was necessary to deal with the objects belonging only to the IMO classiﬁcations. Therefore, to match the pre-viously labeled Prototype-2 to the IMO classiﬁcations (a total of 157 classes), the number of objects to be classiﬁed was reduced from 723 (83 classes) to 623 (59 classes). Subsequently, by generating 200,000 training data with IMO classiﬁcations and retraining the RetinaNet model, the results are shown in Table 7. As the number of classes to be identiﬁed increased, the F1-score slightly decreased to 0.63. \n\n安全计划中有各种各样的对象，有不属于国际海事组织(IMO)分类分配的类别或不遵循国际海事组织(IMO)定义的表达的情况。为了保证目标检测模型的鲁棒性，需要对只属于IMO分类的目标进行处理。因此，为了将之前标记的Prototype-2与IMO分类(共157类)相匹配，需要分类的对象数量从723(83类)减少到623(59类)。随后，通过IMO分类生成20万训练数据，再训练RetinaNet模型，结果如表7所示。随着需要识别的班级数量的增加，f1分数略有下降，为0.63分。\n\n> As mentioned earlier, the RetinaNet model was tested by adjusting the object detection algorithms, but still showed low detection accuracy for the safety plan. Therefore, the two-stage detection model, the Faster R–CNN model, was trained in the same environment and compared. As shown in Table 8, the time required for detection increased. However, the Faster R–CNN model showed better results in safety plan detection than the RetinaNet model. Therefore, the Faster R–CNN model was selected and joined with the object detection algorithm; consequently, a high F1-score was achieved for Prototype-2. \n\n如前所述，我们通过调整目标检测算法来测试RetinaNet模型，但对安全方案的检测精度仍然较低。因此，我们在相同的环境下训练两阶段检测模型Faster R-CNN模型并进行对比。如表8所示，检测所需时间增加。然而，Faster R-CNN模型在安全计划检测方面比视网膜网模型表现出更好的结果。因此，选择Faster R-CNN模型，并与目标检测算法相结合;因此，原型2获得了较高的f1分数。\n\n### 3.2  Variation in the training data \n> Through the analysis, an object detection model was developed using the Faster R–CNN model. To improve the detection accuracy, it was compared by modifying the parameters of the training data generation model. The training data generated according to each parameter are shown in Fig. 15. \n\n通过分析，建立了基于Faster R-CNN模型的目标检测模型。为了提高检测精度，通过修改训练数据生成模型的参数进行比较。根据各参数生成的训练数据如图15所示。\n\n> As shown in Table 9, when noise was added, the recall was slightly reduced compared with the case without noise. However, the precision increased signiﬁcantly. In addition, when scaling was applied to the material of the training data, it showed the best F1-score compared with the other parameters. Therefore, we inserted noise into the base and applied size scaling to the material during generation. \n\n如表9所示，当添加噪声时，召回率比不添加噪声时略有降低。然而，精确度显著提高。此外，当对训练数据的材料进行缩放时，与其他参数相比，f1得分最好。因此，我们在基底中插入噪声，并在生成过程中对材料进行尺寸缩放。\n\n> Next, the undetected object was analyzed to further improve the accuracy of the object detection model. In Fig. 16, the object was not detected owing to the different size of the object in the detection area and training data. When training the object detection model, large ob-jects were rarely detected because training data consisted of small ob-jects only. Therefore, the scaling ratio and primary size of object were increased (from 60 px ± 20% to 80 px± 30% px), and F1-score of 0.90 was achieved for object detection for Prototype-2. However, because this was an unreliable accuracy indicator owing to the use of a single safety plan, it was necessary to obtain the general accuracy for various safety plans. \n\n其次，对未检测到的目标进行分析，进一步提高目标检测模型的精度。在图16中，由于检测区域和训练数据中物体的大小不同，所以没有检测到目标。在训练对象检测模型时，由于训练数据中只有小对象，所以很少能检测到大对象。因此，提高了物体的缩放比和原始尺寸(从60 px±20%提高到80 px±30% px)， Prototype-2的物体检测f1得分为0.90。然而，由于使用单一的安全计划，这是一个不可靠的精度指标，因此有必要获得各种安全计划的一般精度。\n\n> Therefore, 15 types of safety plan data, listed in Table 2, were used to determine the accuracy of the object detection model. A few of the safety plans that were used are shown in Fig. 17. The accuracy of the object detection model was calculated using the average object detection ac-curacy for each safety plan. Although 15 plans may seem insufﬁcient, we can regard them as a large number of test data because the detection window size is 416 px × 416 px. From the point of view of the detection window, detection proceeds for 10,000 objects in about 30,000 images. \n\n因此，我们使用表2所示的15类安全计划数据来确定目标检测模型的准确性。一些使用的安全方案如图17所示。使用每个安全计划的平均目标检测精度来计算目标检测模型的精度。虽然15个方案看似不够，但由于检测窗口大小为416 px × 416 px，我们可以将其视为大量的测试数据。从检测窗口来看，大约3万张图像中有1万个物体进行检测。\n\n> When comparing the accuracy of the Faster R–CNN model using the aforementioned method, the average recall for the 15 safety plans was 0.55, as shown in Fig. 18 (a) and Table 10. It is noteworthy that the F1- score of some safety plans signiﬁcantly lowered the average recall. \n\n在使用上述方法比较Faster R-CNN模型的准确性时，15个安全方案的平均召回率为0.55，如图18 (a)和表10所示。值得注意的是，一些安全计划的F1-得分显著降低了平均召回率。\n\n> Therefore, we analyzed the factors that reduced the accuracy by comparing the detection results of the safety plans. Fig. 19 shows a photograph of the safety plan at the same magniﬁcation. It was observed that the size of the objects in the lowest three recalls is approximately twice as large. Because there were different sizes of objects in various types of plans, we needed to reﬂect this diversity in the training data. Thus, the size of the objects was applied more diversely than before, as shown in Fig. 18 (c). Consequently, the average recall increased from 0.55 to 0.67, as shown in Fig. 18 (b). However, as the size of the training object was diversiﬁed, the precision decreased drastically and the F1- score was lowered, as shown in Table 10. \n\n因此，我们通过比较安全方案的检测结果，分析了降低准确性的因素。图19显示了相同放大倍率下的安全方案照片。我们观察到，在最低的三次召回中，对象的大小大约是前者的两倍。因为在不同类型的计划中有不同大小的对象，我们需要在训练数据中反映这种多样性。因此，对训练对象大小的应用比以前更加多样化，如图18 (c)所示。因此，平均召回率从0.55增加到0.67，如图18 (b)所示。然而，随着训练对象大小的多样化，精度急剧下降，F1-得分下降，如表10所示。\n\n> The signiﬁcant improvement in recall for some safety plans showed that the object detection model was well trained. However, a signiﬁcant drop was observed in recall for other safety plans; this might be due to the characteristics of each safety plan. Therefore, we analyzed the size of the object in the detection window, as shown in Fig. 20. \n\n一些安全计划召回率的显著提高表明目标检测模型得到了很好的训练。然而，其他安全计划的召回量明显下降;这可能是由于每个安全计划的特点。因此，我们对检测窗口内物体的大小进行了分析，如图20所示。\n\n> Fig. 20 shows a comparison of the size of objects between the training data and detection area of the safety plan, which has the lowest F1-score. Although the size of objects in the training data was diversiﬁed as much as possible at the same magniﬁcation, a signiﬁcant difference in size was still observed. However, there was a limitation to diversifying object size because the probability of false detection increases (i.e., lowering the precision) as the sizes of more objects are reﬂected. Therefore, rather than modifying the generation model for the training data, predesignating the magniﬁcation of the detection window according to the safety plan could solve this problem. Because the average size of objects was different for each safety plan, the magniﬁcation of the detection window was adjusted by separately designating the average size of the object before starting object detection. With this modiﬁcation, the average accuracy was calculated again for these 15 safety plans. Consequently, the average recall signiﬁcantly increased from 0.67 to 0.85, as shown in Fig. 18 (d) and Table 10. It is noteworthy that the standard deviation of the recall decreased with the increasing material size distribution. However, precision tended to be lower than that of recall. This tendency was mostly caused by differences in object expression among the safety plans and diversity of object classes. \n\n图20是训练数据与安全方案检测区域的对象大小对比，其中f1得分最低的安全方案检测区域的对象大小比较。尽管在相同的放大倍率下，训练数据中物体的大小是尽可能多样化的，但仍然观察到显著的大小差异。然而，多样化的对象大小是有限制的，因为当更多的对象的大小被反映出来时，错误检测的概率会增加(即降低精度)。因此，与其修改训练数据的生成模型，不如根据安全计划预先指定检测窗口的放大倍数，可以解决这一问题。由于每个安全方案的物体平均大小不同，在开始物体检测之前，通过分别指定物体的平均大小来调整检测窗口的放大倍数。经过修改后，再次计算这15个安全方案的平均精度。因此，平均召回率从0.67显著提高到0.85，如图18 (d)和表10所示。值得注意的是，召回率的标准差随着物料粒度分布的增大而减小。然而，准确率往往低于召回率。这一趋势主要是由于安全方案之间对象表达的差异和对象类的多样性造成的。\n\n## 4  Conclusions and future works\n\nThis study proposed an object detection model for ship safety plans. By comparing the object detection models, the Faster R–CNN model that showed the best accuracy was selected. To detect an enormous area of a safety plan, we proposed a specialized object detection algorithm for safety plans by combining the sliding window and NMS algorithms. It is difﬁcult to obtain training data because labeling is time consuming and labor intensive. Therefore, we proposed a method that automatically generates numerous training data and trains the object detection model. For 15 actual safety plans, the average F1-score was 0.72. Because the model showed good performance even when using the generated training data, this study indicated that it can solve the insufﬁcient labeling data problem. In addition, we succeeded in ﬁnding more than 85% of the objects for two-thirds of the 15 safety plans. Therefore, it is expected that the method proposed in this study will assist in the efﬁcient inspection of safety plans. \n\n本文提出了一种船舶安全计划的目标检测模型。通过比较目标检测模型，选择了准确率最好的Faster R-CNN模型。为了检测安全计划的大面积区域，结合滑动窗口和NMS算法，提出了一种专门的安全计划对象检测算法。由于贴标签费时费力，训练数据难以获取。因此，我们提出了一种自动生成大量训练数据的方法，对目标检测模型进行训练。在15个实际安全计划中，f1得分平均为0.72。由于该模型在使用生成的训练数据时表现出良好的性能，本研究表明它可以解决标记数据不足的问题。此外，在15个安全方案中，我们成功地找到了超过85%的目标。因此，预计本研究中提出的方法将有助于有效地检查安全计划。\n\nIn the future, further research is recommended to obtain higher accuracy. Because the object detection model was trained only with the generated training data, the parameters of the generation model signiﬁcantly inﬂuenced the detection accuracy. Therefore, a quantitative comparison is recommended for each parameter. In the review task, ﬁnding the actual correct answer as many as possible is related to recall, so this study focused on improving recall performance. On the other hand, to increase precision related to the reduction of misrecognition, it is necessary to reﬂect various environments in the training data. The precision and F1-score may be increased through careful parameter control, such as by adjusting the size of the material. The average detection accuracy for the 15 safety plans was used as the accuracy indicator in this study; hence, it is necessary to increase the reliability of the accuracy indicator by labeling more safety plans. Also, because labeling quality differs according to the labeler, an additional review process is recommended for future studies. \n\n今后，建议进一步研究以获得更高的精度。由于目标检测模型只使用生成的训练数据进行训练，因此生成模型的参数对检测精度有显著影响。因此，建议对每个参数进行定量比较。在复习任务中，找出尽可能多的实际正确答案与回忆相关，因此本研究的重点是提高回忆性能。另一方面，为了提高与减少误识别相关的精度，需要在训练数据中反映各种环境。通过精心的参数控制，例如通过调整材料的尺寸，可以提高精度和f1分数。本研究以15种安全方案的平均检测精度作为准确性指标;因此，有必要通过标注更多的安全方案来增加精度指标的可靠性。此外，因为标签的质量根据标签商的不同而不同，建议对未来的研究进行额外的审查过程。\n\nBesides the training data generation model, the object detection method also needs improvement. This study used three well-known models (YOLO v3, RetinaNet, and Faster R–CNN) among the one-stage and two-stage detection models. Recently, models such as a fully convolutional one-stage (FCOS) object detection model (Tian et al., 2019) and YOLO v5 (Jocher et al., 2020) exceeded the performance of these models have emerged. Therefore, we plan to continuously improve detection accuracy and inference time by using the latest model above. Despite almost no overlap between objects in the safety plan, overlap still exists in the overall detection result because the sliding-window algorithm is applied. The NMS algorithm was used to delete the over-lapped low-conﬁdence bounding box. The Soft-NMS (Bodla et al., 2017) algorithm lowers the reliability of the corresponding box without removing the box. Therefore, it is expected to obtain better results when applied to further research. This study will be continuously improved by considering the parameters of the generation model and the latest detection methods. \n\n除了训练数据生成模型，目标检测方法也有待改进。在单阶段和两阶段检测模型中，本研究使用了三个比较知名的模型(YOLO v3、视网膜anet和Faster R-CNN)。最近出现了一些模型，如全卷积单阶段(FCOS)目标检测模型(Tian等人，2019)和YOLO v5 (Jocher等人，2020)，其性能超过了这些模型。因此，我们计划使用上述最新模型来不断提高检测精度和推断时间。虽然安全方案中的目标之间几乎没有重叠，但由于采用了滑动窗口算法，整体检测结果仍然存在重叠。采用NMS算法删除重叠的低置信度边界框。Soft-NMS (Bodla et al.， 2017)算法在不移除盒子的情况下降低了对应盒子的可靠性。因此，当它应用于进一步的研究时，有望获得更好的结果。本研究将结合生成模型的参数和最新的检测方法不断完善。\n","tags":["object detection","deep learning","symbol detection","ship safety plan"],"categories":["文献阅读"]},{"title":"Paper Translation 3|Multiple Wavelet Pooling for CNNs","url":"/2022/09/12/文献翻译3/","content":"\n# Multiple Wavelet Pooling for CNNs\n\n## Abstract\n\n池化层是任何卷积神经网络的重要组成部分。最流行的池化方法，如最大池化或平均池化，都是基于邻域方法，可能过于简单，容易造成视觉失真。为了解决这些问题，最近提出了一种基于Haar小波变换的池化方法。遵循同样的研究思路，在这项工作中，我们探索使用更复杂的小波变换(Coiflet, Daubechies)来执行池化。此外，考虑到小波与滤波器的工作原理类似，我们提出了一种结合多个小波变换的卷积神经网络池化方法。实验结果证明了我们的方法的优点，提高了在不同公共目标识别数据集上的性能。\n\n## 1 介绍\n\n神经网络作为深度学习的主要工具，在计算机科学史上可谓前前后后。池化层是卷积神经网络(cnn)的主要组成部分之一。它们旨在压缩信息，即减少数据维度和参数，从而提高计算效率。由于cnn处理的是整个图像，神经元的数量会增加，计算成本也会增加。因此，需要对数据和参数的大小进行某种控制。然而，这并不是使用池化方法的唯一原因，因为它们对于执行多级分析也非常重要。这意味着，我们查找的不是激活发生的确切像素，而是它所在的区域。池化方法从确定性的简单方法(如最大池化)到概率性的更复杂的方法(如随机池化)各不相同。所有这些方法的共同点是，它们使用邻域方法，尽管速度很快，但会引入边缘晕、模糊和混叠。具体来说，max pooling是一种通常有效的基本技术，但是可能太简单了，因为它忽略了只在激活映射上应用max操作的大量信息。另一方面，平均池化更能抵抗过拟合，但它会对某些数据集产生模糊效果。选择正确的池化方法是获得良好效果的关键。\n\n近年来，小波因不同的目的被纳入到深度学习框架中[4,3,8]，其中包括池化函数[8]。在[8]中，作者提出了一个池化函数，该函数根据快速小波变换(FWT)在小波域中进行二阶分解。作者证明了他们提出的方法优于或优于传统的池化方法。\n\n在本文中，受[8]的启发，我们探索了不同小波变换作为池化方法的应用，并在它们的最佳组合的基础上提出了一种新的池化方法。我们的工作与[8]的区别主要体现在三个方面:\n\n- 我们根据离散小波变换(DWT)在小波域进行一阶分解，因此，我们可以直接从low-low (LL)子带中提取图像\n- 我们探索不同的小波变换，而不是只使用Haar小波\n- 我们提出了一种基于不同小波变换组合的池化方法。\n\n文章的结构如下。在第2节中，我们介绍了多小波池化方法，在第3节中，我们介绍了数据集，实验设置，讨论结果和描述结论。\n\n## 2 多小波池化\n\n小波变换是数据的一种表示，类似于傅里叶变换，它允许我们压缩信息。给定光滑函数f(t)，连续情况定义为：\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220912173045.png)\n\n其中ψ(t)为母小波，s∈Z为尺度指标，l∈Z为位置指标。给定一个大小为(n, n, m)的图像A，可以通过构建矩阵来实现有限离散小波变换(DWT)，在[2]中解释如下:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220912173225.png)\n\n将原始图像A变换为4个子带:LL子带是由低频分量组成的低分辨率残差，这意味着它是我们原始图像的近似;子带HL、LH和HH分别给出了水平、垂直和对角线的细节。\n\n在本文中，我们提出通过组合不同的小波:Haar, Daubechie和Coiflet[1]小波来形成池化层。\n\n多小波池化算法如下:\n\n1. 选择两个不同的小波基，计算它们的相关矩阵W1和W2。\n\n2. 提出图像特征F，并行进行两个相关的离散小波变换W1FW1T和W2FW2T。\n\n3. 从每个矩阵中丢弃HL, LH, HH，从而只考虑两种不同基逼近的图像LL1和LL2。\n\n4. 连接两个结果并传递到下一层。\n\n在图1中，我们可以看到这个池化方法在CNN架构中是如何工作的。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220912173657.png)\n\n## 3 实验结果与结论\n\n我们使用了三个不同的数据集进行测试:MNIST[6]、CIFAR-10[5]和SVHN[7]。为了比较收敛性，我们使用了分类熵损失函数;作为度量标准，我们使用精度。对于MNIST数据集，我们使用了600批大小，我们执行了20个epoch，我们使用了0.01的学习率。对于CIFAR-10数据集，我们执行了两个不同的实验:一个没有dropout，有45个epoch，另一个有dropout，有75个epoch。对于这两种情况，我们都使用动态学习率。对于SVHN数据集，我们进行了一组具有45个epoch和动态学习率的实验。所有的CNN结构都是从各自数据集的[8]获取的。在这种情况下，我们测试没有dropout的算法，以观察池化方法对过拟合的阻力。只有在CIFAR-10的情况下，我们考虑了有退出和没有退出的性能。\n\n表1显示了每种池化方法的准确度及其在排名中的位置;此外，我们用粗体突出显示每个数据集的最佳性能。当我们用dropout进行模型训练时，我们将用“d”表示情况。对于MNIST数据集，与Haar基相比，选择Daubechie基提高了准确性。对于CIFAR-10和SVHN，我们可以看到多小波池化的性能是均匀的，甚至优于max和average池化。特别是对于dropout的情况，多小波池化算法优于其他所有池化算法。","tags":["Wavelet Pooling"],"categories":["文献阅读"]},{"title":"Paper Translation 2|Meta-Learining With Graph Neural Networks：Methods and Applications detectors","url":"/2022/09/03/文献翻译2/","content":"\n# Meta-Learining With Graph Neural Networks：Methods and Applications\n\n## Abstract\n\n图神经网络(GNNs)是一种基于图数据的深度神经网络，已被广泛应用于从药物发现到推荐系统等各个领域。然而，当可用样本很少时，此类应用程序上的gnn是有限的。元学习是解决机器学习中缺乏样本的一个重要框架，近年来，研究人员开始将元学习应用于gnn。在这项工作中，我们提供了一个全面的调查不同的元学习方法涉及gnn在各种图问题上，展示了这两种方法一起使用的力量。我们根据提出的架构、共享表示和应用程序对文献进行分类。最后，我们讨论了几个令人振奋的未来研究方向和有待解决的问题。\n\n## 1 介绍\n\n人工智能(AI)和机器学习的方法在各种应用中都取得了巨大的成功，从自然语言处理[Dev+19]到癌症筛查[Wu+19]。人工智能系统的成功可以归因于各种架构创新，以及深度神经网络(DNN)从欧几里得数据(如图像、视频等)提取有意义的表示的能力。然而，在许多应用程序中，数据是图结构的。例如，在药物发现中，目标是预测给定的分子是否为新药的潜在候选分子，其中输入的分子用图形表示。在推荐系统中，用户和商品之间的交互是用图形表示的，而这种非欧几里得数据对于设计一个更好的系统至关重要。\n\n图结构数据在各种应用中的激增导致了图神经网络(gnn)，它是DNN在图结构输入中的推广。gnn的主要目标是学习图的有效表示。这种表示将顶点、边和/或图映射到一个低维空间，因此图中的结构关系通过表示中的几何关系反映出来[HYL17b]。近年来，gnn被应用于不同的领域，经常有令人惊讶的积极结果，如发现一种新的抗生素[Sto+20]，准确的交通预测[Cui+19]等。\n\n尽管最近GNN在各个领域都取得了成功，但GNN框架也有其不足之处。应用gnn的主要挑战之一，特别是对于大型图结构数据集，是样本数量有限。此外，像推荐系统这样的真实系统通常需要处理不同类型的问题，并且必须在很少观察到的情况下适应新问题。近年来，元学习被证明是解决深度学习系统这些缺点的一个重要框架。元学习背后的主要思想是设计可以利用之前的学习经验快速适应新问题的学习算法，并在很少的样本中学习有用的算法。这种方法在各种应用中都非常成功，如自然语言处理[Liu+19]、机器人[Nag+20]和医疗[cha +19]。\n\n近年来，针对不同的应用提出了多种训练gnn的元学习方法。将元学习应用于图结构数据有两个主要的挑战。首先，一个重要的挑战是确定不同任务共享的表示类型。由于gnn用于从节点分类到图分类的广泛任务，学习到的共享表示需要考虑要解决的任务类型，这使得元学习的体系结构选择和设计非常重要。第二，在多任务设置中，我们通常从每个任务中获得很少的样本。因此，就相似性而言，支持和查询示例的重叠通常有限。例如，在节点分类任务中，节点对给定任务的支持和查询集很少相似。另一方面，在链接预测中，支持边和查询边在图中往往相距较远。因此，将元学习应用于gnn的一个主要挑战是对图中相距很远(包括距离和相似性)的节点(或边)之间的依赖关系建模。在这项调查中，我们回顾了不断增长的文献与gnn的元学习。对gnn [Zho+18;Wu+20]和元学习[Hos+20]，但我们认为这是第一次对现有的gnn元学习文献进行分类和全面回顾。\n\n### 1.1 我们的贡献\n\n除了提供基于gnn的元学习和架构的背景知识外，我们的主要贡献可以总结如下。\n\n- 综合回顾:我们提供了一个综合回顾的元学习技术与gnn的几个图问题。我们根据方法、表示和应用对文献进行分类，并展示了通过元学习解决gnn局限性的各种场景。\n\n- 未来方向:我们讨论了元学习和gnn如何解决几个领域的一些挑战:\n  (i)组合图问题，\n  (ii)图挖掘问题，\n  (iii)其他新兴应用，如交通流预测，分子特性预测和网络对齐。\n\n本文的其余部分组织如下。第2节提供了几个关键图神经网络架构的背景。第三部分概述了元学习的背景和主要理论进展。第4节和第5节描述了在重要图相关问题上使用带有gnn的元学习框架的论文的综合分类。首先，第4节涵盖了元学习框架在解决一些经典图问题中的应用。这里讨论的问题并不是明确提出多任务设置，而是将元学习框架应用于固定图。在第5节中，我们介绍了图元学习的相关文献，当有多个任务时，图可能会随着任务的变化而变化。虽然已经提出了各种各样的gnn用于图元学习，但它们可以根据共享表示的类型进行广泛的分类，这可以是在局部级别(基于节点/边缘)或在全局级别(基于图)。表1提供了按共享表示和应用程序域类型分类的各种论文的概述。表2给出了第5节中基于相应元学习方法所描述的论文。第6节涵盖了gnn上元学习的广泛应用，第7节提出了一些令人兴奋的未来方向。\n\n## 2 图神经网络\n\n在图上推广深度学习已经成为图神经网络(GNNs)的一个令人兴奋的领域。gnn利用来自节点邻域和节点本身的结构和属性信息，将节点嵌入或表示为向量空间中的点。它们通过非线性转换和聚合函数将这些信息编码成最终的表示。提出的架构可以大致分为两类:(i)邻域卷积，(ii)位置感知。\n\n(i)邻域卷积:基于邻域卷积的架构的主要例子包括GCN [KW17]、GRAPHSAGE [HYL17a]和GAT [Vel+18]。这些架构主要通过对其邻域的卷积操作来创建节点的表示，即嵌入，zv,G = NGk (v)\n\n其中，图G中节点v的(k-hop)邻域(节点集)为NGk (v)。因此，邻域相似的两个节点可能具有相似的嵌入。\n\n(ii)位置感知:位置感知框架的gnn示例包括PGNN [YYL19]和GRAPHREACH [Nis+21]。在这种方法中，如果图中两个节点的位置接近(通常是通过跳数)，那么它们应该具有相似的嵌入。如果图具有较高的聚类系数，那么一个节点的单跳邻居之间也会共享许多其他的邻居。因此，如果两个节点距离很近，它们就很有可能存在相似的邻域。许多实图具有小世界和无标度特性，聚类系数高。接下来，我们将简要介绍gnn的关键架构。\n\nGCN [KW17]: [KW17]通过引入图卷积网络(GCNs)，在图上应用神经架构方面做出了主要贡献。GCNs是图上卷积神经网络(CNNs)的类似版本。图卷积的灵感来自于用来自其附近像素的信息来表示一个像素(cnn中的过滤器)，它还应用了从节点的局部邻域聚合特征信息的关键思想。更正式地说，GCNs是产生d维嵌入的神经网络架构\n\n每个节点通过取邻接矩阵A和节点特征X作为输入;GCN(A, X): Rn×n × Rn×p→Rn×d。其思想是从一个节点的邻域(可以泛化为多个跃点)和它自己的特征中聚合特征信息，以产生最终的嵌入。一个2层(邻居为2跳)的GCN可以定义如下:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903160128.png)\n\nGRAPHSAGE [HYL17a]: Hamilton等人[HYL17a]提出了一个归纳框架，该框架具有一个聚合函数，能够跨节点共享权重参数(Wk)，可以推广到不可见的节点，并扩展到大型数据集。为了学习节点v的表示hvk，它遍历所有在其K-hop邻域内的节点。当在节点v上迭代时，它聚合(使用AGGREGATEk) v的邻居的当前表示(hk (v))\n\nGAT [Vel+18]:图注意网络(GATs) [Vel+18]使用注意机制学习边缘权值。与GRAPHSAGE [HYL17a]不同，GAT不假设相邻节点的贡献都相等。GAT学习两个连接节点之间的相对重要性/权重。图卷积运算(第k次迭代)定义如下:\n\n其中αv,u衡量节点v与其邻居u∈N(v)之间的强度。GAT在基准数据集中的转换和归纳设置的节点分类任务中表现优于GCN和GRAPHSAGE。\n\nPGNN [YYL19]:与GRAPHSAGE不同，在GRAPHSAGE中，节点的表示依赖于它的k-hop邻域，PGNN遵循不同的范式，旨在合并一个节点相对于整个网络中的节点的位置信息。其核心思想是，通过量化节点与一组锚节点之间的距离，通过低失真嵌入来获取节点的位置。该框架首先对多组锚节点进行采样。它还学习了一种非线性的聚合方案来组合每个锚集中节点的特征。聚合通过节点与锚集之间的距离进行规范化。\n\n其他变体:基于不同机制，gnn还有其他几种变体和改进:门控注意网络(GAAN) [Zha+18]通过一种自我注意机制进一步扩展了GAT，该机制为每个注意头计算额外的注意分数。图Autoencoders [CLX16;KW16]将节点/图编码到一个潜在的向量空间中，并根据应用程序以无监督的方式从这种编码中进一步重构图相关的数据;复发性卫星系统(进行Sca + 08年;Li+16]在节点上反复应用相同的一组参数来提取高级节点表示。关于gnn的全面调查，请参考[Wu+20]。\n\n### 2.1 应用\n\n对于图上的半监督学习任务(如节点分类)，gnn优于传统方法。gnn的高层应用主要有三个方面:节点分类、链路预测和图分类。对于节点分类和链接预测任务，传统上使用四个基准数据集:Cora、Citeseer、Pubmed和蛋白质-蛋白质相互作用(PPI)数据集。Shchur等人[Shc+18]和Errica等人[Err+19]提供了关键架构在节点和图分类任务上性能的详细比较。在链接预测任务中也使用gnn，该任务在朋友或电影推荐、知识图补全、代谢网络重构等多个领域都有应用[ZC18]。\n\n## 3 元学习的背景\n\n元学习已成为解决各种机器学习应用中数据有限问题的重要框架。元学习背后的主要思想是设计可以利用之前的学习经验快速适应新问题的学习算法，并在很少的样本中学习有用的算法[Sch87]。这种方法在各种应用中都非常成功，如自然语言处理[Liu+19]、机器人[Nag+20]和医疗[cha +19]。\n\n### 3.1 框架\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903160644.png)\n\n### 3.2 训练\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903160720.png)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903160746.png)\n\n### 3.3 表示学习\n\n元学习的另一个视角是学习跨不同任务的共享表示，这对图神经网络的环境尤为重要。这里我们假设，给定一个输入x，从第t项任务的训练数据产生为yt = ft◦h(x) + ηt，其中ηt是一些iid噪声。函数h将输入x映射到一个共享表示，然后应用一个特定于任务的函数ft来生成特定于任务的表示。\n\n在元训练阶段，我们尝试学习共享函数h。假设我们给定T个数据集Dt ={(xti, yti}int=1，当t = 1时，…, T。然后我们求解下面的优化问题来恢复h。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903161022.png)\n\n我们现在实例化这个框架，用于对图的节点进行分类。与之前一样，我们使用两层GCN，其中模型定义在公式(2)中。然而，我们现在假设第一层跨不同的任务共享，只有第二层为新任务进行训练。特别地，我们假设Wt = [W;Wt(2)]。虽然，式(3)中的优化问题是NP-hard解决这种特殊类型的表示，我们可以写下一个算法来解决元参数W使用梯度下降。算法2描述了这个算法，并返回元参数W。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903161423.png)\n\n### 理论\n\n尽管取得了巨大的成功，但我们还没有完全理解元学习算法的理论基础。Baxter [Bax00]首先证明了多任务学习问题的泛化界，通过考虑一个从生成式模型中采样具有共享表示的任务的模型。Pontil等人[PM13]和Maurer等人[MPRP16]开发了基于统一收敛的通用框架来分析多任务表示学习。然而，他们假设甲骨文获得了一个全球经验风险最小化。最近，从表征学习来理解元学习已经有了一些很有希望的尝试。主要思想是任务共享一个公共的共享表示和一个特定于任务的表示[TJJ20b;TJJ20a;杜+ 20)。如果从训练任务中学习到共享表示，那么只需要几个样本就可以学习到新任务的特定任务表示。最后，最近有一些有趣的研究试图理解基于梯度的元学习。[Fin+19; BKT19; KBT19; Den+19]在在线凸优化(OCO)框架下分析了基于梯度的元学习。它们假设任务的参数接近于OCO框架中绑定后悔的共享参数。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903161619.png)\n\n表1:基于应用和底层图相关表示的元学习和gnn论文的组织。框架(方法)的缩写如下。GPN:图原型网络，MetaR:元关系学习，GEN:图外推网络，RALE:相对和绝对位置em -床上，am - gnn:属性匹配元学习图神经网络，SAME:单任务适应多任务嵌入，SELAR:自监督辅助学习，GFL:图少镜头学习，GDN:图偏差网络，MI-GNN:图神经网络元归纳框架，AS-MAML:自适应步骤模型不确定元学习。\n\n## 4 关于固定图的元学习\n\n在本节中，我们回顾了元学习在解决图上一些经典问题中的应用。这里我们考虑当底层图是固定的，节点/边缘特征不随不同任务而改变时的设置。事实上，我们并不是在一个多任务框架中，在这个框架中有很多任务，每个任务中只有很少的示例。相反，元学习的框架被应用于各种图问题，通过创建多个任务，要么考虑节点，要么考虑边。\n\n### 4.1节点嵌入\n\n节点嵌入的目标是学习图中节点的表示，这样任何下游应用程序都可以直接使用这些表示，而不需要考虑原始图。这个问题在实际应用中常常具有挑战性，因为大多数图的度分布都遵循幂律分布，且节点很多，连接很少。Liu等人[Liu+20]通过将元学习应用于图的节点嵌入问题来解决这个问题。他们建立了一个具有公共先验的回归问题来学习节点嵌入。由于高阶节点的基本表示是准确的，所以将它们作为元训练集来学习公共先验。低阶节点只有少数几个邻居(样本)，学习它们表示的回归问题被表述为元测试问题，公共先验采用少量样本学习此类节点的嵌入。\n\n### 4.2节点分类\n\n节点分类任务的目的是推断给定部分标记图中节点的缺失标签。这个问题经常出现在文档分类和蛋白质分类等不同的环境中[Tan+08;Bor+05]，近年来受到了极大的关注。然而，通常许多类是新颖的，即它们有少量标记节点。这使得元学习或少镜头学习特别适合这个问题。\n\nZhou等人[Zhou +19]已经将元学习框架应用于图上的节点分类问题，通过使用来自有许多标记示例的类的数据学习可转移表示。然后，在元测试阶段，这个共享表示用于对带有少量标记样本的新类进行预测。Ding等人[Din+20]改进了之前的方法，考虑了每个类的原型表示，并将原型表示作为每个类加权表示的平均值进行元学习。Lan等人[Lan+20]通过元学习解决了同样的问题，但在不同的设置中，节点没有属性。他们的方法只使用图结构来获得任务节点的潜在表示。随后，Liu等人[Liu+21]指出，学习任务中节点之间的依赖关系也很重要，并建议使用中心性得分高的节点(或hub节点)来更新GNN学习到的表示。这是通过选择一个小型集线器节点集，并对每个节点v，考虑从集线器节点集到节点v的所有路径来实现的。它有助于对图中的绝对位置进行编码。与这些发展并行的是，Yao等人[Yao+20]考虑了一种基于度量学习的方法，其中节点的标签被预测为可转移度量空间中最近的类原型。他们首先使用GNN学习特定于类的表示，然后使用层次图表示学习特定于任务的表示。\n\n最后，在不同任务支持集中存在噪声或不准确标签的情况下，也使用了少镜头节点分类任务。Ding等人[Din+21b]提出了一种方法(Graph Hallucination Network)，通过从一个类中提取指定数量的样本来创建一个集合。然后，该方法学习对集合中每个节点的标签的准确性产生一个置信度得分。通过使用这些权重/分数，最终生成更清晰(即，噪声更小)的节点表示。算法的其余部分遵循标准的MAML框架。\n\n### 4.3 链接预测\n\n链路预测问题的目标是识别将形成或不形成链路的节点对。元学习已经被证明可以通过边/链接学习新的关系，特别是在多关系图中。\n\n在多关系图中，一条边由三个端点和一个关系表示。这样的图形出现在许多重要的领域，如药物-药物相互作用预测。在多关系图中，链接预测的目标是在给定关系r的一个端点的情况下，通过观察r周围的几个三元组来预测新的三元组。这是一个具有挑战性的问题，因为通常只有少数几个三元组可用。Chen等[Che+19]用元学习分两步解决链路预测问题。首先，他们设计了一个关系元学习者，它学习跨多个关系的共享结构。这样的元学习者从支持集中的正面和反面嵌入产生关系元。其次，它们使用嵌入学习器，通过端点的嵌入和关系元计算支持集中三元组的真值。\n\n随着时间的推移，多关系图由于其动态特性(添加新节点)而更难管理，而当这些新进化的节点之间只有很少的链接时，学习就更加困难了。Baek等人[BLH20]介绍了一种少镜头的图外链接预测技术，他们预测了可见节点和未见节点之间以及未见节点之间的链接。其主要思想是将给定图中的实体随机分成模拟不可见实体的元训练集和真实不可见实体的元测试集。\n\n最后，Hwang等人[Hwa+21]通过结合元学习的自我监督辅助学习框架，展示了图神经网络对下游任务(如节点分类和链接预测)的有效性。辅助任务如元路径预测不需要标签，因此该方法成为自我监督的方法。在元学习框架中，使用各种辅助任务来提高底层主任务(如链路预测)的泛化性能。该方法有效地组合了辅助任务，并自动平衡它们，提高了主要任务的性能。该方法还可以灵活地处理任何图形神经网络结构，而无需额外的数据。\n\n## 5 图神经网络的元学习\n\n我们现在讨论的是关于图元学习的日益增长和令人兴奋的文献，其中有多个任务，底层图可以在任务之间改变。当节点/边缘特征发生变化，或者底层网络结构随着任务发生变化时，图就会发生变化。在元学习的背景下，近年来提出了几种架构。然而，它们底层的公共线程是图的共享表示，可以是本地节点/边级别的，也可以是全局图级别的。基于共享表现的类型，我们将现有作品分为两类。现有文献大多采用MAML算法[FAL17]对提出的gnn进行训练。MAML的外层循环更新共享参数，而内层循环更新当前任务的特定于任务的参数。表2列出了本节中所有论文的共享参数和特定于任务的参数。\n\n### 5.1节点/边缘级共享表示\n\n首先，我们考虑本地共享表示的设置，即基于节点或基于边。Huang等人[HZ20]考虑了节点分类问题，即不同任务的输入图和标签可能不同。他们分两步学习每个结点u的表示。首先，该方法提取节点集{v: d(u, v)≤h}对应的子图Su，其中d(u, v)是节点u和v之间最短路径的距离，然后通过GCN给子图Su学习节点u的表示。考虑图Su的理论动机是节点v对u的影响随着它们之间的最短路径距离的增加而指数下降。一旦对节点进行编码，就可以学习到将编码映射到类标签的任何函数fθ。Huang等人[HZ20]使用MAML在一个新任务上用很少的样本学习这个函数，享受了节点分类中节点级共享表示的好处。\n\nWang等[Wan+20]也考虑了网络结构固定，但节点特征随任务变化的节点分类问题。特别是给定一个带有节点特征矩阵的基图\n\nX∈Rn×d，提出的模型学习第t个任务的新特征矩阵Xt = Xαt(φ) + βt(φ)，然后使用GNN fθ(Xt)学习第t个任务的节点表示。训练时，外环更新φ参数，而MAML内环只更新θ-参数。这样可以快速适应新任务。\n\nWen等人[WFL21]研究了归纳设置下的节点分类问题，其中测试和训练中的图实例不重叠。他们的方法包括使用多层感知器(MLP)在给出一个图(即它的表示)之前计算一个任务。这些表示对于图级适配非常有用。他们在他们的方法中使用了传统的MAML范式来进行任务级适应。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220903162433.png)\n\n### 5.2 图级共享表示\n\n在本小节中，我们讨论了全局共享表示即图级共享表示时的设置。这种表示的一个典型应用是图分类问题，其目标是将给定的图分类为许多可能的类中的一个。从生物信息学到社会网络分析，很多应用中都出现了这个问题[YV15]。然而，在很多情况下，用于特定任务的样本/图的数量很少，图分类任务往往需要大量的样本来进行高质量的预测。这些挑战可以通过元学习来解决。现有的使用元学习进行图分类的论文通常学习底层的共享表示，并将其用于新任务。\n\nChauhan等人[CNK20]提出了基于图谱测度的少镜头图分类任务。特别是，他们训练一个特征提取器Fθ(·)来从元训练中的图中提取特征。在分类方面，他们首先使用单位Csup来预测一个图的超类概率，该图是一个丰富基类标签的聚类。然后他们使用Catt，一个注意力网络来预测实际的阶级标签。在元测试阶段，固定网络Fθ(·)和Csup的权重，并在新的测试类上对网络Catt进行再训练。由于特征提取器Fθ是公共共享结构，且不需要在测试任务上进行再训练，因此该方法对新类的样本要求很少。\n\n尽管Chauhan等人[CNK20]提出了一种新颖的图分类元学习体系结构，但存在一些局限性。首先，该体系结构假设测试的超类结构和训练集之间存在显著的重叠。其次，固定特征提取器无法针对新任务进行更新。Ma等人[Ma+20]设计了一种更好的元学习技术，允许特征提取器有效地适应新任务。他们使用两个网络-嵌入层(θe)，然后是分类层(θc)来对给定的图进行分类。但是，对于一个新任务，θe和θc都被更新。特别是，作者使用MAML [FAL17]来更新参数，并使用基于强化学习的控制器来确定内环是如何运行的，即，什么是一个新任务的最佳适应步骤。利用图的嵌入质量和元学习者的训练状态更新控制器的参数。\n\nJiang等人[Jia+21]通过一种不同于mml的元学习中的度量学习方法[Wan+19]解决了少镜头图分类问题。在训练阶段，我们的想法是获得支持集中每个类中实例的平均表示。查询的预测是基于最近的邻居。\n\n这里的图表示是通过图同构网络(GIN)模型得到的。为了获取图的全局结构，他们在最终的聚合方案中对不同的GIN层使用不同的权值。为了对决定图标签的关键局部结构进行编码，本文嵌入了子图，并包含了它们具有不同注意权值的表示。\n\n最后，Buffelli等人[BV20]试图开发一个可以适应三种不同任务的框架——图分类、节点分类和链路预测。像[CNK20;Ma+20]它们使用两个不同的层;一个是生成节点嵌入并将图形转换为表示，另一个是用于三种类型任务的多头输出层。在mml初始化阶段训练节点嵌入层，并根据任务类型在mml内环中更新多头输出层。\n\nBose等人[Bos+19]考虑了少镜头链接预测问题，其目标是预测仅包含其真实标签的一小部分链接/边的标签。他们假设图是由一个公共分布p(·)生成的，并学习了一个可以快速适应新的图G ~ p(·)的元链接预测模型。特别地，作者使用变分图自动编码器(VGAE) [KW16]来建模基本链路预测模型。有两组参数:VGAE的全局初始化参数和本地图签名sG = (G)，该签名是通过GCN传递图G，然后使用k层MLP获得的。训练是使用mml完成的，并且只更新测试图的图签名。\n\n## 6 其他应用程序\n\n我们讨论了带有gnn的元学习在节点分类、链路预测和图分类方面的应用。事实上，这个框架是相当普遍的，可以应用于许多其他相关的重要问题。\n\n异常检测:异常检测的问题往往是由于标签的稀缺，为异常获取标签通常是劳动密集型的。Ding等[Din+21a]研究了标签稀缺时的异常检测，不同的任务涉及不同的图。该方法利用传统的gnn结构嵌入节点，并在得到嵌入后再增加一层来预测异常评分。最后利用传统的mml框架部署元学习者。内部循环优化特定任务的参数，例如graph。外环优化所有图的通用参数。\n\n网络对齐(Network Alignment, NA):网络对齐的目的是映射或链接来自不同网络的实体，并在许多应用领域相关，如跨域推荐和广告。Zhou等人[Zhou +20]通过元学习解决了这个对齐问题。如果两个不同的网络共享一些公共节点或锚点，那么这些网络是部分对齐网络。两个锚点之间的虚拟链接称为锚点链接。在NA中，给定一组网络和一些已知的锚节点(或链接)，目标是识别所有其他(未知的)潜在锚节点(或链接)。[Zho+20]的主要思想是将该问题框架为一枪分类问题，利用已知锚节点的元度量学习获得连接未知锚节点的潜在先验。\n\n流量预测:最近，流量预测问题[Pan+20]通过元学习得到了解决。在交通预测中，主要的挑战是对复杂的交通时空相关性进行建模，并获取这些相关性随位置变化的多样性。Pan等人[Pan+20]通过基于元学习的模型解决了这些挑战。他们的方法可以同时预测所有地点的交通状况。该框架由一个序列对序列的架构组成，该架构使用一个编码器来学习交通历史，并使用一个解码器来进行预测。对于编码器和解码器组件，使用图注意网络和递归神经网络的组合来分别建模不同的空间和时间相关性。\n\n## 7 未来的发展方向\n\n在图形特定应用中使用gnn进行元学习的应用是一个不断增长和令人兴奋的研究领域。在本节中，我们提出了几个未来的研究方向。\n\n### 7.1 图上的组合优化问题\n\n图中出现的组合优化问题在许多领域都有应用，如社交网络中的病毒营销[KKT03]，医疗[Wil+18]，基础设施开发[Med+18]，并提出了几种基于gnn的架构来解决这些问题[Dai+17;LCK18;气体+ 19;男人+ 20)。这些优化问题通常是np困难的，多项式时间算法，无论是否有近似保证，往往是可取的，并在实践中使用。然而，一些技术[LCK18;基于gnn的Man+20]需要在生成实际解集之前生成候选解节点/边。请注意，这些问题的解决方案集中每个节点的重要性形式的标签通常很难得到。元学习可以在标签稀缺的情况下使用。此外，这些组合问题通常具有相似的结构。例如，影响最大化问题[KKT03]与最大覆盖问题有相似之处。然而，即使是执行贪心迭代算法来为影响最大化问题生成解/标签，计算成本也很高。使用元学习来解决具有更少节点标签的更难的组合问题(看不见的任务)的想法将在更容易的问题(看不见的任务)上学习，在那里标签可以以更低的成本产生。最近，通过神经方法解决图上的组合优化问题获得了很多关注，我们建议读者参考[Cap+21]进行进一步阅读。\n\n###  7.2 图挖掘问题\n\n最近有人尝试用gnn来解决经典的图挖掘问题。例如，一个流行的问题是学习两个图之间的相似度，即找到两个图之间的图编辑距离(相似度)[Bai+19]。当相似度的概念改变，没有足够的数据来通过标准的监督学习方法学习时，元学习是否有帮助?另一个流行的图挖掘问题是检测两个输入图之间的最大公共子图(MCS)，应用于生物医学分析和恶意软件检测。在药物设计中，化合物中常见的子结构可以减少人体实验的次数。然而，MCS计算是np困难的，最先进的精确MCS求解器不能扩展到大型图。为MCS问题设计基于学习的模型[Bai+20]，同时利用尽可能少的标记MCS实例仍然是一项具有挑战性的任务，元学习可能有助于缓解这一挑战。\n\n### 7.3 理论\n\n本文指出了基于gnn的元学习的几个重要理论问题。最自然的问题是了解在gnn中迁移学习的好处。Garg等人[GJJ20]和Scarselli等人[STH18]最近建立了gnn的泛化界。另一方面，在元学习的背景下，Tripu-raneni等人[TJJ20b]考虑了fj·h形式的函数，其中fj∈F是任务特定函数，h是共享函数。元测试阶段所需样本数量随C(F)增长，显著低于从头学习fj·h。如果能通过推广[GJJ20]和[STH18]的结果来证明gnn的类似加速结果，这将是很有趣的。另一个有趣的问题是确定共享表示的正确级别，并弄清楚这种结构的表达能力。Xu等人的开创性工作[Xu+18]证明了gnn的变种，如GCN和GraphSAGE并不比Weisfeiler-Lehman (WL)测试更具鉴别性。由于用于元学习的gnn进一步限制了所使用的架构类型，一个有趣的问题是，它是否会在表达性方面带来任何额外的成本。最后，第5节中讨论的方法在一个关键方面有所不同——它们是对新任务中的共享元参数进行微调和更新，还是保持共享元参数不变。最近，Chua等人[CLL21]表明，微调元参数在某些情况下可能是有益的，特别是当新任务的样本数量很大时。在gnn上的元学习环境中，理解这种微调何时有助于提高新任务的性能将是很有趣的。\n\n### 7.4 应用程序\n\n在第6节中，我们已经讨论了一些使用gnn框架的元学习应用。这个通用框架与该领域的许多重要问题相当相关。\n\n网络对齐:元学习可能有用的一个潜在问题是网络对齐(NA) [zhou +20]。在NA中，主要目标是映射或链接来自不同网络的实体，现有的方法很难扩展。一个有趣的研究方向是考虑元学习来克服这种可扩展性的挑战。\n\n分子性质预测:gnn也被用于预测分子性质。然而，主要的挑战之一是，分子是异质结构，每个原子通过不同类型的键与不同的相邻原子连接。其次，通常只有有限的标记分子特性数据;因此，为了预测新的分子性质，元学习技术[Guo+21]可以是相关的和有效的。\n\n动态图:在许多应用中，图的出现伴随着其动态特性，也就是说，节点和边以及它们的属性可以随着时间的推移而改变(添加或删除)。上面讨论的大多数论文都使用了基于元学习和用于静态图的gnn的框架。一个有趣的方向是将该框架扩展为动态图。动态特性给新添加的节点或边缘带来了新的挑战，如难以获得标签。例如，在知识图中，新添加的边会引入新的关系。另一个挑战是效率，因为管理和预测不断变化的网络本身就是一项艰巨的任务。元学习将有助于解决这些挑战。\n\n## 8  结论\n\n在这项调查中，我们对图神经网络(gnn)和元学习结合的工作进行了全面的回顾。除了概述gnn和元学习的背景外，我们还以有组织的方式在多个类别中组织了过去的研究。我们还提供了一个彻底的审查，方法的总结，和应用在这些类别。此外，我们还描述了几个利用GNN进行元学习的未来研究方向。元学习在gnn上的应用是一个不断发展和令人兴奋的领域，我们相信许多图问题将从这两种方法的结合中获益良多。\n","tags":["mete-learning","GPN"],"categories":["文献阅读"]},{"title":"Paper Translation 1|Graph Prototypical Networks for Few-shot Learning on Attributed Networks detectors","url":"/2022/09/01/文献翻译1/","content":"\n# Translation|Graph Prototypical Networks for Few-shot Learning on Attributed Networks\n\n## 0 摘要\n\n如今，归因网络在众多影响深远的应用中无处不在，例如社会网络分析、财务欺诈检测和药物发现。节点分类作为分布式网络的核心分析任务，受到了学术界的广泛关注。在真实的带属性网络中，很大一部分节点类只包含有限的标记实例，呈现长尾节点类分布。现有的节点分类算法无法处理镜头较少的节点类。作为一种补救措施，“少拍学习”在研究界引起了极大的关注。然而，少镜头节点分类仍然是一个具有挑战性的问题，我们需要解决以下问题:\n\n(i) 如何从一个属性网络中提取元知识用于少镜头节点分类?\n(ii) 如何确定每个标签实例的信息量，以建立稳健有效的模型?\n\n为了回答这些问题，本文提出了一种图元学习框架——图原型网络(GPN)。通过构建半监督节点分类任务池来模拟真实的测试环境，GPN能够在一个有属性的网络上进行元学习，并衍生出一个高度可推广的模型来处理目标分类任务。大量实验证明了GPN在少镜头节点分类方面的优越性。\n\n## 1 介绍\n\n由于其强大的建模能力，属性网络越来越多地被用于建模无数基于图的系统，如社交媒体网络、引用网络和基因调控网络。在属性网络的各种分析任务中，节点分类是一项必不可少的任务，它的应用范围很广，包括社交圈学习、文档分类和蛋白质分类等。简单地说，目标是在给定部分标记属性网络的情况下推断节点的缺失标签。为了解决这个问题，许多方法已经在研究界提出，并显示出有前景的性能。\n\n解决节点分类问题的主流方法通常遵循监督或半监督范式，这种范式通常依赖于所有节点类都有足够的标记节点。尽管如此，在许多真实世界的带属性网络工作中，很大一部分节点类只包含有限数量的标记实例，呈现了节点类标签的长尾分布。如图1所示，DBLP是一个数据集，其中节点表示出版物，节点标签表示场所。在所有节点类中，超过30%的节点类的标记实例少于10个。同时，许多实际应用都要求学习模型具有处理此类小概率类的能力。一个典型的例子是流量网络上的入侵检测问题，对手不断开发新的攻击和威胁。由于密集的标记成本，对于特定类型的攻击，只能访问少数示例。因此，通过有限的标记数据来理解这些攻击类型对于提供有效的反制措施至关重要。由于缺乏标记训练数据，现有的节点分类算法无法利用这些节点类学习出有效的模型。因此，研究在少镜头设置下的有属性网络上的节点分类问题是具有挑战性和必要性的。\n\n近年来，在利用少量标记例子来解决分类等任务的少镜头学习(FSL)方面取得了很大的研究进展。一般来说，一个FSL模型在不同的元训练任务中进行学习，这些任务从具有大量标记数据的类中采样，并且可以自然地从训练中看不到的类中泛化到一个新任务(即元测试任务)。这样的元学习过程使模型能够从以前的经验中适应知识，并导致了在FSL问题上的重大进展。具体来说，一些主要的研究(如连体网络、匹配网络和关系网络)试图通过比较共享度量空间中的查询实例和标记示例来进行预测。这些学习比较的方法由于其简单和有效而流行起来。\n\n尽管它们取得了丰硕的成功，但在归属网络上的少shot学习在很大程度上仍未得到探索，主要是因为以下两个挑战:\n\n(i)构建这些元训练任务的过程依赖于数据独立和同分布(i.i.d)的假设，这在归属网络上是无效的。除了传统的文本或图像数据外，属性网络位于非欧几里得空间，对节点之间的内在依赖关系进行编码。直接嫁接现有方法无法捕获底层数据结构，这使得嵌入的节点表示缺乏表现力。因此，如何在属性网络上发挥元学习的力量，从数据中提取元知识是必不可少的;\n(ii)大多数现有的FSL方法简单地假设所有标记的例子在描述它们所属的类时都具有同等的重要性。然而，忽略标记节点的个体信息将不可避免地限制模型在现实世界的属性网络上的性能:一方面，由于标记数据严重受限，这使得FSL模型非常容易受到噪声或异常值的影响[33,45];另一方面，这与一个节点的重要性可能在很大程度上偏离另一个节点的事实相反。直观上，社区中的那些中心(核心)节点应该更具有代表性的[46]。因此，如何获取每个标记节点的信息量是在有属性网络上建立有效的少镜头分类模型的另一个挑战。\n\n为了解决上述挑战，我们提出了图原型网络(GPN)，这是一个图元学习框架，用于解决属性网络上的少镜头节点分类问题。与其直接对节点进行分类，GPN试图学习一个可转移的度量空间，通过寻找最近的类原型来预测节点的标签。该框架由两个基本组件组成，它们无缝地一起学习每个类的原型表示。具体而言，GPN中的网络编码器首先通过图神经网络(GNNs)将输入网络压缩为表达节点表示，以捕获属性网络的数据异构性。同时，开发了另一个基于gnn的节点评估器，通过利用网络中编码的额外信息来估计每个标记实例的信息量。通过这种方式，GPN获得了高鲁棒性和代表性的类原型。此外，通过跨半监督节点分类任务池进行元学习，GPN在一个有属性网络上逐步提取元知识，进一步在目标少镜头分类任务上获得更好的泛化能力。综上所述，我们工作的主要贡献如下:\n\n- 问题: 我们研究了具有属性网络上的少镜头节点分类的新问题。特别地，我们强调它在实际应用中的重要性，并进一步提供正式的问题定义。\n\n- 算法: 针对该问题，我们提出了一个原则性框架GPN，它利用图神经网络和元学习在属性网络上学习一个强大的少镜头节点分类模型。\n\n- 评估: 我们在各种真实世界的数据集上进行了广泛的实验，以证实我们的方法的有效性。实验结果表明，GPN在具有属性的网络中具有较好的少镜头节点分类性能。\n\n## 2 相关工作\n\n在本节中，我们将相关工作简要总结为两类:\n(1)图神经网络;\n(2)少镜头学习。\n\n### 2.1图神经网络\n\n在深度学习取得巨大成功的推动下，近年来，人们开始致力于开发用于图结构数据的深度神经网络。作为先行者的工作之一，GNN通过循环神经结构传播邻居的信息来学习节点表示。基于图谱理论，出现了一系列图卷积网络(GCNs)，通过设计不同的图卷积层展现了优越的学习性能。其中，对GCNs的第一个突出研究叫做谱CNN，它将卷积运算扩展到谱域用于网络表示学习。此后，随着图卷积网络的扩展，其研究取得了越来越多的进展。除了谱图卷积模型，遵循邻域聚合方案的图神经网络也被广泛研究。这些方法不是为每个节点训练单独的嵌入，而是学习一组聚合器函数，从节点的局部邻域聚合特征。GraphSAGE学习了一个通过从节点的局部邻域采样和聚合特征来生成嵌入的函数。类似地，GATs (Graph Attention Networks)在聚合节点的邻域信息时，引入了可训练的注意权值来指定对邻居的细粒度权值。此外，图同构网络(GIN)在多集上使用任意的聚合函数扩展了这一思想，并被证明与Weisfeiler-Lehman (WL)图同构检验一样具有强大的理论能力。然而，现有的GNN模型都侧重于半监督节点分类。当前gnn的主要挑战之一是无法处理具有严重有限样本的不可见类。在本文中，我们提出了一种新的GNN框架来解决图结构数据上的少镜头节点分类问题。\n\n### 2.2 Few-shot学习\n\n少镜头学习(FSL)的目的是基于从以往经验中获得的知识，用有限数量的例子解决新的任务。现有的FSL模型一般分为两大类:(1)基于优化的方法，主要是在给定梯度的少数镜头实例上学习模型参数的优化。一个例子是基于LSTM的元学习器，它旨在学习用于训练神经分类器的有效参数更新规则。mml学习适合不同FSL任务的参数初始化，并兼容任何使用梯度下降训练的模型。Meta-SGD在元学习方面做了进一步的改进，主张在一个步骤内学习权重初始化、梯度更新方向和学习率。蜗牛模型是另一种结合时间卷积和软注意来学习最优学习策略的模型。但是，这一行的工作通常会受到微调的计算成本的影响。(2)基于指标的方法，试图学习跨不同任务的查询和支持集之间的通用匹配指标。例如，匹配网络学习带有注意网络的加权最近邻分类器。prototype Network通过取支持实例的均值向量来计算每个类的原型，通过计算查询实例的欧氏距离来分类。Ren等人提出了一种原型网络的扩展，在少镜头学习中同时考虑了标记数据和未标记数据。关系网络训练一个辅助网络来学习每个查询和支持集之间的非线性度量。值得一提的是，由于简单和有效，我们的方法也遵循这个范例。近年来，图上的少镜头学习受到了越来越多的研究关注。然而，这些方法平等地对待支持例子，使模型不稳定的噪声或异常值。在本文中，我们学习了一个鲁棒和强大的少镜头学习模型，通过考虑标签支持例子的个体重要性。\n\n## 3 问题陈述\n\n遵循常用的表示法，在本文中，我们使用书法字体、粗体小写字母和粗体大写字母来表示集合(如G)、向量(如G)。， x)和矩阵(例如，x)。矩阵X的𝑖𝑡ℎ行用X𝑖表示，矩阵X的转置用XT表示。我们在表1中总结了整篇论文中使用的主要符号。对于其他特殊的符号，我们将在相应的部分中说明它们。\n\n形式上，一个带属性的网络可以表示为𝐺= (V, E, X)，其中V表示节点集{𝑣1，𝑣2，…，𝑣𝑛}和E表示边集{𝑒1，𝑒2，…,𝑒𝑚}。每个节点关联一个特征向量x𝑖∈R1×𝑑，x = [x1;x2;。； x𝑛)∈R𝑛×𝑑\n\n表示所有节点特性。因此，更一般地，有属性的网络可以表示为𝐺= (A, X)，其中A ={0,1}𝑛×𝑛是一个表示网络结构的邻接矩阵。其中，A𝑖，𝑗= 1表示节点𝑣𝑖和节点𝑣𝑗之间有一条边;否则，A𝑖，𝑗= 0。研究问题可以写成:\n\n问题定义1。带属性网络上的少镜头节点分类:给定一个带属性网络G = {A, X}，假设我们有大量的带标签的节点，用于一组节点类𝐶𝑡𝑟𝑎𝑖𝑛。在对𝐶𝑡𝑟𝑎𝑖𝑛中的标记数据进行训练后，该模型的任务是预测节点(即查询集Q)的标签，这些节点来自一个不相连的节点类集𝐶𝑡𝑒𝑠𝑡，每个类中只有少数标记节点(即支持集S)可用。\n\n按照FSL中的常见设置，如果𝐶𝑡𝑒𝑠𝑡包含𝑁类，并且支持集S包含每个类𝐾标记的节点，这个问题就被命名为𝑁-way𝐾-shot节点分类问题。本质上，这个问题的目标是学习一个元分类器，它可以适应只有几个标记节点的新类。因此，如何从𝐶𝑡𝑟𝑎𝑖𝑛中提取可转移的元知识是解决研究问题的关键。\n\n## 4 图原型网络\n\n由于现有的FSL模型并不是为图结构数据量身定制的，因此直接应用于研究问题是不可行的。在本节中，我们详细介绍了在有属性网络上用于少镜头节点分类的图原型网络(GPN)。具体来说，我们的框架设计和构建是为了解决三个具有挑战性的研究问题:\n\n- 如何在有属性的网络(非i.i.d。数据)提取元知识?\n\n- 如何通过考虑节点属性和拓扑结构，从输入属性网络中学习表达节点表示?\n\n- 如何识别每个标记节点的信息，以学习稳健和鉴别类表示?\n\n图2概述了提出的图原型网络(GPN)。在4.1节中，我们介绍了所提模型的骨干培训机制。在4.2节和4.3节中，我们介绍了如何设计GPN中的两个基本模块。然后我们讨论如何使用第4.4节中提出的框架进行少镜头节点分类。最后，我们将在4.5节中介绍复杂性分析。\n\n### 4.1 基于归属网络的情景训练\n\n我们的方法是一个元学习框架，遵循流行的情景训练范式。具体来说，GPN在不同的元训练任务中进行大量的学习，而不是只在目标元测试任务中学习。情景训练的关键思想是通过从𝐶𝑡𝑟𝑎𝑖𝑛采样节点来模拟真实的测试环境。训练环境与测试环境的一致性缓解了分布差距，提高了模型的泛化能力。具体来说，在每一集中，我们构建一个𝑁-way𝐾-shot元训练任务:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831211754.png)\n\n其中元训练任务T𝑡的支持集S𝑡和查询集Q𝑡都从𝐶𝑡𝑟𝑎𝑖𝑛采样。支持集S𝑡包含每个类的𝐾节点，而查询集Q𝑡包含从每个𝑁类的剩余部分采样的𝑀查询节点。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831211912.png)\n\n整个训练过程基于一组𝑇元训练任务T𝑡𝑟𝑎𝑖𝑛= {T𝑡}𝑡=1。在每个元训练任务T𝑡中，该模型被训练以最小化其对查询集Q𝑡的预测的损失，并逐集进行训练，直到收敛。通过这种方式，模型逐渐通过那些元训练任务收集元知识，然后可以自然地泛化到元测试任务T𝑡𝑒𝑠𝑡= {S, Q}，其中包含不可见的类𝐶𝑡𝑒𝑠𝑡。\n\n与构建监督元训练任务池[12]的传统情景训练不同，在每个情景中，我们采样𝑁-way𝐾-shot标记节点，并将其余节点屏蔽为未标记节点。这样，我们就可以用部分标记的属性网络创建一个半监督元训练任务。通过同时考虑标记数据和未标记数据及其依赖关系，我们能够在元学习过程中学习更有表现力的节点表示，用于少镜头节点分类。\n\n### 4.2 网络表示学习\n\n为了从一个有属性的网络中学习表达节点表示，我们开发了一个网络编码器来捕捉数据的异构性。具体来说，网络编码器拥有一个GNN主干，它将每个节点转换为一个低维潜在表示。一般情况下，gnn采用邻域聚合方案，通过递归聚合和压缩本地邻域的节点特征来计算节点表示。简单地说，GNN层可以定义为:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831212101.png)\n\n式中h𝑙𝑖为𝑙层节点𝑖的节点表示，N𝑖为𝑣𝑖相邻节点的集合。Combine和Aggregate是gnn的两个关键功能，有一系列可能的实现方法[15,17,39]。\n\n通过在网络编码器中叠加多个GNN层，学习到的节点表示能够捕获网络中远程节点依赖关系:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831212201.png)\n\n其中Z为从网络编码器学习到的节点表示。为简单起见，我们将使用𝑓𝜽(·)用𝐿GNN层表示网络编码器。\n\n模型计算。有了从网络编码器学习到的节点表示，接下来，我们的目标是用支持集中的标记节点计算每个类的表示。我们遵循原型网络[35]的思想，它鼓励每个类集群的节点围绕一个特定的原型表示。形式上，类原型可以通过以下方式计算:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831212258.png)\n\n其中S𝑐表示𝑐类的标记样例集，Proto为原型计算函数。例如，在vanilla Prototypical Networks[35]中，每个类的原型是通过取属于该类的所有嵌入节点的平均值来计算的:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831212351.png)\n\n### 4.3 节点重要性评估\n\n尽管它很简单，但直接将嵌入支持实例的平均向量作为原型可能不会为我们的问题提供有希望的结果。它不仅忽略了网络中每个节点的重要程度不同，而且由于标记数据[45]严重受限，使得FSL模型对噪声非常敏感。因此，细化这些类原型对于构建健壮而有效的FSL模型变得尤为重要。\n\n为了识别每个标记节点的信息量，我们认为节点的重要性与其邻居的重要性[27]是高度相关的。据此，我们设计了一个基于gnn的节点评估器𝑔𝜙(·)(如图3所示)，通过分数聚合层估计节点重要性分数，其定义如下:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831212628.png)\n\n其中𝑠𝑙𝑖为𝑙-th层节点𝑣𝑖的重要性得分(𝑙=1，…𝐿); 是节点𝑣𝑖和𝑣𝑗之间的关注权重，我们通过共享关注机制计算:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831212903.png)\n\n其中||为串联算子，a为权值向量。为了计算初始重要性评分𝑠0，我们使用评分层来压缩节点特征。我们的计分层是一个具有tanh非线性的前馈层。其中，节点𝑣𝑖的初始得分计算方法为:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831213032.png)\n\n其中w𝑠∈R𝑑是可学习的权重向量，𝑏𝑠∈R1是偏差。中心的调整。在之前关于节点重要性估计的研究中[26,27]认为，节点的重要性与其在图中的中心性呈正相关。考虑到节点𝑣𝑖的度度deg(𝑖)是其中心性和受欢迎程度的常用代理，我们定义节点𝑣𝑖的初始中心性𝐶(𝑖)为:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831213123.png)\n\n其中𝜖是一个小常数。为了计算最终的重要性得分，我们对最后一层的估计得分𝑠𝐿应用中心性调整，并应用一个sigmoid非线性，如下所示:\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220831213236.png)\n\n通过这种方式，节点赋值器通过利用网络中编码的附加信息来调整支持集中标记示例的重要性。\n","tags":["mete-learning","GPN"],"categories":["文献阅读"]},{"title":"光影旅途|花束般的恋爱","url":"/2022/08/10/花束般的恋爱/","content":"\n不谈恋爱的时间里，我想过两个问题，和喜欢的人一定要在一起吗？一定要等到自己是最好的自己时才适合谈恋爱吗？这部电影给了我一些答案。\n\n我想起张佳玮在知乎上的一个回答：「非得准备周全了，非得把自己打造完美了，非得到一定时间了，才去恋爱——这是许多人谈恋爱的想法。殊不知感情这种事，不是你把自己揉搓好了，就有好回报的。」\n\n「给人感觉似乎游刃有余的男人\n\n​\t大部分说白了就是瞧不起女方」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_012555_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_012602_com.ly.sjm.yuli.jpg)\n\n「据说要是一起吃过三次饭还没表白的话\n\n​\t就会沦为普通朋友」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_020522_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_020605_com.ly.sjm.yuli.jpg)\n\n「如果喜不喜欢一个人\n\n​\t是以分开时想念对方的时长来判定的话\n\n​\t那我肯定是喜欢她的」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_020638_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_020706_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_020735_com.ly.sjm.yuli.jpg)\n\n这部电影中有很多这样的虚实转换，以及巧妙的构图技巧。\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_021414_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_021426_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_021434_com.ly.sjm.yuli.jpg)\n\n「开始亦是结束的开始\n\n​\t相遇总是伴随着别离\n\n​\t恋爱就像宴席终会散去\n\n​\t所以恋爱中的人们\n\n​\t只是带来各自喜欢的东西相对而坐\n\n​\t隔着桌子聊天\n\n​\t苦中作乐罢了」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_022859_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_022929_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023103_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023118_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023135_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023157_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023204_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023227_com.ly.sjm.yuli.jpg)\n\n「如果女生教男生认识了一朵花\n\n​\t以后不管过了多久\n\n​\t男生一看到那种花就会想起那个女生」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023616_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023620_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_023628_com.ly.sjm.yuli.jpg)\n\n「不知怎么的 我不断感受到了时间的流逝」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_024940_com.ly.sjm.yuli.jpg)\n\n「感觉好像已经没意义了\n\n​\t虽然最后还是分手了\n\n​\t但当时也有可能就跟他结婚了\n\n​\t好像连他让我看不惯的地方都习惯了\n\n​\t看不惯的这种情绪也都习惯了」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032750_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032801_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032905_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032909_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032918_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032926_com.ly.sjm.yuli.jpg)\n\n「只要有过一次分手的念头\n\n​\t就会像伤口的结痂 忍不住想要撕掉」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032943_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_032954_com.ly.sjm.yuli.jpg)\n\n「爱情就像生鲜食品 是有保质期的\n\n​\t一旦过期 就会变成为了维持平局\n\n​\t一味把球传来传去的消极状态」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_033021_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_033025_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_033030_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_033040_com.ly.sjm.yuli.jpg)\n\n「两个人的孤独远比一个人的孤独更悲伤」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_033051_com.ly.sjm.yuli.jpg)\n\n「我们这一路走来一直一帆风顺\n\n​\t可惜功亏一篑」\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_035228_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_035241_com.ly.sjm.yuli.jpg)\n\n再见啦\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_035537_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_035557_com.ly.sjm.yuli.jpg)\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/Screenshot_20220810_035759_com.ly.sjm.yuli.jpg)","tags":["观后感"],"categories":["光影旅途"]},{"title":"Educated|你当像鸟飞往你的山","url":"/2022/08/04/你当像鸟飞往你的山/","content":"\n**“先找出你的能力所在**，\n  **然后再决定你是谁。“**\n\n我花了一周的时间读完了这本书，准确来说是5天，其中有两天我一直读到天亮。我想这是一个很好的方式，当被琐事压着的时候，通过阅读来抵抗，而阅读也会以正反馈的方式作用于我，于是夜里的胡思乱想得以遏制。我在试图培养起这个习惯。\n\n我记录阅读的分类是**解体与救赎**，这个名字起源于法国作家萧沆的《解体概要》，我大学时读了两遍，但几乎还是未能理解。有段时间我很喜欢书上的一句话**“我们的力量来自于遗忘和无能”**。\n\n片段式的记录不足以支撑这漫长而近乎残忍的故事叙述，但阅读一本书还是想留下点什么，以此纪念这些文字在与我初次见面时，如何触动我的心弦泛起阵阵涟漪。\n\n# 2022-07-30\n\nP26\n当我拿到第一份证明我是个人的法律证明时，我感觉怪怪的，就好像权利被人剥夺了：在此之前，我从未意识到这还需要证明。\n\nP28\n男人就愿意这么想，是他们拯救了陷入困境的傻女人。我只需要靠边站，任他扮演英雄就好了！\n\n> 我想起我的一个朋友，我和他经常会就女生的话题展开一些讨论，多半是他叙述，我评价；我提问，他回答。并非是评头论足之类的下流的对话，但我也很难称其为思考。以前我认为了解女人最好的方式是接触女人，直接面对她们，后来发现好像并不是，因为从见识过很多的男人那里也能了解到，而且还能收获两种视角，前提是这个男人足够聪明。我喜欢和聪明人对话。再后来我开始意识到男人和女人思考的方式东西很不一样，不论你从男人那里了解到什么样女人，你了解到的总归是男人，是男人的视角以及思考以及肤浅~~或深刻~~的见解。真正想了解什么，还是得亲身去接触，并直接接触目标。\n\nP48\n十年后我的理解会发生转变，想起汇而构成人一生的所有决定——人们共同或者独自做出的那些决定，聚合起来，制造了每一桩单独事件。沙粒不可计数，叠压成沉积物，然后成为岩石。 \n\n> 我记录的最后一句话也提到了十年。\n> 我想起孟烦了问父亲的那句“了儿的苟活，终究是难堪还是骄傲？”\n> 十年之前过于沉重甚至难堪、羞耻，十年之间折磨但是骄傲，十年之后真的可以得到救赎与解放吗？\n\n# 2022-07-31\n\nP73\n我在学习的这个技能至关重要，那就是对不懂的东西耐心阅读。 \n\n# 2022-08-02\n\nP120\n我这一生中，这些直觉一直在教导我一个道理——只有依靠自己，胜算才更大。 \n\n# 2022-08-03\n\nP193\n我知道这不会持续下去，下一次我们说话的时候，一切都会不一样，此刻的柔情将被遗忘，我们之间会再次上演无休止的斗争。但今晚他想帮我，这就够了。\n\nP211\n我已觉察出我们是如何被别人给予我们的传统所塑造，而这个传统我们有意或无意地忽视了。我开始明白，我们为一种话语发声，这种话语的唯一目的是丧失人性和残酷地对待他人——因为培养这种话语更容易，因为保有权力总让人感觉在前进。\n\nP212\n我再也不允许自己在一场我并不理解的冲突中首当其冲。 \n\n# 2022-08-04\n\nP221\n对我来说最重要的不是爱情或友情，而是我自欺欺人的能力：相信自己很坚强。\n\nP229\n承认不确定性，就是被迫承认自己的软弱和无能，但也意味着你相信你自己。这是一个弱点，但这个弱点中透出一股力量：坚信活在自己的思想中，而不是别人的思想中。\n\nP230\n我从未允许自己拥有这样的特权：不确定，但拒绝让位于那些声称确定的人。我的一生都活在别人的讲述中。他们的声音铿锵有力，专制而绝对。之前我从未意识到，我的声音也可以与他们的一样有力。\n\nP268\n“先找出你的能力所在，然后再决定你是谁。”\n\nP283\n无论你成为谁，无论你把自己变成了什么，那就是你本来的样子。它一直在你心中。不是在剑桥，而是在于你自己。你就是黄金。回到杨百翰大学，甚至回到你家乡的那座山，都不会改变你是谁。那可能会改变别人对你的看法——即便是黄金，在某些光线下也会显得晦暗——但那只是错觉。金子一直是金子。\n\nP284\n“决定你是谁的最强大因素来自你的内心。”\n\nP307\n多年以后我才明白那天晚上发生了什么，我在其中又扮演了什么角色。我是如何在本该沉默时开口，却在本该说话时闭上了嘴巴。我们需要的是一场革命，一场自我们童年起就一直扮演的那种古老、脆弱的角色的颠覆——从托词中解放出来，证明自己是一个人。表达意见，采取行动，蔑视顺从。\n\nP317\n我告诉他们，我曾经贫穷而无知。当我告诉他们这些时，我丝毫不感到羞耻。那时我才明白羞耻感的来源：不是因为我不曾在铺着大理石的音乐学院学习，也不是我没有当外交官的父亲；不是因为父亲是半个疯子，也不是因为母亲跟着他亦步亦趋。我的羞耻源自我有一个将我朝吱嘎作响的大剪刀刀刃推去，而不是将我拉走远离它们的父亲；我的羞耻感源自我躺在地上的那些时刻，源自知道母亲就在隔壁房间闭目塞听，那一刻完全没有选择去尽一个母亲的责任。\n\nP317\n我想我终于可以坦然地面对过去的生活了。那并不完全是事实，但更广泛的意义上讲，的确如此：未来真的会更好。现在一切都已变得更好。\n\nP317\n过去是一个幽灵，虚无缥缈，没什么影响力。只有未来才有分量。\n\nP339\n那是一种狂热的遗忘方式，整个夏天我都在追逐它：在成群的游客中忘记自我，允许自己抹去全部的个性、性格和历史。\n\nP361\n在过去的十年里，我穿越的距离——物理上的和精神上的——几乎让我无法呼吸，让我思考自己是否已改变太多。我所有的学习、阅读、思考和旅行，是否已将我变成一个不再属于任何地方的人？ \n","tags":["阅读","文摘"],"categories":["解体与救赎"]},{"title":"Paper Reading 6|YOLOv7：Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors","url":"/2022/07/28/文献阅读6/","content":"\n# 摘要\n\nYOLOv7在5FPS到160FPS范围内的速度和精度都超过了所有已知的目标检测算法，在GPU V100上具有30FPS的帧率，在所有已知的30FPS以上的实时目标检测器中，YOLOv7的准确率最高，达到56.8% AP。\n\n# Introduction\n\n- 实时目标检测是计算机视觉领域中的一个重要课题，是计算机视觉系统中必不可少的组成部分。在多目标跟踪、自动驾驶、机器人、医学图像分析等领域都有体现。YOLOV7主要希望它能够同时支持移动GPU和从边缘到云端的GPU设备。\n- 近年来，针对不同边缘设备的实时目标检测仍在不断发展。\n  - **MCUNet**和**NanoNet**——专注于生产低功耗单片机和提高边缘CPU上的推理速度；\n  - **YOLOX**和**YOLOR**等——专注于提高各种GPU的 推理速度；\n  - **实时目标检测器**——主要集中在高效体系结构的设计上；\n    - 可以在CPU上使用的实时目标检测器——**MobileNet**，**ShuffleNet**或**GhostNet**\n    - 针对GPU开发的实时目标检测器——**ResNet**、**DarkNet**或**DLA**(Deep Layer Aggregation,一种深度网络特征融合方法，CVPR2018)\n- 本文所提出的方法，其发展方向不同于目前主流的实时目标检测器。除了架构优化之外，该方法是将重点放在训练过程的优化上，即一些优化的模块和优化方法，可能会增强训练成本以提高目标检测的准确性，但不增加推理成本。这种模块和优化方法称为可训练的免费包(bag-of-freebies)。\n- 近年来，模型重新参数化(model re-parameterization)和动态标签分配(dynamic label assignment)已成为网络训练和目标检测中的重要课题。在上述新概念提出后，目标检测器的训练又发展出许多新问题，本论文中作者介绍了一些发现的新问题，并设计了解决这些问题的有效方法。\n\n# 本文的贡献总结：\n\n(1)  我们设计了几种可训练的bag-of-freebies方法，使得实时目标检测可以在不增加推理成本的情况下大大提高检测精度；\n(2)  对于目标检测方法的演进，我们发现了两个新问题，即重新参数化的模块如何替换原始模块，以及动态标签分配策略如何处理分配给不同输出层的问题。此外，我们还提出了解决这些问题所带来的困难的方法；\n(3)  我们提出了实时目标检测器的“扩展”和“复合缩放”方法，可以有效地利用参数和计算；\n(4)  我们提出的方法可以有效减少最先进的实时目标检测器约40%的参数和50%的计算量，并且具有更快的推理速度和更高的检测精度。\n\n# 相关工作\n\n## 实时物体检测器\n\n目前最先进的实时目标检测器主要基于 YOLO 和 FCOS .能够成为最先进的实时目标检测器通常需要以下特性：\n(1)更快更强的网络架构；\n(2) 更有效的特征整合方法；\n(3) 更准确的检测方法;\n(4) 更稳健的损失函数 ；\n(5) 一种更有效的标签分配方法 ；\n(6) 更有效的训练方法。\n在本文中，将针对与上述 (4)、(5) 和 (6) 相关的最先进方法衍生的问题设计新的可训练免费赠品方法。\n\n## 模型重新参数化\n\n模型重新参数化是在推理阶段将多个计算模块合并为一个。有两种方法：\n\n- 模块级集成：模块级重新参数化是最近比较热门的研究问题。这种方法在训练时将一个模块拆分为多个相同或不同的模块分支，在推理时将多个分支模块整合为一个完全等效的模块。\n\n- 模型级集成：模型级重新参数化有两种做法来获得最终的推理模型。一种是用不同的训练数据训练多个相同的模型，然后对多个训练模型的权重进行平均。另一种是对不同迭代次数的模型权重进行加权平均。\n\n## 模型缩放\n\n模型缩放是一种放大或缩小已设计模型并使其适合不同计算设备的方法。模型缩放方法通常使用不同的缩放因子，例如分辨率（输入图像的大小）、深度（层数）、宽度（通道数）和阶段（特征金字塔的数量），从而在网络参数量、计算量、推理速度和准确率之间取得良好的折衷。\n\nNAS(Network architecture search，网络架构搜索)是一种常用的模型扩展方法。\n\n- 优点：从搜索空间中自动搜索合适的缩放因子，而不需要定义太复杂的规则；\n- 缺点：需要非常昂贵的计算来完成模型缩放因子的搜索。\n\n通过查阅文献发现几乎所有的模型缩放方法都是独立分析单个缩放因子的，甚至复合缩放类别中的方法也是独立优化缩放因子的。这是因为大多数流行的NAS架构都处理不太相关的伸缩因子。\n\n# 网络结构\n\n## 扩展的高效层聚合网络\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220728162225.png)\n\n在大多数关于高效建筑结构设计的文献中，主要考虑的因素在于参数的数量、计算量和计算密度，有些还涉及到内存访问代价等，分析**输入/输出信道比、体系结构分支数量和单元操作对网络推理速度的影响**。上图(b)中的CSPVoVNet设计时VoVNet的变体，CSPVoVNet的架构除了考虑上述的基本设计问题外，还对梯度路径进行了分析，使不同层的权值能够学习到更多样化的特征，这样的梯度分析方法使推断更快、更准确。上图(c)中的ELAN考虑了以下设计策略——**通过控制最短最长梯度路径，深度网络可以有效学习和收敛**。在本文中，作者提出了基于ELAN的Extended-ELAN(E-ELAN)，其主要架构如上图(d)所示。\n\n## Partial -> Partial Convolutional (部分卷积)\n\n部分卷积的概念由英伟达在ECCV2018发表的论文《Image Inpainting for Irregular Holes Using Partial Convolutions》中提出，最初用于图像修复。\n\n哪天想起来了爷再看。\n\n## 基于串联的模型的模型缩放\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220728171808.png)\n\n模型缩放的主要目的是调整模型的一些属性，生成不同尺度的模型，以满足不同推理速度的需要。例如，EfficientNet的缩放模型考虑了宽度、深度和分辨率；scaled-YOLOv4的缩放模型是调整阶段数。上述方法主要应用于PlainNet、ResNet等架构中，当这些架构在执行放大或缩小时，每一层的入度和出度不会发生变化，因此我们可以独立分析每个缩放因子对参数量和计算量的影响。然而，如果将这些方法应用到基于连接的体系结构中，我们将发现当对深度进行放大或缩小时，位于基于连接的计算块之后的转换层的程度将减少或增加，如上图的 (a)和(b)所示。\n\n从上述现象可以推断，对于基于串联的模型，我们不能单独分析不同的缩放因子，而必须综合考虑。以scaling-up depth为例，这样的动作会导致transition layer的输入通道和输出通道的比例发生变化，这可能会导致模型的硬件使用率下降。因此，我们必须为基于级联的模型提出相应的复合模型缩放方法。当我们缩放计算块的深度因子时，我们还必须计算该块的输出通道的变化。然后，我们将对过渡层进行等量变化的宽度因子缩放，结果如上图（c）所示。我们提出的复合缩放方法可以保持模型在初始设计时的特性并保持最佳结构。\n\n# 可训练的bag-of-freebies\n\n## 重新参数化卷积\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220729142928.png)\n\n- **问题**：虽然RepConv在VGG上已经取得了优异的性能，但将其应用在ResNet和DenseNet等架构时，其精度会显著降低；\n\n- **原因**：RepConv实际上是在一个卷积层中结合了3×3卷积、1×1卷积和恒等连接。但 RepConv 中的恒等连接破坏了 ResNet 中的残差和 DenseNet 中的连接，因此文中提出了RepConvN；\n\n- **解决方法**：用梯度流传播路径来分析重新参数化卷积如何与不同的网络相结合，并据此设计了规划的重新参数化卷积，即RepConvN。\n\n- **具体描述**：当一个带有残差或连接的卷积层被重新参数化的卷积取代时，应该没有恒等连接。\n\n上图显示了在PlainNet和ResNet中使用的“Planned re-parameterized convolution”的示例。\n\n## 标签分配（粗为辅助，细为主要损失）\n\n*深度监督是一种常用的深度网络训练技术。其主要思想是在深度神经网络的某些中间隐藏层加了一个辅助的分类器作为一种网络分支来对主干网络进行监督。*\n\nYOLOv7将负责最终输出的头部称为**引导头**(lead head)，辅助训练的头部称为 **辅助头**(auxiliary head).\n\n- 问题：\n\n  - 过去：标签分配通常直接指向ground truth，根据给定的规则生成硬标签；\n\n  - 近年：以目标检测为例，利用网络预测输出的质量和分布，再结合真实信息考虑，使用一些计算和优化方法生成可靠的软标签。如YOLO利用bounding box回归预测IoU和ground truth作为软标签；\n\n  - 目前：针对“如何将软标签分配给辅助头和引导头”的问题还没有相关文献进行探讨；\n\n本文作者将网络预测结果和地面真实值一起考虑，然后分配软标签的机制称为**标签分类器**\n\n- 解决方法：\n  - 流行方法：是将辅助头和引导头分开，利用各自的预测结果和ground truth执行标签分配。\n  - 本文方法：通过引线头预测同时引导辅助引线头和引线头的标签分配新方法。也就是说，我们以导头预测为指导，生成粗到细的层次标签，分别用于辅助导头学习和导头学习。\n\n这两种深度监管标签分配策略如下图所示：\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220729151737.png)\n\n1. 导头引导标签分配器：根据根据引导头的预测结果和ground truth进行计算，通过优化过程生成软标签。这套标签将作为辅助头和导头的目标获取训练模型。*这样做的原因是lead head具有较强的学习能力*，因此由它生成的软标签应该更能代表源数据与目标数据之间的分布和相关性。*此外，可以把这种学习看作一种广义剩余学习，通过让较浅的辅助头直接学习引导头已经学习过的信息，引导头将更能专注于学习尚未学习到的剩余信息*。\n2. 粗到细导头引导标签分配器：利用引导头的预测结果和ground truth生成软标签，**但是**，在这个过程中生成了两组不同的软标签，即粗标签和细标签，其中细标签与引导头引导标签分配器生成的软标签相同，而粗标签是通过降低正样本分配过程的约束没让更多的网络作为正目标来生成的。*这是因为辅助头的学习能力不如导头强*，为了避免丢失学习的信息，重点将优化辅助头在对象检测任务中的回忆。对于引导头的输出，可以从高查全率的结果中过滤出高精度的结果作为最终输出。\n\n*ground truth：基准真相，是一个相对概念。它是指相对于新的**测量**方式得到的测量值，作为基准的，由已有的、可靠的测量方式得到的测量值（即**经验证据**）。人们往往会利用基准真相，对新的测量方式进行**校准**，以降低新测量方式的误差和提高新测量方式的准确性。机器学习领域借用了这一概念。使用训练所得模型对样本进行推理的过程，可以当做是一种广义上的测量行为。因此，在**有监督学习**中，ground truth 通常指代样本集中的标签。可以理解为**真实信息***。\n\n\n\n## 其他可训练的bag-of-freebies\n\n1. 批处理归一化：将批处理归一化直接连接到卷积层；\n2. YOLOR中的隐形知识结合卷积特征映射的加法和乘法方式：通过推理阶段的预计算，将YOLOR中的隐性知识简化为向量，该向量可以与前一层或后一层的偏差和权重相结合；\n3. EMA模型：EMA是mean teacher中使用的一种技术，在本文中使用EMA模型作为最终推断模型。\n","tags":["Object Detection","yolo"],"categories":["文献阅读"]},{"title":"Paper Reading 5|Swin Transformer：Hierarchical Vision Transformer using Shifted Windows","url":"/2022/05/26/文献阅读5/","content":"\n# 引言\n\n- 作者的目的：就网络结构而言，CNN在计算机视觉上处于统治地位，Transformer在NLP上处于统治地位，作者想扩展Transformer在计算机视觉上比肩CNN，使其能够作为计算机视觉各种任务通用的骨干。\n\n- 两个挑战：\n\n  - 规模(scale)：与word token在transformer中作为基本处理元素不同，视觉元素可以在规模上有很大的变化，这是一个在对象检测等任务中需要注意的问题。在现有的基于transformer的视觉模型中，token都是固定规模的，这一属性不适合这些视觉应用。\n  - 分辨率(resolution)：图像的像素分辨率比文字段落中的文字高得多，目前存在许多视觉任务，如语义分割，需要在像素级进行密集预测，而这在高分辨率图像上对于Transformer来说是难以实现的，因为其自注意的计算复杂度是图像大小的二次方。\n\n为了克服这些问题，作者提出了一个通用的Transformer主干，称为Swin-Transformer，它构建分层特征映射，计算复杂度与图像大小成线性关系，如图1(a)所示，Swin Transformer构建了一个分层表示，从较小的补丁(用灰色表示)开始，逐渐将相邻的补丁合并到更深的Transformer层中。通过这些分层特征映射，Swin Transformer模型可以方便地利用**高级技术进行密集预测**，如**特征金字塔网络(FPN)[42]或U-Net[51]**。线性计算复杂度是通过在划分图像(用红色表示)的非重叠窗口中局部计算自我注意来实现的。由于每个窗口中的patch数量是固定的，因此复杂度与图像大小成线性关系。这些优点使Swin Transformer适合作为各种视觉任务的通用骨干，而不像以前的基于Transformer的架构[20]，后者生成单一分辨率的特征图，复杂度为二次型。\n\n![](https://s2.loli.net/2022/05/26/Lwa8ZWdf2JDu4VX.png)","tags":["Object Detection","CVPR2021","transformer"],"categories":["文献阅读"]},{"title":"数学建模学习笔记5|数据包络分析方法","url":"/2022/04/22/数学建模学习笔记5/","content":"\n# 数据包络分析方法\n\n数据包络分析方法（Data Envelopment Analysis，DEA）是运筹学、管理科学与数理经济学交叉研究的一个新领域。它是根据多项投入指标和多项产出指标，利用线性规划的方法，对具有可比性的同类型单位进行相对有效性评价的一种数量分析方法。DEA 方法及其模型已广泛应用于不同行业及部门，并且在处理多指标投入和多指标产出方面，体现了其得天独厚的优势。\n\n## DEA 模型\n\n开发出一种技术，通过明确地考虑多种投入的运用和多种产出的产生，它能够用来比较提供相似服务的多个服务单位之间的效率，这项技术被称为数据包络线分析（DEA）。它避开了计算每项服务的标准成本，因为它可以把多种投入和多种产出转化为效率比率的分子和分母，而不需要转换成相同的货币单位。因此，用 DEA 衡量效率可以清晰地说明投入和产出的组合，从而，它比一套经营比率或利润指标更具有综合性并且更值得信赖。\n\nDEA 是一个线形规划模型，表示为产出对投入的比率。通过对一个特定单位的效率和一组提供相同服务的类似单位的绩效的比较，它试图使服务单位的效率最大化。在这个过程中，获得 100% 效率的一些单位被称为相对有效率单位，而另外的效率评分低于 100% 的单位被称为无效率单位。\n\n这样，企业管理者就能运用 DEA 来比较一组服务单位，识别相对无效率单位，衡量无效率的严重性，并通过对无效率和有效率单位的比较，发现降低无效率的方法。\n\n## DEA 模型建立\n\n#### 1) 定义变量\n\n设 $Ek(k=1，2，……， K)$ 为第 $k$ 个单位的效率比率，这里 $K$ 代表评估单位的总数。\n\n设 $uj(j=1，2，……， M)$ 为第 $j$ 种产出的系数，这里 $M$ 代表所考虑的产出种类的总数。变量 $uj$ 用来衡量产出价值降低一个单位所带来的相对的效率下降。\n\n设 $vI(I=1，2，……，N)$ 为第 $I$ 种投入的系数，这里 $N$ 代表所考虑的投入种类的综合素。变量 $vI$ 用来衡量投入价值降低一个单位带来的相对的效率下降。\n\n设 $Ojk$ 为一定时期内由第 $k$ 个服务单位所创造的第 $j$ 种产出的观察到的单位的数量。\n\n设 $Iik$ 为一定时期内由第 $k$ 个服务单位所使用的第 $i$ 种投入的实际的单位的数量。\n\n#### 2) 目标函数\n\n　　目标是找出一组伴随每种产出的系数 $u$ 和一组伴随每种投入的系数 $ν$，从而给被评估的服务单位最高的可能效率。 $$ \\max E_{e}=\\frac{u_{1} O_{i e}+u_{2} O_{2 e}+\\ldots \\ldots+u_{M} O_{M e}}{v_{1} I_{1 e}+v_{2} I_{2 e}+\\ldots \\ldots+v_{N} I_{N e}}\\tag{*} $$ 式中，$e$ 是被评估单位的代码。 这个函数满足这样一个约束条件，当同一组投入和产出的系数（$uj$ 和 $vi$）用于所有其他对比服务单位时，没有一个服务单位将超过 100% 的效率或超过 1.0 的比率。\n\n#### 3) 约束条件\n\n$$ \\frac{u_{1} O_{1k}+u_{2} O_{2 k}+\\ldots \\ldots+u_{M} O_{M k}}{v_{1} I_{1 k}+v_{2} I_{2 k}+\\ldots \\ldots+v_{N} I_{N k}} \\leq 1.0\\quad,　k=1,2,...,K\\tag{**} $$\n\n式中所有系数值都是正的且非零。\n\n为了用标准线性规划软件求解这个有分数的线性规划，需要进行变形。要注意，目标函数和所有约束条件都是比率而不是线性函数。通过把所评估单位的投入人为地调整为总和 1.0，这样等式（*）的目标函数可以重新表述为： $$ \\max E_{e}=u_{1} O_{1 e}+u_{2} O_{2 e}+\\ldots+u_{M} O_{M e} $$ 满足以下约束条件： $$ v_{1} I_{1 e}+v_{2} I_{2 e}+\\ldots+v_{N} I_{N e}=1 $$ 对于个服务单位，等式（**）的约束条件可类似转化为： $$ u_{1} O_{1 k}+u_{2} O_{2 k}+\\ldots+u_{M} O_{M k}-\\left(v_{1} I_{1 k}+v_{2} I_{2 k}+\\ldots+v_{N} I_{N k}\\right) \\leq 0\\quad,\\quad k=1,2,…,K $$ 式中 $u_j≥0 ,j=1,2,...,M$，$v_i≥0 ,i=1,2,...,N$\n\n　　关于服务单位的样本数量问题是由在分析种比较所挑选的投入和产出变量的数量所决定的。下列关系式把分析中所使用的服务单位数量 $K$ 和所考虑的投入种类数 $N$ 与产出种类数 $M$ 联系出来，它是基于实证发现和 DEA 实践的经验： $$ K\\geq2(N+M) $$\n","tags":["matlab"],"categories":["数学建模"]},{"title":"Paper Reading 4|基于注意力机制特征增强的舰船目标识别","url":"/2022/04/21/文献阅读4/","content":"\n# 概述\n\n本文中作者提出了基于注意力机制的特征增强架构(Feature enhancement architecture based on attention mechanism，FBAM)，在这个架构中，作者改进了两个模块：\n\n- 顶层特征增强模块(Top-level feature enhancement，TLFE)\n  通过融合通道注意力和空间注意力，为舰船识别提供丰富的语义信息和位置信息。\n- 自适应ROI特征增强(Adap-tive ROIfeature enhancement，ROIFE)\n  网络自适应组合多层次的ROI特征信息，增强舰船识别的细粒度级别特征，提高舰船识别的定位能力。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421135113.png)\n\n顶层特征增强模块是一个通道和空间信息双重注意力网络，最顶层的特征层{C5}经过通道注意力和空间注意力之后进行融合为一个新的特征层，以此保证顶层特征层的特征信息更完整的得到保留，将得到的新特征层与后续的特征层再进行融合。自适应ROI特征增强模块为每一个ROI汇集所有特征金字塔层的特征，从特征融合之后的特征金字塔{P2，P3，P4，P5}中的每一层学习生成更好的ROI特征，ROIFE为不同层的ROI特征生成不同的空间权重，将ＲOI 特征加权相融合\n\n# 改进方法\n\n以深度学习目标检测算法作为基础网络，对网络中特征融合部分 FPN 进行改进。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421141539.png)\n\n即骨干网络提取特征后送入FPN进行特征融合，在特征融合的**过程**中，设计增加TLFE模块，在特征融合**之后**，设计增加ROIFE模块，将不同层的特征求和作为这个ROI最终的特征。\n\n改进示意图如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421140317.png)\n\n这两种改进都是针对特征金字塔FPN部分的改进，第一个是**将{C5}通过并联的通道注意力和空间注意力模块**，赋予其更多的语义和空间信息；第二个是在特征融合后，对于任意一个ROI预测，**提取出该ROI在{P2,P3,P4,P5}上的所有对应的特征，然后利用网络本身学习权重参数，将不同层的特征求和**作为这个ROI最终的特征。\n\n## 顶层特征增强模块\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421142432.png)\n\n其中上半部分是空间注意力机制，通过一个卷积Conv+一个激活函数Sigmoid；下半部分先经过全局平均池化，获取全局感受野，再通过ReLU和Sigmoid激活函数；然后将这两部分原始的C5与通道权重值相乘，得到的两个有关注度的新特征层之后，将空间关注度与通道关注度的特征图相融合构成新的特征层。\n\n> 全局平均池化：编码了全局的统计信息。从空间的角度来看，通道注意力是全局的，而空间注意力是局部的。通道注意力顺着通道维度对C5进行全局平均池化压缩，获取全局感受野，经过Sigmoid非线性处理，将输出结果作为每个通道的权重值。\n\n## 自适应ROI特征增强\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421143742.png)\n\n4层特征信息进行concat操作 –> 全局最大池化 –> 两次卷积 –> 激活函数\n\n- 全局最大池化：保留通道信息\n- 第一次1×1卷积：缩放通道值\n- 第二次1×1卷积：恢复通道信息值\n\n# 验证实验\n\n## 网络\n\n以**faster R-CNN**作为基础算法，以**resnet50**和**resnet101**这两种网络作为骨干网络\n\n## 数据集\n\n数据集选用HRSC2016数据集。\n\n## 评价指标\n\n精准召回曲线(PRC)和平均精确度(AP)\n\n## 对比试验\n\n实 验 利 用 **FasterＲ-CNN**、**Cascade Ｒ-CNN**、**RetinaNet** 3 种算法，以及**Resnet50** 和**Reanext101** 2 种骨干网络验证FBAM 的有效性。","tags":["CNN","FPN","attention mechanism"],"categories":["文献阅读"]},{"title":"数学建模学习笔记4|题型总结","url":"/2022/04/19/数学建模学习笔记4/","content":"\n# 2016 E题\n\n## 题目\n\n**粮食最低收购价政策问题研究**\n\n粮食，不仅是人们日常生活的必需食品，而且还是维护国家经济发展和政治稳定的战略物资，具有不可替代的特性。由于耕地减少、人口增加、水资源短缺、气候变化等问题日益凸显，加之国际粮食市场的冲击，我国粮食产业面临着潜在的风险。因此，研究我国的粮食保护政策具有十分重要的作用和意义。\n\n一般而言，粮食保护政策体系主要由三大支持政策组成：粮食生产支持政策、粮食价格支持政策和收入支持政策。粮食最低收购价政策就属于粮食价格支持政策范畴。\n\n一般情况下，我国粮食收购价格由市场供需情况决定，国家在充分发挥市场机制作用的基础上实行宏观调控。为保护农民利益、保障粮食市场供应，国家对重点粮食品种，在粮食主产区实行最低收购价格政策，并每年事先公布重点粮食品种的最低收购价。在最低收购价格政策执行期（粮食收获期，一般在2-5个月）内，当市场粮食实际收购价低于国家确定的最低收购价时，国家委托符合一定资质条件的粮食企业，按国家确定的最低收购价格收购农民种植的粮食，以保护粮农的种植积极性。\n\n我国自2005年起开始对粮食主产区实行了最低收购价政策，并连续多年上调最低收购价价格。2016年国家发展与改革委员会公布的小麦（三等）最低收购价格为每50公斤118元，比首次实施小麦最低收购价的2006年提高了66.2%；早籼稻（三等）、中晚籼稻（三等）和粳稻（三等）最低收购价格分别为每50公斤133元、138元和155元，分别比首次实施水稻最低收购价的2005年提高了84.72%、91.67%和106.67%。显而易见，粮食最低收购价政策已经成为了国家保护粮食生产的最为重要的举措之一。\n\n然而，也有学者不认同这项最低收购价政策。他们认为，粮食的实际收购价格（以后称为粮食市场收购价）应该由粮食供需双方通过市场调节来决定。粮食最低收购价政策作为一种粮食种植保护政策，扭曲了粮食市场的供需行为，即该政策的实施很有可能抬高了市场收购价格，导致粮食企业承担了很大的经营风险。\n\n对于粮食最低收购价政策实施效果的评价，学者们也是见解不一。部分地区某些粮食品种种植面积、粮食总产量不增反降，导致部分学者质疑粮食最低收购价政策的效果；但也有学者高度肯定了粮食最低收购价政策，认为如果不实施粮食最低收购价政策，这些地区某些粮食品种的种植面积可能会下降得更快，因而认为粮食最低收购价政策在稳定或增加粮食种植面积方面是有着积极的作用。\n\n一般来讲，粮食的种植面积是决定粮食供给的关键因素，也是保障粮食安全的重要前提。衡量粮食最低收购价政策实施的效果，主要是比较政策实施前后粮食种植面积是否有显著性变化。然而，可能影响粮食种植面积的因素有很多，除了粮食最低收购价政策外，还可能有其他很多的影响因素，如农业劳动力人口、粮食进出口贸易、农民受教育程度、城乡收入差距、家庭负担等。因此，要研究粮食最低收购价政策的实施效果，不能仅仅根据种植面积的变化来评定。\n\n与此同时，也有一些学者就粮食最低收购价制定的合理范围进行了探讨。最低收购价并不是实际的市场收购价格，而是一种心理安慰价，是收购粮食的底价。粮农决定是否种植粮食，取决于很多因素，但最主要的还是看种植粮食所获得的纯收益的大小。粮食最低收购价的公布，使得粮农能清楚地算出这笔经济账。因此粮食最低收购价的高低直接影响着当年的粮食生产。中国是一个“以粮为纲”的国家，存储的粮食一般要能够满足全国人民三年的吃饭和需求。同时国家对于粮食的补贴金额也是有限制的，在保持合理库存的前提下，一般不会超出各地粮食市场价格的10%。因此，过高的粮食最低收购价不仅会提高粮食市场价格从而加重消费者负担，同时也会增加粮食的库存压力和国家财政的支出风险。另一方面，过低的粮食最低收购价会打压粮农种植粮食的积极性，造成粮食种植面积的萎缩，这更不是国家所愿意看到的。\n\n请你们查阅相关资料和数据，结合数据特点，回答下列问题：\n\n1. 影响粮食种植面积的因素比较多，它们之间的关系错综复杂而且可能存在着粮食品种和区域差异。请你们建立影响粮食种植面积的指标体系和关于粮食种植面积的数学模型，讨论、评价指标体系的合理性，研究他们之间的关系，并对得出的相应结果的可信度和可靠性给出检验和分析。\n2. 对粮食最低收购价政策的作用，学者们褒贬不一。请你们建立粮食最低收购价政策执行效果的评价模型。并运用你们所建立的评价模型，结合粮食品种和区域差异，选择几个省份比较研究粮食主产区粮食最低收购价执行的效果。\n3. 粮食市场收购价是粮食企业收购粮食的市场价格，是由粮食供需双方通过市场调节来决定。它与粮食最低收购价一起构成粮食价格体系，是宏观价格调控系统中有一定相对独立性的重要措施。请你们运用数据分析或建立数学模型探讨我国粮食价格所具有的特殊规律性。\n4. 结合前面的研究和国家制定粮食最低收购价政策的初衷，请你们建立粮食最低收购价的合理定价模型，进而对“十二五”期间国家发展与改革委员会公布的粮食最低收购价价格的合理性做出评价，并运用你们所建立的模型对2017年的粮食最低收购价的合理范围进行预测。\n5. 与2000年相比，2015年我国小麦种植面积略有下降。如果国家想让小麦种植面积增加5%，通过调整粮食最低收购价是否能够达到这一目的？请说明理由。\n6. 根据你们的研究结论，请提出调控粮食种植的优化决策和建议。\n\n## 分析\n\n1. 影响粮食种植面积的因素比较多，它们之间的关系错综复杂而且可能存在着粮食品种和区域差异。请你们建立影响粮食种植面积的指标体系和关于粮食种植面积的数学模型，讨论、评价指标体系的合理性，研究他们之间的关系，并对得出的相应结果的可信度和可靠性给出检验和分析。\n\n   - Spearman 相关检验, 主成分回归模型\n\n   - 非参数 Spearman 相关检验法，偏最小二乘回归\n\n   - 结构方程模型，证性因子分析和路径分析\n\n   - Kolmogorov-Smirnov，相关性检验和主成分因子分析，Granger 因果关系检验，Granger 因果关系检验，似然比检验法，单位根检验和协整检验法\n\n   - 多元线性回归模型，相关系数、残差和显著性水平\n2. 对粮食最低收购价政策的作用，学者们褒贬不一。请你们建立粮食最低收购价政策执行效果的评价模型并运用你们所建立的评价模型，结合粮食品种和区域差异，选择几个省份比较研究粮食主产区粮食最低收购价执行的效果。\n\n   - 主成分分析，混合线性模型\n   - 三角模糊数\n   - 最低收购价政策执行效果综合评价指数模型，基于粒子群和投影寻踪算法的权重确定模型\n   - 主成分分析法\n3. 粮食市场收购价是粮食企业收购粮食的市场价格，是由粮食供需双方通过市场调节来决定。它与粮食最低收购价一起构成粮食价格体系，是宏观价格调控系统中有一定相对独立性的重要措施。请你们运用数据分析或建立数学模型探讨我国粮食价格所具有的特殊规律性\n\n   - 市场收购价理论和局部调整模型，包括 供应量模型、企业收购量模型和市场收购价格模型\n   - 基于供需理论构建粮食供需及价格联动模型， ARIMA 模型\n   - “蛛网”模型， ARCH 类模型\n   - 局部均衡模型和正反馈系统\n4. 结合前面的研究和国家制定粮食最低收购价政策的初衷，请你们建立粮食最低收购价的合理定价模型，进而对“十二五”期间国家发展与改革委员会公布的粮食最低收购价价格的合理性做出评价，并运用你们所建立的模型对2017年的粮食最低收购价的合理范围进行预测。\n\n   - 以粮食产量为目标模型，以价格波动、财政支出、库存和种植面积为约束条件建立粮食最低收购价合理定价的线性规划模型\n\n   - 优化模型\n   - GARCH 模型、单变量二阶差分方程模型(DDE)、支持向量机预测模型(SVM模型)以及马尔科夫链的时变权组合预测模型(HM-TWA)\n   - 基于正态分布随机数遗传，多目标合理定价模型，基于有序加权平均(OWA) 算子\n\n## 优秀论文\n","tags":["matlab"],"categories":["数学建模"]},{"title":"数学建模学习笔记3|预测模型","url":"/2022/04/18/数学建模学习笔记3/","content":"\n距离五一数学建模竞赛还有15天。\n\n# 灰色预测\n\n灰色预测是对既含有已知信息又含有不确定信息的系统进行预测，就是对在一定范围内变化的、与时间有关的灰色过程进行预测。灰色预测对原始数据进行生成处理来寻找系统变动的规律，并生成有较强规律性的数据序列，然后建立相应的微分方程模型，从而预测事物未来发展趋势的状况。\n\n# GM(1,1)模型：Grey(Gray)Model\n\nGM(1,1)是使用原始的离散非负数据列，通过一次累加生成削弱随机性的较有规律的新的离散数据列，然后通过建立微分方程模型，得到在离散点处的解经过累减生成的原始数据的近似估计值，从而预测原始数据的后续发展。\n\n## GM(1,1)原理介绍\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418213714.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418213802.png)\n\n## 一阶微分方程\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418214034.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418214258.png)\n\n## 准指数规律的检验\n\n![image-20220418214358823](C:/Users/Pengyk/AppData/Roaming/Typora/typora-user-images/image-20220418214358823.png)\n\n## GM(1,1)模型的评价与检验\n\n![image-20220418214529891](C:/Users/Pengyk/AppData/Roaming/Typora/typora-user-images/image-20220418214529891.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418215756.png)\n\n# 灰色预测的应用场景\n\n1. 数据是以年份度量的非负数据（如果是月份或者季度数据一定要用我们上一讲学过的时间序列模型）；\n2. 数据能经过准指数规律的检验（除了前两期外，后面至少90%的期数的光滑比要低于0.5）；\n3. 数据的期数较短且和其他数据之间的关联性不强（小于等于10，也不能太短了，比如只有3期数据），要是数据期数较长，一般用传统的时间序列模型比较合适。\n\n# 预测题小策略\n\n1. 看到数据后先画时间序列图并简单的分析下趋势（例如：我们上一讲学过的时间序列分解）；\n2. 将数据分为训练组和试验组，尝试使用不同的模型对训练组进行建模，并利用试验组的数据判断哪种模型的预测效果最好（比如我们可以使用SSE这个指标来挑选模型，常见的模型有指数平滑、ARIMA、灰色预测、神经网络等）；\n3. 选择上一步骤中得到的预测误差最小的那个模型，并利用全部数据来重新建模，并对未来的数据进行预测；\n4. 画出预测后的数据和原来数据的时序图，看看预测的未来趋势是否合理。\n\n# 灰色预测例题\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418220207.png)\n\n# 例题对应代码讲解\n\n1. 画出原始数据的时间序列图，并判断原始数据中是否有负数或期数是否低于4期，如果是的话则报错，否则执行下一步；\n\n2. 对一次累加后的数据进行准指数规律检验，返回两个指标：指标1：光滑比小于0.5的数据占比（一般要大于60%）指标2：除去前两个时期外，光滑比小于0.5的数据占比（一般大于90%）并让用户决定数据是否满足准指数规律，满足则输入1，不满足则输入0\n\n3. 如果上一步用户输入0，则程序停止；如果输入1，则继续下面的步骤。\n\n4. 让用户输入需要预测的后续期数，并判断原始数据的期数：\n\n  - 数据期数为4：分别计算出传统的GM(1,1)模型、新信息GM(1,1)模型和新陈代谢GM(1,1)模型对于未来期数的预测结果，为了保证结果的稳健性，对三个结果求平均值作为预测值。\n\n  - 数据期数为5,6或7：取最后两期为试验组，前面的n-2期为训练组；用训练组的数据分别训练三种GM模型，并将训练出来的模型分别用于预测试验组的两期数据；利用试验组两期的真实数据和预测出来的两期数据，可分别计算出三个模型的SSE；选择SSE最小的模型作为我们建模的模型。\n\n  - 数据期数大于7：取最后三期为试验组，其他的过程和4.2类似。\n\n5. 输出并绘制图形显示预测结果，并进行残差检验和级比偏差检验。\n\n# 灰色预测运行结果\n","tags":["matlab"],"categories":["数学建模"]},{"title":"Yolov5-v6.0学习笔记10|val.py代码详解","url":"/2022/04/18/Yolov5学习笔记10/","content":"# val.py简介\n\nval.py文件主要是在每一轮的训练结束后，验证当面模型的mAP、混淆矩阵等指标。\n\n> - mAP：英文全称为 Mean Average Precision，作为目标检测中的平均精度\n>\n>   AP：(平均精度)是衡量目标检测算法好坏的常用指标，在Faster R-CNN，SSD等算法中作为评估指标。\n>\n>   AP等于recall值取0-1时，precision值的平均值\n>\n> - 混淆矩阵：也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类相比较计算的。\n\n实际上这个脚本最常用的应该是通过train.py调用run函数，而不是通过执行val.py的。所以在这个脚本中，最重要的就是run函数。\n\n# opt参数\n\n```python\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, default=ROOT / 'data/ship.yaml', help='dataset.yaml path')\n    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp4/weights/best.pt', help='model.pt path(s)')\n    parser.add_argument('--batch-size', type=int, default=2, help='batch size')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')\n    parser.add_argument('--task', default='val', help='train, val, test, speed or study')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\n    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\n    parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')\n    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n    parser.add_argument('--save-json', action='store_true', help='save a COCO-JSON results file')\n    parser.add_argument('--project', default=ROOT / 'runs/val', help='save to project/name')\n    parser.add_argument('--name', default='exp', help='save to project/name')\n    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n    opt = parser.parse_args()\n    opt.data = check_yaml(opt.data)  # check YAML\n    opt.save_json |= opt.data.endswith('coco.yaml')\n    opt.save_txt |= opt.save_hybrid\n    print_args(FILE.stem, opt)\n    return opt\n```\n\nopt参数详解：\n\n- opt参数详解 \n\n- data: 数据集配置文件地址 包含数据集的路径、类别个数、类名、下载地址等信息\n\n- weights: 模型的权重文件地址 weights/yolov5s.pt\n\n- batch_size: 前向传播的批次大小 默认32\n\n- imgsz: 输入网络的图片分辨率 默认640\n\n- conf-thres: object置信度阈值 默认0.25\n\n- iou-thres: 进行NMS时IOU的阈值 默认0.6\n\n- task: 设置测试的类型 有train, val, test, speed or study几种 默认val\n\n- device: 测试的设备\n\n- single-cls: 数据集是否只用一个类别 默认False\n\n- augment: 测试是否使用TTA Test Time Augment 默认False\n\n- verbose: 是否打印出每个类别的mAP 默认False\n\n- 下面三个参数是auto-labelling(有点像RNN中的teaching forcing)\n\n  save-txt: traditional auto-labelling\n\n  save-hybrid: save hybrid autolabels, combining existing labels with new predictions before NMS (existing predictions given confidence=1.0 before NMS. \n\n  save-conf: add confidences to any of the above commands\n\n- save-json: 是否按照coco的json格式保存预测框，并且使用cocoapi做评估（需要同样coco的json格式的标签） 默认False\n\n- project: 测试保存的源文件 默认runs/test \n\n- name: 测试保存的文件地址 默认exp  保存在runs/test/exp下\n\n- exist-ok: 是否存在当前文件 默认False 一般是 no exist-ok 连用  所以一般都要重新创建文件夹\n\n- half: 是否使用半精度推理 默认False\n\n# main()函数\n\n```python\ndef main(opt):\n    global x\n    check_requirements(requirements=ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n\n    if opt.task in ('train', 'val', 'test'):  # run normally\n        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466\n            LOGGER.info(f'WARNING: confidence threshold {opt.conf_thres} >> 0.001 will produce invalid mAP values.')\n        run(**vars(opt))\n\n    else:\n        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]\n        opt.half = True  # FP16 for fastest results\n        if opt.task == 'speed':  # speed benchmarks\n            # python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...\n            opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False\n            for opt.weights in weights:\n                run(**vars(opt), plots=False)\n\n        elif opt.task == 'study':  # speed vs mAP benchmarks\n            # python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n.pt yolov5s.pt...\n            for opt.weights in weights:\n                f = f'study_{Path(opt.data).stem}_{Path(opt.weights).stem}.txt'  # filename to save to\n                x, y = list(range(256, 1536 + 128, 128)), []  # x axis (image sizes), y axis\n                for opt.imgsz in x:  # img-size\n                    LOGGER.info(f'\\nRunning {f} --imgsz {opt.imgsz}...')\n                    r, _, t = run(**vars(opt), plots=False)\n                    y.append(r + t)  # results and times\n                np.savetxt(f, y, fmt='%10.4g')  # save\n            os.system('zip -r study.zip study_*.txt')\n            plot_val_study(x=x)  # plot\n```\n\n在这个模块中，根据opt.task分为三个分支，即[train, val, test]、[speed]、[study]，最主要的分支还是在\n\n```python\nopt.task in ('train', 'val', 'test')\n```\n\n这段代码的意思是如果task in ['train', 'val', 'test']就正常测试 训练集/验证集/测试集。\n一般直接进入第一个分支，执行run()函数。\n\n# run()函数\n\n```python\nif RANK in [-1, 0]:\n  # mAP\n  callbacks.run('on_train_epoch_end', epoch=epoch)\n  ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\n  final_epoch = (epoch + 1 == epochs) or stopper.possible_stop\n  if not noval or final_epoch:  # Calculate mAP\n      results, maps, _ = val.run(data_dict,\n                                 batch_size=batch_size // WORLD_SIZE * 2,\n                                 imgsz=imgsz,\n                                 model=ema.ema,\n                                 single_cls=single_cls,\n                                 dataloader=val_loader,\n                                 save_dir=save_dir,\n                                 plots=False,\n                                 callbacks=callbacks,\n                                 compute_loss=compute_loss)\n```\n\nrun()函数在train.py中执行，用来在每个epoch后验证当前模型。\n\n## 载入参数\n\n```python\ndef run(data,\n        weights=None,  # model.pt path(s)\n        batch_size=32,  # batch size\n        imgsz=640,  # inference size (pixels)\n        conf_thres=0.001,  # confidence threshold\n        iou_thres=0.6,  # NMS IoU threshold\n        task='val',  # train, val, test, speed or study\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        workers=8,  # max dataloader workers (per RANK in DDP mode)\n        single_cls=False,  # treat as single-class dataset\n        augment=False,  # augmented inference\n        verbose=False,  # verbose output\n        save_txt=False,  # save results to *.txt\n        save_hybrid=False,  # save label+prediction hybrid results to *.txt\n        save_conf=False,  # save confidences in --save-txt labels\n        save_json=False,  # save a COCO-JSON results file\n        project=ROOT / 'runs/val',  # save to project/name\n        name='exp',  # save to project/name\n        exist_ok=False,  # existing project/name ok, do not increment\n        half=True,  # use FP16 half-precision inference\n        dnn=False,  # use OpenCV DNN for ONNX inference\n        model=None,\n        dataloader=None,\n        save_dir=Path(''),\n        plots=True,\n        callbacks=Callbacks(),\n        compute_loss=None,\n        ):\n```\n\n参数解释：\n\n- data: 数据集配置文件地址–包含数据集的路径、类别个数、类名、下载地址等信息 train.py时传入data_dict\n- weights: 模型的权重文件地址 运行train.py=None 运行test.py=默认weights/yolov5s.pt\n- batch_size: 前向传播的批次大小 运行test.py传入默认32 运行train.py则传入batch_size // WORLD_SIZE * 2\n- imgsz: 输入网络的图片分辨率 运行test.py传入默认640 运行train.py则传入imgsz_test\n- conf_thres: object置信度阈值 默认0.25\n- iou_thres: 进行NMS时IOU的阈值 默认0.6\n- task: 设置测试的类型 有train, val, test, speed or study几种 默认val\n- device: 测试的设备\n- single_cls: 数据集是否只用一个类别 运行test.py传入默认False 运行train.py则传入single_cls\n- augment: 测试是否使用TTA Test Time Augment 默认False\n- verbose: 是否打印出每个类别的mAP 运行test.py传入默认Fasle 运行train.py则传入nc < 50 and final_epoch\n- save_txt: 是否以txt文件的形式保存模型预测框的坐标 默认True\n- save_hybrid: 是否save label+prediction hybrid results to *.txt  默认False\n- save_conf: 是否保存预测每个目标的置信度到预测tx文件中 默认True\n- save_json: 是否按照coco的json格式保存预测框，并且使用cocoapi做评估（需要同样coco的json格式的标签）\n  - 运行test.py传入默认Fasle 运行train.py则传入is_coco and final_epoch(一般也是False)\n\n- project: 测试保存的源文件 默认runs/test\n- name: 测试保存的文件地址 默认exp  保存在runs/test/exp下\n- exist_ok: 是否存在当前文件 默认False 一般是 no exist-ok 连用  所以一般都要重新创建文件夹\n- half: 是否使用半精度推理 FP16 half-precision inference 默认False\n- model: 模型 如果执行test.py就为None 如果执行train.py就会传入ema.ema(ema模型)\n- dataloader: 数据加载器 如果执行test.py就为None 如果执行train.py就会传入testloader\n- save_dir: 文件保存路径 如果执行test.py就为‘’ 如果执行train.py就会传入save_dir(runs/train/expn)\n- plots: 是否可视化 运行test.py传入默认True 运行train.py则传入plots and final_epoch\n- wandb_logger: 网页可视化 类似于tensorboard 运行test.py传入默认None 运行train.py则传入wandb_logger(train)\n- compute_loss: 损失函数 运行test.py传入默认None 运行train.py则传入compute_loss(train)\n- return (Precision, Recall, map@0.5, map@0.5:0.95, box_loss, obj_loss, cls_loss)\n\n## 初始化/加载模型并选择处理器\n\n训练时（train.py）调用：初始化模型参数、训练设备\n验证时（val.py）调用：初始化设备、save_dir文件路径、make dir、加载模型、check imgsz、 加载+check data配置信息\n\n```python\n# Initialize/load model and set device\nglobal stride, ap50\ntraining = model is not None\nif training:  # called by train.py\n    device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model\n\n    half &= device.type != 'cpu'  # half precision only supported on CUDA\n    model.half() if half else model.float()\nelse:  # called directly\n    device = select_device(device, batch_size=batch_size)\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n    stride, pt, jit, onnx, engine = model.stride, model.pt, model.jit, model.onnx, model.engine\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n    half &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16 supported on limited backends with CUDA\n    if pt or jit:\n        model.model.half() if half else model.model.float()\n    elif engine:\n        batch_size = model.batch_size\n    else:\n        half = False\n        batch_size = 1  # export.py models default to batch-size 1\n        device = torch.device('cpu')\n        LOGGER.info(f'Forcing --batch-size 1 square inference shape(1,3,{imgsz},{imgsz}) for non-PyTorch backends')\n\n    # Data\n    data = check_dataset(data)  # check\n```\n\n## 调整模型\n\n```python\n    half &= device.type != 'cpu'  # half precision only supported on CUDA\n    model.half() if half else model.float()\nelse:  # called directly\n    device = select_device(device, batch_size=batch_size)\n```\n\n半精度验证half model + 模型剪枝prune + 模型融合conv+bn\n\n## 模型验证\n\n```python\nmodel.eval()\nis_coco = isinstance(data.get('val'), str) and data['val'].endswith('coco/val2017.txt')  # COCO dataset\nnc = 1 if single_cls else int(data['nc'])  # number of classes\niouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\nniou = iouv.numel()\n```\n\n是否是COCO数据集is_coco + 类别数nc + 计算mAP相关参数 + 初始化日志Logging\n\n## 加载val数据集\n\n训练时（train.py）调用：加载val数据集\n验证时（val.py）调用：不需要加载val数据集 直接从train.py 中传入testloader\n\n```python\n# Dataloader\nif not training:\n    model.warmup(imgsz=(1 if pt else batch_size, 3, imgsz, imgsz), half=half)  # warmup\n    pad = 0.0 if task in ('speed', 'benchmark') else 0.5\n    rect = False if task == 'benchmark' else pt  # square inference for benchmarks\n    task = task if task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n    dataloader = create_dataloader(data[task], imgsz, batch_size, stride, single_cls, pad=pad, rect=rect,\n                                   workers=workers, prefix=colorstr(f'{task}: '))[0]\n```\n\n## 初始化配置\n\n```python\nseen = 0\nconfusion_matrix = ConfusionMatrix(nc=nc)\nnames = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\nclass_map = coco80_to_coco91_class() if is_coco else list(range(1000))\ns = ('%20s' + '%11s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\ndt, p, r, f1, mp, mr, map50, map = [0.0, 0.0, 0.0], 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\nloss = torch.zeros(3, device=device)\njdict, stats, ap, ap_class = [], [], [], []\npbar = tqdm(dataloader, desc=s, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n```\n\n初始化混淆矩阵 + 数据集类名 + 获取coco数据集的类别索引 + 设置tqdm进度条 + 初始化p, r, f1, mp, mr, map50, map指标和时间t0, t1, t2 + 初始化测试集的损失 + 初始化json文件中的字典 统计信息 ap等\n\n# 开始验证\n\n```python\nfor batch_i, (im, targets, paths, shapes) in enumerate(pbar):\n```\n\n## 预处理图片和target\n\n```python\nt1 = time_sync()\nif pt or jit or engine:\n    im = im.to(device, non_blocking=True)\n    targets = targets.to(device)\nim = im.half() if half else im.float()  # uint8 to fp16/32\nim /= 255  # 0 - 255 to 0.0 - 1.0\nnb, _, height, width = im.shape  # batch size, channels, height, width\nt2 = time_sync()\ndt[0] += t2 - t1\n```\n\n## model前向推理\n\n```python\n# Inference\nout, train_out = model(im) if training else model(im, augment=augment, val=True)  # inference, loss outputs\ndt[1] += time_sync() - t2\n```\n\n## 计算验证集损失\n\n```python\n# Loss\nif compute_loss:\n    loss += compute_loss([x.float() for x in train_out], targets)[1]  # box, obj, cls\n```\n\n## 运行NMS\n\n```python\n# NMS\ntargets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\nlb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\nt3 = time_sync()\nout = non_max_suppression(out, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls)\ndt[2] += time_sync() - t3\n```\n\n首先将真实框target的xywh(因为target是在labelimg中做了归一化的)映射到img(test)尺寸； \n\n- save_hybrid: adding the dataset labels to the model predictions before NMS \n\n  意思是在NMS之前将数据集标签targets添加到模型预测中\n  这允许在数据集中自动标记(for autolabelling)其他对象(在pred中混入gt) 并且mAP反映了新的混合标签\n\n- targets: [num_target, img_index+class_index+xywh] = [31, 6]\n- lb: {list: bs} 第一张图片的target[17, 5] 第二张[1, 5] 第三张[7, 5]\n\n## 统计每章图片的真实框、预测框信息\n\n```python\n# Metrics\nfor si, pred in enumerate(out):\n    labels = targets[targets[:, 0] == si, 1:]\n    nl = len(labels)\n    tcls = labels[:, 0].tolist() if nl else []  # target class\n    path, shape = Path(paths[si]), shapes[si][0]\n    seen += 1\n\n    if len(pred) == 0:\n        if nl:\n            stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n        continue\n\n\t# Predictions\n    if single_cls:\n         pred[:, 5] = 0\n     predn = pred.clone()\n     scale_coords(im[si].shape[1:], predn[:, :4], shape, shapes[si][1]) # native-space pred\n```\n\n为每张图片做统计，写入预测信息到txt文件，生成json文件字典，统计tp等\n\n- out: list{bs}  [300, 6] [42, 6] [300, 6] [300, 6]  [:, image_index+class+xywh]\n\n获取第si张图片的gt标签信息，包括class、x、y、w、h，target[:, 0]为标签属于哪张图片的编号\n\n- nl为第si张图片的gt个数\n- path为第si张图片的地址\n\n如果预测为空，则添加空的信息到stats里\n\n- predn 将预测坐标映射到原图img中\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Vision Transform学习笔记2|Self-Attention","url":"/2022/04/13/ViT支线-Self-Attention/","content":"\n# 什么是Attention(注意力机制)\n\n在认知科学中，由于信息处理的瓶颈，人类会选择性地关注所有信息的一部分，同时忽略其他可见的信息。通俗点来说就是，我们在认知事物时，有着明显的主观色彩和测重，比如「我喜欢踢足球，但我更喜欢打篮球」，对于人类显然知道这个人更喜欢打篮球，但对于深度学习或计算机来说，它没办法领会到「更」的含义，因此没有办法知道这个结果。所以我们在训练模型的时候，会大家「更」字的权重，让它在句子中的重要性获得更大的占比。\n\n综上，注意力机制主要有两个方面：**决定需要关注输入的哪部分**；**分配有限的信息处理资源给重要的部分**。\n\n# 什么是Self-Attention\n\n在知道了attention在机器学习中的含义之后（下文都称之为注意力机制）。人为设计的注意力机制，是非常主观的，而且没有一个准则来评定，这个权重设置为多少才好。所以，如何让模型自己对变量的权重进行**自赋值**成了一个问题，这个**权重自赋值**的过程也就是self-attention。\n\n# Self-Attention原理\n\n$$\nAttention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n$$\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220413150524.png)\n\n## Softmax操作\n\n抛开Q,K,V三个矩阵不谈，self-attention最原始的形态是$Softmax(XX^T)X$\n\n这个公式表示什么意思呢？\n\n==Q1：$XX^T$代表什么？==\n\n一个矩阵乘以它自己的转置，会得到什么结果，有什么意义？\n\n我们知道，矩阵可以看作由一些向量组成，一个矩阵乘以它自己转置的运算，其实可以看成这些向量分别与其他向量计算内积。（此时脑海里想起矩阵乘法的口诀，第一行乘以第一列、第一行乘以第二列......嗯哼，矩阵转置以后第一行不就是第一列吗？这是在计算**第一个行向量与自己**的内积，第一行乘以第二列是计算**第一个行向量与第二个行向量的内积**第一行乘以第三列是计算**第一个行向量与第三个行向量的内积**.....）\n\n回想我们文章开头提出的问题，向量的内积，其几何意义是什么？\n\n**A1：表征两个向量的夹角，表征一个向量在另一个向量上的投影**\n\n实例：我们假设$X=[x^T_1;x^T_2;x^T_3]$，其中X为一个二维矩阵，$X^T_i$为一个行向量，下图中模拟运算了$XX^T$：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220413153154.png)\n\n首先，行向量$X^T_i$分别与自己和其他两个向量做内积，得到了一个新的向量。\n\n==Q2：新的向量有什么意义？表征什么？==\n\n**A2：投影的值大，说明两个向量相关度高。**\n\n更进一步，这个向量是词向量，是词在高维空间的数值映射。词向量之间相关度高表示什么？是不是**在一定程度上**（不是完全）表示，在关注词A的时候，应当给予词B更多的关注？\n\n==Q3：$XX^T$的意义是什么？==\n\n**A3：矩阵**$XX^T$**是一个方阵，我们以行向量的角度理解，里面保存了每个向量与自己和其他向量进行内积运算的结果。**\n\n==Q4：Softmax操作的意义是什么？==\n\n回到Softmax的公式：$Softmax(z_i)=\\frac{e^{z_i}}{\\sum^C_{c=1}e^{z_c}}$\n\n**A4：归一化。**\n\n也就是说通过Softmax操作后，这些数字的和为1了。\n\n==Q5：那么Attention机制的核心是什么呢？==\n\n**A5：加权求和。**\n\n## Q,K,V矩阵\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220413155750.png)\n\n在很多文章中提到的Q,K,V矩阵、查询矩阵之类的字眼，本质上都是**X矩阵的线性变换**，其来源是X与某个矩阵的乘积。\n\n==Q6：为什么不直接使用X而要对其进行线性变换呢？==\n\n**A6：当然是为了提升模型的拟合能力，矩阵W都是可以训练的，起到一个缓冲的作用。**\n\n## $\\sqrt{d_k}$的意义\n\n假设Q,K里的元素全为0，方差为1，那么$A^T=Q^TK$中元素的均值为0，方差为d。当d变得很大时，A中的元素的方差也会变得很大，如果A中的元素方差很大，那么Softmax(A)的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。\n\n**总结**：Softmax(A)的分布会和d有关。因此A中每一个元素除以$\\sqrt{d_k}$后，方差又变为1。这使得Softmax(A)的分布陡峭程度与d解耦，从而使得训练过程中梯度保持稳定。\n\n**对self-attention来说，它跟每一个input vector都做attention，所以没有考虑到input sequence的顺序**。\n\n# Self-Attention的优点\n\n与RNN相比，RNN的一个最大的问题是：前面的变量在经过多次RNN计算后，已经失去了原有的特征。越到后面，最前面的变量占比就越小，这是一个很反人类的设计。而self-attention在每次计算中都能保证每个输入变量a a*a*的初始占比是一样的，这样才能保证经过self-attention layer计算后他的注意力系数是可信的。\n\n总结下来，它的优点是：\n\n- 需要学习的参数量更少\n- 可以并行计算\n- 能够保证每个变量初始化占比是一样的\n\n# Multi-head Self-Attention\n\n这里继续讲解multi-head self-attention，所谓head也就是指一个a a*a*衍生出几个q , k , v 。上述所讲解的self-attention是基于single-head的。以2 head为例：\n\n首先，$a^i$先生成$q^1$，$ k^1$，$ v^1$。然后，接下来就和single-head不一样了，$q^i$生成 $q^{i,1}$,$q^{i,2}$，生成的方式有两种：\n\n1.  $q^i$乘上一个$W^{q,1}$得到 $q^{i,1}$，乘上 $W^{q,2}$得到$q^{i,2}$这个和single-head的生成是差不多的；\n2. $q^i$**直接从通道维，平均拆分成两个，得到$ q^{i,1}$,$q^{i,2}$；\n\n这两种方式，在最后结果上都差不多。至于为啥，后面会讲一下原因。\n\n那么这里的图解使用第1个方式，先得到$q^{i,1}$ $k^{i,1}$ $v^{i,1}$。对 $a^j$做同样的操作得到 $q^{j,1}$**, $k^{j,1}$,$ v^{j,1}$。这边需要注意的一点，$q^{i,1}$是要和$k^{j,1}$做矩阵乘法，而非$k^{j,2}$，一一对应。后面计算就和single-head一样了，最后得到$b^{i,1}$。\n\n# Position Encoding","tags":["python","计算机视觉","Vision Transform","Self-Attention"],"categories":["计算机视觉"]},{"title":"数学建模综述","url":"/2022/04/12/数学建模综述/","content":"\n# 数学建模基本题型\n\n## 预测类\n\n### 概述\n\n指的是通过分析已有的数据或者现象，找出其内在发展规律，然后对未来情形做出预测的过程。\n根据已知条件和求解目的，往往将预测类问题分为：\n\n- 小样本内部预测\n- 大样本内部预测\n- 小样本未来预测\n- 大样本随机因素或周期特征的未来预测\n- 大样本的未来预测\n\n### 例题——2021年第十一届MathorCup高校数学建模挑战赛B题\n\n> **B题 三维团簇的能量预测**\n> 团簇，也称超细小簇，属纳米材料的尺度概念。团簇是由几个乃至上\n> 千个原子、分子或离子通过物理或化学结合力组成的相对稳定的微观或亚\n> 微观聚集体，其物理和化学性质随所含的原子数目而变化。\n>\n> 团簇是材料尺度纳米材料的一个概念。团簇的空间尺度是几埃至几百埃的范围，用无机分子来描述显得太小，用小块固体描述又显得太大，许多性质既不同于单个原子分子，又不同于固体和液体，也不能用两者性质的简单线性外延或内插得到。因此，人们把团簇看成是介于原子、分子与宏观固体物质之间的物质结构的新层次。团簇科学是凝聚态物理领域中非常重要的研究方向。\n>\n> 团簇可以分为金属团簇和非金属团簇，由于金属团簇具有良好的催化性能，因此备受关注。但由于团簇的势能面过于复杂，同时有时候还需要考虑相对论效应等，所以搜索团簇的全局最优结构（即能量最低）显得尤为困难。其中，传统的理论计算方法需要数值迭代求解薛定谔方程，并且随原子数增加，高精度的理论计算时间呈现指数增长，非常耗时。因此，目前需要对这种方法加以改进，例如：考虑全局优化算法，结合机器学习等方法，训练团簇结构和能量的关系，从而预测新型团簇的全局最优结构，有利于发现新型团簇材料的结构和性能。请建立三维团簇能量预测的数学模型，并使用附件中的坐标和能量数据，解决下列问题。\n>\n> 备注：附件中数据集格式为xyz，第一行是原子数，第二行是能量，后面是原子的三维坐标。可用文本阅读器打开，并用VMD 等软件进行可视化。\n> **问题1**：针对金属团簇，附件给出了1000 个金团簇$Au_{20}$的结构，请你们建立金团簇能量预测的数学模型，并预测金团簇$Au_{20}$ 的全局最优结构，\n> 描述形状；\n> **问题2**：在问题1 的基础上，请你们设计算法，产生金团簇不同结构的异构体，自动搜索和预测金团簇$Au_{32}$的全局最优结构，并描述其几何形\n> 状，分析稳定性；\n> **问题3**：针对非金属团簇，附件给出了3751 个硼团簇$B_{45}$的结构，请你们建立硼团簇能量预测的数学模型，并预测硼团簇B45的全局最优结构，描述形状；\n> **问题4**：在问题3 的基础上，请你们设计算法，产生硼团簇不同结构的异构体，自动搜索和预测硼团簇$B_{40}$的全局最优结构，并描述其几何形状，分析稳定性。\n\n**注：预测问题主要是以某个小问的形式出现，很少有整个赛题所有小问全是预测要求的**\n\n## 评价类\n\n指的是按照一定的标准对事物的发展或者现状进行划分的过程在数学建模中题点可体现在对生态环境，社会建设，方案策略等进行评价。评价类赛题往往没有明确的指标体系和评价标准，往往是需要查阅各类资料进行构建的，因此评价类赛题也没有明确的答案。\n\n**注：解决评价类赛题的关键是指标体系的构建，构建完评价体系后在选择合适的评价方法即可，体系建立应秉承全面，准确，独立的三要素**\n\n## 机理分析类赛题\n\n机理分析是根据对现实对象特性的认识，分析其因果关系，找出反映内部机理的规律。在求解机理分析类问题时首先需要探寻与问题相关的物理，化学，经济等相关的知识，然后通过对已知数据或现象的分析对事物的内在规律做出必要的假设，最后通过构建合适的方程或关系式对其内在规律进行数值表达。\n\n**注：机理分析立足于建立事物内部的规律，相对于其他类型的赛题均有章可循，机理分析类赛题往往需要结合众多关联知识才可以进行求解，如空气动力学，流体力学，热力学等**\n\n## 优化类\n\n指在现有现有条件固定的情况下，如何使目标效果达到最佳。如在一座城市公交车公司拥有的公交车数量是固定的，问如何安排线路能够使盈利达到最高。优化类问题往往需要分析三个关键因素：目标函数，决策变量和约束条件，三者往往缺一不可。\n\n**注：解决优化类赛题必须知道优化的目的，约束的条件和所求解的关键变量，需要有较强的编程能力和赛题分析挖掘能力**\n\n# 数学建模算法\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/模型与算法分类.png)","tags":["数学建模"],"categories":["数学建模"]},{"title":"数学建模学习笔记2|综合评价类模型","url":"/2022/04/11/数学建模学习笔记2/","content":"\n# 简介\n\ntopsis综合评价法即根据有限个评价对象与理想化目标的接近程度进行排序的方法，是在现有的对象中进行相对优劣的评价，是一种逼近于理想解的排序法。\n\n> 基本过程为先将原始数据矩阵统一指标类型（一般正向化处理）得到正向化的矩阵，再对正向化的矩阵进行标准化处理以消除各指标量纲的影响，并找到有限方案中的最优方案和最劣方案，然后分别计算各评价对象与最优方案和最劣方案间的距离，获得各评价对象与最优方案的相对接近程度，以此作为评价优劣的依据。该方法对数据分布及样本含量没有严格限制，数据计算简单易行。\n\n**适用于：**决策层中指标的数据是已知的，利用这些数据使得评价的更加准确。\n\n# 算法原理\n\n## 统一指标类型\n\n将所有的指标转化为极大型称为**指标正向化**(最常用)\n\n## 第一步：将原始矩阵正向化\n\n常见的四种指标：\n\n|      指标名称      |     指标特点     |           例子           |\n| :----------------: | :--------------: | :----------------------: |\n| 极大型(效益型)指标 |   越大(多)越好   | 成绩、GDP增速、企业利润  |\n| 极小型(成本型)指标 |   越小(少)越好   |  费用、坏品率、污染程度  |\n|     中间型指标     | 越接近某个值越好 |    水质量评估时的PH值    |\n|     区间型指标     | 落在某个区间最好 | 体温、水中植物性营养物量 |\n\n*注：将原始矩阵正向化，就是要将所有的指标类型统一转化为极大型指标*\n**转换的函数形式可以不唯一**\n\n### 各种类型指标的转换\n\n- 极小型指标→极大型指标\n\n  - 公式：$max-x$\n\n  - **补充**：如果所有元素均为正数，那么也可以使用1/x\n\n- 中间型指标→极大型指标\n\n  - 中间型指标：指标值既不要太大也不要太小，取某特定值最好(如水质量评估PH值)。\n\n  - 公式：$M=max{|X_i-X_{best}|}$,$\\widetilde{X_i}=1-\\frac{|X_i-X_{best}|}{M}$，其中{X~i~}是一组中间型指标序列，且最佳的数值为X~best~。\n\n- 区间型指标→极大型指标\n\n  - 区间型指标：指标值落在某个区间内最好，例如人的体温在36°～37°这个区间比较好。\n\n  - 公式：$M=max[{a-min({x_i}),max(x_i)-b}]$， $\\widetilde{X_i}=\\begin{cases}{ 1-\\frac{a-x_i}{M},x_i<a} \\\\ 1 ,     a\\le x_i \\le b\\\\ 1-\\frac{x_i-b}{M}, x_i > b \\end{cases}$\n\n    其中$x_i$是一组区间型指标序列，且最佳的区间为[a,b]\n\n## 第二步：正向化矩阵标准化\n\n**标准化的目的是消除不同指标量纲的影响**\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220412222627.png)\n\n## 第三步：计算得分并归一化\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220412222728.png)\n\n# 例题\n\n> 题目：评价下表中20条河流的水质情况。\n> 注：含氧量越高越好；PH值越接近7越好；细菌总数越少越好；植物性营养物量介于10‐20之间最佳，超过20或低于10均不好。\n> ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220412222931.png)\n\n# 实现代码\n\n## 第一步：把数据复制到工作区，并将矩阵命名为X\n\n（1）在工作区右键，点击新建（Ctrl+N)，输入变量名称为X\n（2）在Excel中复制数据，再回到Excel中右键，点击粘贴Excel数据（Ctrl+Shift+V）\n（3）关掉这个窗口，点击X变量，右键另存为，保存为mat文件\n注意：代码和数据要放在同一个目录下，且Matlab的当前文件夹也要是这个目录。\n\n==导入数据代码：==\n\n```matlab\nload XX.mat\n```\n\n## 第二步：判断是否需要正向化\n\n```matlab\n[n,m] = size(X); %将表格数据转换为矩阵\n```\n\n==正向化代码：==\n\n- 极小型→极大型\n\n  ```matlab\n  function [posit_x] = Min2Max(x)\n      posit_x = max(x) - x;\n       %posit_x = 1 ./ x;    %如果x全部都大于0，也可以这样正向化\n  end\n  ```\n\n- 中间型→极大型\n\n  ```MATLAB\n  function [posit_x] = Mid2Max(x,best)\n      M = max(abs(x-best));\n      posit_x = 1 - abs(x-best) / M;\n  end\n  ```\n\n- 区间型→极大型\n\n  ```MATLAB\n  function [posit_x] = Inter2Max(x,a,b)\n      r_x = size(x,1);  % row of x \n      M = max([a-min(x),max(x)-b]);\n      posit_x = zeros(r_x,1);   %zeros函数用法: zeros(3)  zeros(3,1)  ones(3)\n      % 初始化posit_x全为0  初始化的目的是节省处理时间\n      for i = 1: r_x\n          if x(i) < a\n             posit_x(i) = 1-(a-x(i))/M;\n          elseif x(i) > b\n             posit_x(i) = 1-(x(i)-b)/M;\n          else\n             posit_x(i) = 1;\n          end\n      end\n  end\n  ```\n\n## 第三步：对正向化后的矩阵进行标准化\n\n```matlab\nZ = X ./ repmat(sum(X.*X) .^ 0.5, n, 1);\ndisp('标准化矩阵 Z = ')\ndisp(Z)\n```\n\n## 第四步：计算与最大值的距离和最小值的距离，并算出得分\n\n```MATLAB\nD_P = sum([(Z - repmat(max(Z),n,1)) .^ 2 ],2) .^ 0.5;   % D+ 与最大值的距离向量\nD_N = sum([(Z - repmat(min(Z),n,1)) .^ 2 ],2) .^ 0.5;   % D- 与最小值的距离向量\nS = D_N ./ (D_P+D_N);    % 未归一化的得分\ndisp('最后的得分为：')\nstand_S = S / sum(S)\n[sorted_S,index] = sort(stand_S ,'descend')\n```\n\n## 补充——幻方矩阵\n\nA = magic(5)  % 幻方矩阵\nM = magic(n)返回由1到n^2的整数构成并且总行数和总列数相等的n×n矩阵。阶次n必须为大于或等于3的标量。\nsort(A)若A是向量不管是列还是行向量，默认都是对A进行升序排列。sort(A)是默认的升序，而sort(A,'descend')是降序排序。\nsort(A)若A是矩阵，默认对A的各列进行升序排列\nsort(A,dim)\ndim=1时等效sort(A)\ndim=2时表示对A中的各行元素升序排列\nA = [2,1,3,8]\nMatlab中给一维向量排序是使用sort函数：sort（A），排序是按升序进行的，其中A为待排序的向量；\n若欲保留排列前的索引，则可用 [sA,index] = sort(A,'descend') ，排序后，sA是排序好的向量，index是向量sA中对A的索引。\nsA  =  8     3     2     1\nindex =  4     3     1     2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["matlab"],"categories":["数学建模"]},{"title":"碎碎念|与无意义的对抗","url":"/2022/04/09/与无意义的对抗/","content":"\n所以你读了几篇论文？\n写了几段代码？\n真正思考过什么东西？\n你真的没有在努力。\n\n只要我还在一直读书，我就能够一直理解自己的痛苦。\n一直与自己的无知、狭隘、偏见、阴暗见招拆招。\n很多人说：“和自己握手言和”，\n我不要做这样的人，我要拿着石头打磨我这块石头。\n会一直读书，一直痛苦，一直爱着从痛苦荒芜里生出来的喜悦。\n乘兴而来，尽兴而归，在一生中，这是很难得很难得的一件事情。\n\n再后来我知道阅读只是理解世界的一种方式，\n如果你有其他的方式与世界产生沟通，进行理解，\n被世界在风雨雷电中触动，那甚至不读书也行，\n不是必须要走的路，也不是那条路比另一条路高级，\n你需要做的甚至也不是阅读，是与世界产生对话，理解自己所在的时代。\n\n你知道你最大的问题是什么么？\n误认为自己有无限的时间和无限的可能，又不知道自己需要努力的方向在哪里。","tags":["随笔"],"categories":["像梦一场"]},{"title":"mAP值解析","url":"/2022/04/06/Yolov5支线学习之mAP值/","content":"\n# 目标检测算法评价指标——mAP值\n\n> mAP值即为平均精度，是衡量目标检测算法优劣的常用指标。\n> AP（平均精度）是衡量目标检测算法好坏的常用指标，在Faster R-CNN，SSD等算法中作为评估指标。\n> AP等于recall值取0-1时，precision值的平均值。\n\n## Precision & Recall(查准率和查全率)\n\n### 概念\n\n **Precision**：衡量你的模型预测准确度。即预测的数目中正确的百分比。\n\n> 例：你预测100个图片是苹果，其中80个真的是苹果，那么你的Precision为0.8\n\n**recall**：召回表示预测正确的目标数量。\n\n> 例：总共有100张苹果图片，你成功找到其中50张，那么你的recall为0.5\n\n### 定义\n\n以二分类结果为例：\n\n对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为**真正例(true positive)**、**假正例(false positive)**、**真反例(true negative)**、**假反例(false negative)**四种情形，令**TP**、**FP**、**TN**、**FN**分别表示其对应的样例数，则显然有**TP+FP+TN+FN=样例总数**。分类结果的“混淆矩阵”(confusion matrix)如表所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220409165719.png)\n\n查准率P和查全率R分别定义为：\n\n$ P = \\frac{TP}{TP+FP}$        $R = \\frac{TP}{TP+FN}$\n\n一般来说查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。\n\n**平衡点(BER):查准率=查全率时的取值,用来比较模型好坏.** \n\n## 交并比(IoU)\n\nIoU是预测框与ground truth的交集和并集的比值。\n\n为了计算precision和recall，与所有机器学习问题一样，我们必须鉴别出**True Positives**（真正例）、**False Positives**（假正例）、**True Negatives**（真负例）和 **False Negatives**（假负例）。\n\n假设边界框对应的IoU大于某个阈值（一般来说，比较常用的IoU阈值是0.5），我们就可以说这个预测的边界框是对的，或者说可以被划分为TP中。反之如果IoU小于阈值，那么这个预测的边界框就是错的，或者说是一个FP。如果对于图像中某个物体来说，我们的模型没有预测出对应的边界框，那么这种情况就可以被记为一次FN。\n\n- **True Positive (TP)**: IOU>=阈值的检测框\n- **False Positive (FP)**: IOU<阈值的检测框\n- **False Negative (FN)**: 未被检测到的GT\n- **True Negative (TN)**: 忽略不计\n\n对于每一个图片，ground truth数据会给出该图片中各个类别的实际物体数量。我们可以计算每个Positive预测框与ground truth的IoU值，并取最大的IoU值，认为该预测框检测到了那个IoU最大的ground truth。然后根据IoU阈值，我们可以计算出一张图片中各个类别的正确检测值（True Positives, TP）数量以及错误检测值数量（False Positives, FP）。\n\n既然我们已经得到了正确的预测值数量（True Positives），也很容易计算出漏检的物体数（False Negatives, FN）。\n\n## AP值\n\n### 定义\n\n**PR曲线下面积的近似，是一个0~1之间的数值，也可用来衡量模型的performance**。\n\n- PR曲线比较直观，但由于曲线的上下震荡，不方便比较不同模型的PR曲线\n- AP是一个数字，模型的AP大，则模型更好，方便比较不同模型\n\n### 计算方法\n\n计算AP值，一般有两种方法：\n\n1. 11点插值法：\n\n   选取当Recall >= 0, 0.1, 0.2, ..., 1共11个点时的Precision最大值，AP是这11个Precision的平均值，此时只由11个点去近似PR曲线下面积。\n   $$\n   AP = \\frac{1}{11}\\sum_{r\\in(0,0,,1…1)}\\rho_{interp(r)}\n   $$\n\n   $$\n   \\rho_{interp(r)} = max_{\\widetilde{r}:\\widetilde{r}\\geq{r}}\\rho(\\widetilde{r})\n   $$\n\n   \n\n2. 所有点插值法：\n\n   针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值：\n   $$\n   \\sum_{r=0}^{1}(r_{n+1}-r_n)\\rho_{interp}(r_{n+1})\n   $$\n\n   $$\n   \\rho_{interp(r)} = max_{\\widetilde{r}:\\widetilde{r}\\geq{r}}\\rho(\\widetilde{r})\n   $$\n\n   由于此方法用了所有点去近似PR曲线下面积，计算的AP比11点插值法更准确。\n\n## mAP(mean Average Precision, 即各类别AP的平均值)\n\n对于各个类别，分别按照上述方式计算AP，取所有类别的AP平均值就是mAP。这就是在目标检测问题中mAP的计算方法。可能有时会发生些许变化，如COCO数据集采用的计算方式更严格，其计算了不同IoU阈值和物体大小下的AP.\n\n**在评测时，COCO评估了在不同的交并比(IoU)[0.5:0.05:0.95]共10个IoU下的AP，并且在最后以这些阈值下的AP平均作为结果，记为mAP@[.5, .95]。**\n\n而在Pascal VOC中，检测结果只评测了IOU在0.5这个阈值下的AP值。因此相比VOC而言，COCO数据集的评测会更加全面：不仅评估到物体检测模型的分类能力，同时也能体现出检测模型的定位能力。因此在IoU较大如0.8时，预测框必须和真实的框具有很大的重叠比才能被视为正确。","categories":["计算机视觉"]},{"title":"Deep_Learning学习笔记2|优化模型","url":"/2022/04/05/Deep_Learning学习笔记2/","content":"# Deep_Learning学习笔记2\n\n## 极简手写数字识别模型\n\n### 基础模型：神经网络\n\n- 套用房价预测的模型\n- 输入：由28*28改为784/每个像素值\n- 输出：1，预测的数据值\n\n### 以类的方式组建网络\n\n- 初始化函数：定义每层的函数\n- Forward函数：层之间的串联方式\n\n```python\n# 定义mnist数据识别网络结构，同房价预测网络\nclass MNIST(fluid.dygraph.Layer):\n    def __init__(self, name_scope):\n        super(MNIST, self).__init__(name_scope)\n        name_scope = self.full_name()\n        # 定义一层全连接层，输出维度是1,激活函数为None，即不使用激活函数\n        self.fc = Linear(input_dim=784, output_dim=1, act=None)\n\n    # 定义网络结构的前向计算过程\n    def forward(self, inputs):\n        outputs = self.fc(inputs)\n        return outputs\n```\n\n> input_dim设置为784,即输入为784\n>\n> output_dim为1，即网络层数为1\n>\n> act为None，即不使用激活函数\n>\n> 在init()中申明网络结构，在forward()函数中把这些结构串联，\n\n### 训练过程\n\n- 代码几乎与房价预测任务一致\n- 包含四个部分：\n  - 生成模型实例，设为“训练”状态\n  - 配置优化器，SGD Optimizer\n  - 两层循环的训练过程\n  - 保存模型参数，便于后续使用\n\n- 仅在向模型灌入数据的代码不同\n  - 先转变成np.array格式\n  - 再转换成框架内置格式 to variable\n\n```python\n# 通过with语句创建一个dygraph运行的context\n# 动态图下的一些操作需要在guard下进行\nwith fluid.dygraph.guard():\n    model = MNIST(\"mnist\")\n    # 启动训练模式\n    model.train()\n    # 加载训练集 batch_size 设为 16\n    train_loader = paddle.batch(paddle.dataset.mnist.train(), batch_size=16)\n    # 定义优化器，使用随机梯度下降SGD优化器，学习率设置为0.001\n    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.001,parameter_list=model.parameters())\n    EPOCH_NUM = 10\n    for epoch_id in range(EPOCH_NUM):\n        for batch_id, data in enumerate(train_loader()):\n            # 准备数据并转化成符合框架要求的格式\n            image_data = np.array([x[0] for x in data]). astype('float32')\n            label_data = np.array([x[1] for x in data]). astype('float32').reshape(-1, 1)\n            # 将格式转为飞桨动态图格式\n            image = fluid.dygraph.to_variable(image_data)\n            label = fluid.dygraph.to_variable(label_data)\n            # 前向计算的过程\n            predict = model(image)\n            # 计算损失\n            loss = fluid.layers.square_error_cost(predict, label)\n            avg_loss = fluid.layers.mean(loss)\n            # 每训练了1000批次的数据，打印下当前Loss的情况\n            if batch_id != 0 and batch_id % 1000 == 0:\n                print(\"epoch_id: {}, batch_id: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n            # 后向传播，更新参数的过程\n            avg_loss.backward()\n            optimizer.minimize(avg_loss)\n            model.clear_gradients()\n# 保存模型\nfluid.save_dygraph(model.state_dict(),'mnist1')\n```\n\n每训练1000批次打印的Loss数据如下:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331155448.png)\n\n可以看出训练的效果并不好\n\n- Loss的值并没有在1以下，甚至有的还超过3\n- 从epoch0到epoch9，Loss值总体上下降趋势并不明显\n\n### 测试效果\n\n```python\n# 测试效果\ndef load_image(img_path):\n    im = Image.open(img_path).convert('L')\n    # print(np.array(im))\n    im = im.resize((28, 28), Image.ANTIALIAS)\n    im = np.array(im).reshape(1, -1).astype(np.float32)\n    im = 2 - im / 127.5\n    return im\n\n# 定义预测过程\nwith fluid.dygraph.guard():\n    model = MNIST(\"mnist\")\n    params_file_path = 'mnist3'\n    img_path = './work/example_0.jpg'\n    # 加载数据模型\n    model_dict, _ = fluid.load_dygraph(\"mnist3\")\n    model.load_dict(model_dict)\n\n    model.eval()\n    tensor_img = load_image(img_path)\n    result = model(fluid.dygraph.to_variable(tensor_img))\n    # 预测输出取整，即为预测的数字\n    print(\"本次预测的数字是:\",result.numpy().astype('int32'))\n```\n\n预测的结果如下图所示:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331161622.png)\n\n显然是不准确的\n\n## 优化版手写数字识别模型\n\n### 网络模型\n\n#### 多层感知机\n\n- 代码如下：\n\n  ```python\n  # 定义多层全连接神经网络\n  class MNIST(paddle.nn.Layer):\n      def __init__(self):\n          super(MNIST, self).__init__()\n          # 定义两层全连接隐含层，输出维度是10，当前设定隐含节点数为10，可根据任务调整\n          self.fc1 = Linear(in_features=784, out_features=10)\n          self.fc2 = Linear(in_features=10, out_features=10)\n          # 定义一层全连接输出层，输出维度是1\n          self.fc3 = Linear(in_features=10, out_features=1)\n      # 定义网络的前向计算，隐含层激活函数为sigmoid，输出层不使用激活函数\n      def forward(self, inputs):\n          # inputs = paddle.reshape(inputs, [inputs.shape[0], 784])\n          outputs1 = self.fc1(inputs)\n          outputs1 = F.sigmoid(outputs1)\n          outputs2 = self.fc2(outputs1)\n          outputs2 = F.sigmoid(outputs2)\n          outputs_final = self.fc3(outputs2)\n          return outputs_final\n  ```\n\n  > - 输入层的尺度为28×28，但批次计算的时候会统一加1个维度（大小为batch size）。\n  > - 中间的两个隐含层为10×10的结构，激活函数使用常见的Sigmoid函数。\n  > - 与房价预测模型一样，模型的输出是回归一个数字，输出层的尺寸设置成1。\n\n- 训练效果\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331200615.png)\n\n#### 卷积神经网络\n\n- 代码如下\n\n  ```python\n  # 多层卷积神经网络实现\n  class MNIST(paddle.nn.Layer):\n      def __init__(self):\n          super(MNIST, self).__init__()\n          # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n          self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n          # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n          self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n          # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n          self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n          # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n          self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n          # 定义一层全连接层，输出维度是1\n          self.fc = Linear(input_dim=980, output_dim=1)\n      # 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出\n      # 卷积层激活函数使用Relu，全连接层不使用激活函数\n      def forward(self, inputs):\n          x = self.conv1(inputs)\n          x = F.relu(x)\n          x = self.max_pool1(x)\n          x = self.conv2(x)\n          x = F.relu(x)\n          x = self.max_pool2(x)\n          x = paddle.reshape(x, [x.shape[0], -1])\n          x = self.fc(x)\n          return x\n  ```\n\n- 训练结果如下\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331201203.png)\n\n**比较经典全连接神经网络和卷积神经网络的损失变化，可以发现卷积神经网络的损失值下降更快，且最终的损失值更小。**\n\n### 损失函数\n\n#### 均方误差\n\n上述卷积神经网络的模型的损失函数用的即为均方误差。\n\n#### 交叉熵——sigmoid()\n\n修改计算损失的函数:\n\n- 从：`loss = paddle.nn.functional.square_error_cost(predict, label)`\n- 到：`loss = paddle.nn.functional.cross_entropy(predict, label)`\n\n训练结果如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331210432.png)\n\n### 优化算法\n\n在深度学习神经网络模型中，通常使用标准的随机梯度下降算法更新参数，学习率代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。学习率和深度学习任务类型有关，合适的学习率往往需要大量的实验和调参经验。探索学习率最优值时需要注意如下两点：\n\n- **学习率不是越小越好**。学习率越小，损失函数的变化速度越慢，意味着我们需要花费更长的时间进行收敛，如 **图2** 左图所示。\n- **学习率不是越大越好**。只根据总样本集中的一个批次计算梯度，抽样误差会导致计算出的梯度不是全局最优的方向，且存在波动。在接近最优解时，过大的学习率会导致参数在最优解附近震荡，损失难以收敛，如 **图2** 右图所示。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331210808.png)\n\n#### 设置学习率\n\n在训练前，我们往往不清楚一个特定问题设置成怎样的学习率是合理的，因此在训练时可以尝试调小或调大，通过观察Loss下降的情况判断合理的学习率，设置学习率的代码如下所示。\n\n```python\n#设置不同初始学习率\nopt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n# opt = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())\n# opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n```\n\n#### 学习率的主流优化算法\n\n学习率是优化器的一个参数，调整学习率看似是一件非常麻烦的事情，需要不断的调整步长，观察训练时间和Loss的变化。经过研究员的不断的实验，当前已经形成了四种比较成熟的优化算法：SGD、Momentum、AdaGrad和Adam，效果如下图所示。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331211905.png)\n\n- **SGD**： 随机梯度下降算法，每次训练少量数据，抽样偏差导致的参数收敛过程中震荡。\n- **Momentum**： 引入物理“动量”的概念，累积速度，减少震荡，使参数更新的方向更稳定。\n\n> 每个批次的数据含有抽样误差，导致梯度更新的方向波动较大。如果我们引入物理动量的概念，给梯度下降的过程加入一定的“惯性”累积，就可以减少更新路径上的震荡，即每次更新的梯度由“历史多次梯度的累积方向”和“当次梯度”加权相加得到。历史多次梯度的累积方向往往是从全局视角更正确的方向，这与“惯性”的物理概念很像，也是为何其起名为“Momentum”的原因。类似不同品牌和材质的篮球有一定的重量差别，街头篮球队中的投手（擅长中远距离投篮）喜欢稍重篮球的比例较高。一个很重要的原因是，重的篮球惯性大，更不容易受到手势的小幅变形或风吹的影响。\n\n- **AdaGrad**： 根据不同参数距离最优解的远近，动态调整学习率。学习率逐渐下降，依据各参数变化大小调整学习率。\n\n> 通过调整学习率的实验可以发现：当某个参数的现值距离最优解较远时（表现为梯度的绝对值较大），我们期望参数更新的步长大一些，以便更快收敛到最优解。当某个参数的现值距离最优解较近时（表现为梯度的绝对值较小），我们期望参数的更新步长小一些，以便更精细的逼近最优解。类似于打高尔夫球，专业运动员第一杆开球时，通常会大力打一个远球，让球尽量落在洞口附近。当第二杆面对离洞口较近的球时，他会更轻柔而细致的推杆，避免将球打飞。与此类似，参数更新的步长应该随着优化过程逐渐减少，减少的程度与当前梯度的大小有关。根据这个思想编写的优化算法称为“AdaGrad”，Ada是Adaptive的缩写，表示“适应环境而变化”的意思。RMSProp是在AdaGrad基础上的改进，学习率随着梯度变化而适应，解决AdaGrad学习率急剧下降的问题。\n\n- **Adam**： 由于动量和自适应学习率两个优化思路是正交的，因此可以将两个思路结合起来，这就是当前广泛应用的算法。\n\n#### 利用不同的优化算法训练模型\n\n```python\n#四种优化算法的设置方案，可以逐一尝试效果\n    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n    # opt = paddle.optimizer.Momentum(learning_rate=0.01, momentum=0.9, parameters=model.parameters())\n    # opt = paddle.optimizer.Adagrad(learning_rate=0.01, parameters=model.parameters())\n    # opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n```\n\n### 超参数\n\n在深度学习中，超参数有很多，比如学习率α、使用momentum或Adam优化算法的参数（β1，β2，ε）、层数layers、不同层隐藏单元数hidden units、学习率衰退、mini=batch的大小等。\n\n","tags":["CNN","deep_learning","python"],"categories":["深度学习"]},{"title":"Paper Reading 3|EfficientNet","url":"/2022/04/04/文献阅读3/","content":"\n# 文献阅读3|EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\n\n## Intrudoction\n\n在论文中，作者介绍了**放大卷积神经网络**是一种常见的提高模型准确率的方法。但是在传统的方法中，通常只是在某单一维度上进行放大（**宽度width**，**深度depth**，**图片分辨率resolution**），宽度就是网络中的过滤器的数量，因为增加了过滤器的数量，该层的输出的通道数就相应变大了，深度可以理解为整个网络结构的长度，即网络中layer的数量。那么为什么在这几个维度上进行放大可以提高准确率？因为增加了图片的分辨率或则增加了网络的宽度，网络就能够捕获到更过细节的特征，而增加网络的深度能够捕获到更丰富和更复杂的特征。\n虽然也可以任意的放大两个或三个维度，但是因为维度变多，设计空间也随之变大，因此随意的放大多个维度需要耗费较大的人力来调整，并且也通常会一个次优的精度和效率。因此作者通过研究实验提出了一种新的缩放方法——复合缩放方法(compound scaling method)。\n\n在一些手工设计网络中(如AlexNET、VGG、ResNet等)，我们常常会有这样的疑问：为什么输入图像分辨率要固定为224，为什么卷积的个数要设置为这个值？为什么网络的深度设为这么深？这些问题你要问设计作者的话，估计回复就四个字——工程经验。\n这篇论文使用NAS(Neural Architecture Search)技术来搜索网络的图像输入分辨率r，网络的深度depth，以及channel的宽度width三个参数的合理化配置。\n\t\tEfficientNetB0到B7与其他网络的对比如下图所示:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402161317.png)\n\n为什么在这几个维度上进行放大可以提高准确率呢？因为增加了图片的分辨率或则增加了网络的宽度，网络就能够捕获到更过细节的特征，而增加网络的深度能够捕获到更丰富和更复杂的特征。\n\n虽然也可以任意的放大两个或三个维度，但是因为维度变多，设计空间也随之变大，因此随意的放大多个维度需要耗费较大的人力来调整，并且也通常会有一个次优的精度和效率。因此作者通过研究实验提出了一种新的缩放方法——**复合缩放方法(compound scaling method)**。\n下图所展示的便是放大神经网络的几种方法：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402162025.png)\n\n在之前的论文中，有的会通过增加网络的**width**即增加卷积核的个数(增加特征矩阵的**channels**)来提升网络的性能(上图b)，有的会通过增加网络的**深度**即**使用更多的层结构**来提升网络的性能(上图c)，有的会通过增加输入网络的分辨率来提升网络的性能(上图d)。而在本篇论文中会同时增加网络的**width**、网络的**深度**以及输入网络的**分辨率**来提升网络的性能(上图e)\n\n但是因为网络结构的缩放并不会改变具体某一层的卷积操作，所以一个良好的基线网络是必须的，作者在论文中也提出了一种新的基线网络结构——**EfficientNet**。\n\n## compound scaling method\n\n### 论文思想\n\n- **Depth（d）**：缩放网络深度是许多卷积网络中最常用的方法。*更深的卷积网络能够捕获到更丰富和复杂的特征，但是更深的网络由于存在**梯度消失**的问题而难以训练。*尽管有一些方法可以解决梯度消失（例如 跨层连接skip connections和批量归一化 batch normalization），但是对于非常深的网络所获得的准确率的增益会减弱。例如ResNet-1000和ResNet-101有着相近的准确率尽管depth相差很大。*下图的中间的曲线图表示用不同的深度系数d缩放模型的准确率曲线，并且表明了对于非常深的网络，准确率的增益会减弱。*\n\n  > The intuition is that deeper ConvNet can capture richer and more complex features, and generalize well on new tasks. However, deeper networks are also more difficult to train due to the vanishing gradient problem\n\n- **Width（w）**：缩放网络宽度对于小规模的网络也是很常用的一种方式。*更宽的网络更能够捕捉到更多细节的特征，也更容易训练。*很宽但很浅的网络结构很难捕捉到更高层次的特征。*下图中左边的曲线图则是作者的不同宽度系数实验结果曲线，当w不断增大的时候，准确率很快就饱和了。*\n\n  > wider networks tend to be able to capture more fine-grained features and are easier to train. However, extremely wide but shallow networks tend to have difficulties in capturing higher level features.\n\n- **Resolution（r）**：使用更高分辨率的图像，网络能够捕获到更细粒度的特模式。增加输入网络的图像分辨率能够潜在得获得更高细粒度的特征模板，但对于非常高的输入分辨率，*准确率的增益也会减小，并且大分辨率图像会增加计算量*。*下图中右边的曲线图则是作者的不同分辨率系数实验结果曲线，对于非常高分辨率的图像，准确率的增益会减弱。（r=1.0表示224x224，r=2.5表示560x560）。*\n\n  > With higher resolution input images, ConvNets can potentially capture more fine-grained patterns. but the accuracy gain diminishes for very high resolutions.\n\n下图展示了在基准**EfficientNetB-0**上分别增加**width**、**depth**以及**resolution**后得到的统计结果。通过下图可以看出大概在Accuracy达到80%时就趋于饱和了。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402203414.png)\n\n通过以上实验得出**结论1：对网络深度、宽度和分辨率中的任一维度进行缩放都可以提高精度，但是当模型非常大时，这种放大的增益都会减弱**。\n\n接着作者又做了一个实验，采用不同的d , r 组合，然后不断改变网络的width就得到了如下图所示的4条曲线，通过分析可以发现在相同的FLOPs下，同时增加d和r的效果最好。\n\n### 复合缩放**Compound Scaling**\n\n> 作者通过实验发现缩放的各个维度并不是独立的。直观上来讲，对于分辨率更高的图像，我们应该增加网络深度，因为需要更大的感受野来帮助捕获更多像素点的类似特征。为了证明这种猜测，作者做了一下相关实验：比较宽度缩放在不同深度和分辨率之下对准确率的影响。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402204530.png)\n\n*通过上图的结果我们可以看到d=2.0，r=1.3时宽度缩放在相同flops下有着更高的准确率。*\n得到**结论2：为了达到更好的准确率和效率，在缩放时平衡网络所有维度至关重要。**\n\n为了方便后续理解，我们先看下论文中通过**NAS**(**Neural Architecture Search**)技术搜索得到的EfficientNetB0的结构，如下图所示，整个网络框架由一系列Stage，$\\widehat{Fi}$表示对应stage的运算操作，$\\widehat{Li}$表示在该stage中重复$\\widehat{Fi}$的次数：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402210952.png)\n\n作者在论文中对整个网络的运算进行抽象：$N(d,\\omega,r) = ^{\\bigodot}_{i=1…s}F_i^{L_i}(X_{<H_iW_iC_i>})$\n\n其中：\n\n- $ ^{\\bigodot}_{i=1…s}$表示连乘运算\n- $F_i$表示一个运算操作(如上图中的**operator**)，那么$F_i^{L_i}$表示在Stage_i中$F_i$运算被重复执行$L_i$次。\n- X表示输入Stage_i的特征矩阵(**input tensor**)\n- $<H_i,W_i,C_i>$表示X的高度，宽度以及**Channels**(**shape**)。\n\n为了探究d,r,w这三个因子对最终准确率的影响，作者将d,r,w加到公式中，我们可以得到抽象化后的优化问题(在指定资源限制下)，其中s.t.代表限制条件：\n\n> Our target is to maximize the model accuracy for any given resource constraints, which can be formulated as an optimization problem:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403140239.png)\n\n其中：\n\n- d用来缩放深度$\\widehat{Li}$\n- r用来缩放分辨率即影响$\\widehat{Hi}$和$\\widehat{Wi}$\n- $\\omega$用来缩放特征矩阵的channel即$\\widehat{Ci}$\n- target_memory为memory限制\n- target_flops为FlOPs限制\n\n然后，作者又提出了一种新的复合缩放方法使用了一个复合系数**ϕ** ，通过这个系数按照以下原则来统一的缩放**网络深度**、**宽度**和**分辨率**：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402204755.png)\n\n这里：\n\n- FLOPs(理论计算量)与depth的关系是：当depth翻倍，FLOPs也翻倍。\n- FLOPs与width的关系是：当width翻倍(即channel翻倍)，FLOPs会翻4倍，因为卷积层的FLOPs约等于$feature_\\omega \\times feature_h \\times feature_c \\times kernel_\\omega \\times kernel_h \\times kernel_{number}$(假设输入输出特征矩阵的高宽不变)，当width翻倍，输入特征矩阵的channels($feature_c$)和输出特征矩阵的channel或卷积核的个数($kernel_{number}$)都会翻倍，所以FLOPs会翻4倍。\n- FLOPs与resolution的关系是：当resolution翻倍，FLOPs也会翻4倍，和上面类似因为特征矩阵的宽度$feature_\\omega$和特征矩阵的高度$feature_h$都会翻倍。\n\n所以总的FLOPs倍率可以近似用($({\\alpha \\cdot \\beta^2 \\cdot \\gamma^2})^\\phi$)来表示，当限制${\\alpha \\cdot \\beta^2 \\cdot \\gamma^2} \\approx 2$，对于任意一个$\\phi$而言FLOPs想当增加了$2^\\phi$倍\n\n接下来作者在基准网络EfficientNetB-0上使用NAS来搜索$\\alpha,\\beta,\\gamma$这三个参数。\n\n1. 首先固定$\\phi = 1$，并基于上面给出的公式(2)和(3)进行搜索，作者发现对于EfficientNetB-0最佳参数为\n2. 接着固定$\\alpha=1.2,\\beta=1.1,\\gamma=1.15$，在EfficientNetB-0的基础上使用不同的$\\phi$分别得到EfficientNetB-1至EfficientNetB-7。\n\n需要注意的是，对于不同的基准网络搜索出的α , β , γ 也不一定相同。还需要注意的是，在原论文中，作者也说了，如果直接在大模型上去搜索α , β , γ 可能获得更好的结果，但是在较大的模型中搜索成本太大，所以这篇文章就在比较小的EfficientNetB-0模型上进行搜索的。\n\n> Notably, it is possible to achieve even better performance by searching for α, β, γ directly around a large model, but the search cost becomes prohibitively more expensive on larger models. Our method solves this issue by only doing search once on the small baseline network (step 1), and then use the same scaling coefficients for all other models (step 2).\n\n## 网络详细结构\n\n下表为EfficientNet-B0的网络框架（B1-B7就是在B0的基础上修改**Resolution**，**Channels**以及**Layers**），可以看出网络总共分成了9个**Stage**，第一个Stage就是一个卷积核大小为3x3步距为2的普通卷积层（包含BN和激活函数**Swish**），**Stage2～Stage8**都是在重复堆叠**MBConv**结构（最后一列的**Layers**表示该**Stage**重复**MBConv**结构多少次），而**Stage9**由一个普通的1x1的卷积层（包含BN和激活函数**Swish**）一个平均池化层和一个全连接层组成。表格中每个**MBConv**后会跟一个数字1或6，这里的1或6就是倍率因子n即**MBConv**中第一个1x1的卷积层会将输入特征矩阵的**channels**扩充为n倍，其中k3x3或k5x5表示**MBConv**中**Depthwise Conv**所采用的卷积核大小。**Channels**表示通过该**Stage**后输出特征矩阵的Channels。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403144813.png)\n\n### MBConv\n\nMBConv其实就是MobileNetV3网络中的InvertedResidualBlock，但也有些许区别。一个是采用的激活函数不一样(EfficientNet的MBConv中使用的都是Swish激活函数)，另一个是在每个MBConv中都加入了SE(Squeeze-and-Excitation)模块。\n\n以下结构图为B站UP主[霹雳吧啦Wz](https://space.bilibili.com/18161609)绘制的MBConv结构。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403145314.png)\n\n如图所示，**MBConv**结构主要由一个1x1的普通卷积（升维作用，包含**BN**和**Swish**），一个**kxk**的**Depthwise Conv**卷积（包含**BN**和**Swish**）**k**的具体值可看**EfficientNet-B0**的网络框架主要有3x3和5x5两种情况，一个SE模块，一个1x1的普通卷积（降维作用，包含BN），一个**Droupout**层构成。搭建过程中还需要注意几点：\n\n- 第一个升维的**1x1**卷积层，它的卷积核个数是输入特征矩阵**channel**的n倍， $n \\in \\left\\{1, 6\\right\\}$(n∈{1,6})。\n- 当n = 1时，不要第一个升维的1x1卷积层，即**Stage2**中的**MBConv**结构都没有第一个升维的1x1卷积层（这和**MobileNetV3**网络类似）。\n- 关于**shortcut**连接，仅当输入**MBConv**结构的特征矩阵与输出的特征矩阵**shape**相同时才存在（代码中可通过$stride==1 and inputc_channels==output_channels$条件来判断）。\n- SE模块如下所示，由==一个全局平均池化==，==两个全连接层==组成。第一个全连接层的节点个数是输入该MBConv特征矩阵channels的$\\frac{1}{4}$ 且使用Swish激活函数。第二个全连接层的节点个数等于Depthwise Conv层输出的特征矩阵channels，且使用Sigmoid激活函数。\n- Dropout层的dropout_rate在tensorflow的keras源码中对应的是drop_connect_rate后面会细讲（注意，在源码实现中只有使用shortcut的时候才有Dropout层）。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403153958.png)\n\n### EfficientNet(B0-B7)参数\n\n|     Model      | input_size | width_coefficient | depth_coefficient | drop_connect_rate | dropout_rate |\n| :------------: | :--------: | :---------------: | :---------------: | :---------------: | :----------: |\n| EfficientNetB0 |  224x224   |        1.0        |        1.0        |        0.2        |     0.2      |\n| EfficientNetB1 |  240×240   |        1.0        |        1.1        |        0.2        |     0.2      |\n| EfficientNetB2 |  260x260   |        1.1        |        1.2        |        0.2        |     0.3      |\n| EfficientNetB3 |  300x300   |        1.2        |        1.4        |        0.2        |     0.3      |\n| EfficientNetB4 |  380x380   |        1.4        |        1.8        |        0.2        |     0.4      |\n| EfficientNetB5 |  456x456   |        1.6        |        2.2        |        0.2        |     0.4      |\n| EfficientNetB6 |  528x528   |        1.8        |        2.6        |        0.2        |     0.5      |\n| EfficientNetB7 |  600x600   |        2.0        |        3.1        |        0.2        |     0.5      |\n\n- **input_size**代表训练网络时输入网络的图像大小\n- **width_coefficient**代表**channel**维度上的倍率因子，比如在 **EfficientNetB0**中**Stage1**的3x3卷积层所使用的卷积核个数是32，那么在B6中就是32 × 1.8 = 57.6 32 \\times 1.8=57.632×1.8=57.6接着取整到离它最近的8的整数倍即56，其它**Stage**同理。\n- **depth_coefficient**代表depth维度上的倍率因子（仅针对**Stage2**到**Stage8**），比如在**EfficientNetB0**中**Stage7**的 $\\widehat L_i=4$，那么在B6中就是4 × 2.6 = 10.4 4 \\times 2.6=10.44×2.6=10.4接着向上取整即11。\n- **drop_connect_rate**是在**MBConv**结构中**dropout**层使用的**drop_rate**，在官方keras模块的实现中**MBConv**结构的**drop_rate**是从0递增到**drop_connect_rate**的（具体实现可以看下官方源码，注意，在源码实现中只有使用**shortcut**的时候才有**Dropout**层）。还需要注意的是，这里的**Dropout**层是**Stochastic Depth**，即会随机丢掉整个**block**的主分支（只剩捷径分支，相当于直接跳过了这个**block**）也可以理解为减少了网络的深度。\n- **dropout_rate**是最后一个全连接层前的**dropout**层（在**stage9**的Pooling与FC之间）的**dropout_rate**。\n\n最后是原论文中关于EfficientNet与当时主流网络的性能参数对比:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403155412.png)\n","tags":["计算机视觉","EfficientNet"],"categories":["文献阅读"]},{"title":"文件夹指定软件打开","url":"/2022/04/03/文件夹指定软件打开/","content":"\n# 鼠标右键属性相关\n\n## 右键文件夹指定软件打开\n\nGitHub上下载的代码通常需要对应的编译器打开项目文件，比如我们通常要指定用pycharm或webcharm打开相关文件夹。\n\n教程如下：\n\n1. win+R，输入regedit打开注册表编辑器。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408145825.png)\n\n2.在路径HKEY_CURRENT_USER\\SOFTWARE\\Classes\\Directory\\shell下新建项，命名为Open Folder as XXX Project。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408150134.png)\n\n3. 在此文件夹右击新建字符串值Icon,属性为 软件的本地安装路径\\xxx.exe\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408150400.png)\n\n4. 在Open Folder as XXX Project文件夹下新建项Command,其值修改为 “软件的本地安装路径\\xxx.exe” “%1”\n\n   ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408150617.png)\n\n5. 设置完成，选项即可出现在您右击文件夹之后！\n\n## 删除不需要的鼠标右键属性\n\n1. 打开注册表\n\n   win+R -> input “regedit”\n\n2. 在地址栏输入“HKEY_CLASSES_ROOT\\Directory\\shell”，并回车进入\n3. 进入ContextMenuHandlers项后，可以根据个人的需求，把鼠标右键菜单中不需要项目删掉，或者保留new项，其他的全部删掉即可删除鼠标右键菜单选项。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408151748.png)","categories":["使用技巧"]},{"title":"远处的拉莫","url":"/2022/04/03/远处的拉莫/","content":"\n**“远处的拉莫在看着你，那是你的神。**\n\n**你存在的每一秒，被痛苦占据的每一秒，他都在看着你。**\n\n**有时候你可以感觉到他，但一生只有那么几个瞬间。”**\n\n# 2022.4.3\nP54\n重要的是，我知道痛苦其他的样貌，它们像是白色的羽毛，像是水面上的烟火，像是雪山的幽灵，它们是一切不可诉说的、静默在永恒里的、被掩埋着的枯萎、灰败和消亡。\n\nP62\n很久之前，我就告诉自己不能被任何所击垮，只要有一瞬间的崩塌，便会迅速瓦解。\n\nP68\n人就是这样，要有比他们自身更糟的东西在上方控制着他们，才能不处于濒死的状态。\n\nP69\n有一种叫作塌陷的感受，几乎每次入睡时都会溢出来，最开始是胸腔，然后是腹部、膝盖，向上抵达脊椎，向下抵达生殖器，最后是四肢的末端，全部塌陷，然后进入睡眠。\n\nP74\n那些聪明人，从古至今追求着智慧的人，他们令文明得到进化，逐利使文明扩张，扩张代表着侵蚀、封锁、屠杀，然而仍有奔向智慧的人，一切糟糕的结果由他们而起，他们进化着文明的同时，让更野蛮的力量得以无限扩张。这从来都不是双刃剑，一直都是通向此刻的必然。\n\nP79\n上一代人总是会不遗余力地压制下一代，这与进化的意志相反。在我吃这橘子的那个夜晚，我的朋友说。他的女朋友坐在一侧，腿放在他手腕上。\n你被压制什么了？我说\n我被不剥夺了很多，也对抗不了，他们扣押了我所有的版权。就像现在，我把这些称作邪恶，但可能二十年后，我也会这么干。我剥夺年轻人，压制他们，利用他们，可能只是因为他们拥有的东西令我心烦。他揉着那个女人的脚，我能看出这中间有种色情的意味，情侣喜欢在公开场合以不起眼的方式调情，这种色情使他们有乐趣。如果没有旁观者，充斥在这里的只剩下乏味。\n我能理解你，就你所说的这种邪恶，人们会在不同的年龄以不同的方式发作出来。我说。\n童年时是什么？他说。\n杀戮。\n杀戮？他的女朋友抽回了脚。\n我总觉得，虽然所有阶段都会产生杀戮，但杀戮始于童年，你身边更为强大的个体告诉你杀戮是可怕的。某个儿童敲死一片蚂蚁，这被认为是不好的，但这个不好，只是因为你屈服于周围的强大，毕竟那段日子，你没有选择任何事物的权利。\n所以呢？朋友说。\n所以杀戮被掩埋住，在一些年代以别的方式发泄出来。像你所说的，在一些年代你被剥夺了，在另一些年代以直接的杀戮呈现。\n哈哈，那青年呢？女人问。\n侵占。\n我没觉得自己在侵占什么啊。朋友说。\n让自己覆盖更多的事物，侵占所有可以看得到的。我仔细想想，我觉得这个民族的自负跟这个有关系。这个民族，还停留在青年人的阶段，也就是一个侵占的时期，必然会认为自己无所不能。\n那中年呢？\n我还不知道，但我观察到，中年已经开始向毁灭过渡了，不计任何后果地令世界丑陋下去。\n你这样看待周遭，因此活得糟糕透顶。朋友说。\n我无论怎么看待，这都是注定的。你能想象十几年之后的样子吗？我们还能坐在这里，你递给我两个橘子，你虚伪地跟我说起这漫长的友谊，你讲起我们的过去那看起来好玩的事情。但到了某些情况下，即便是很脆弱的情况，我认为所有人也会毫不犹豫地获得那个强大的本能。\n十几年后，我们已经结婚很多年，有了两个孩子，会告诫他们不能变成你这样。女人笑着说。\n他们的狗过来咬着我的拖鞋。\n\nP83\n愚笨是因为安逸。\n\nP84\n“我们无法触碰，亦不可调和”\n\nP85\n我们，与什么事物调和过呢？我抬起自己的手，看着那条渐变如山脊的伤痕，上面沾满了尘土，我与自身的伤口都无法调和。\n\nP92\n“其实听别人的故事，不会让你感受到什么。”\n\n# 2022.4.9\nP100\n实际上，喝酒这件事，不需要破产或者家破人亡，哪怕摔伤了膝盖，或者一根手指不小心被划伤，都可以喝酒。\n\nP136\n“爱情有一个衰变期，如果之前没有变化的话，便会走向终结。”\n\nP147\n我喜欢庸俗的女人，以前还没有发现，现在我很确定了。比如归结到容貌、性格，或者其他乱七八糟的，根本不是。我只是喜欢庸俗的女人。她们考虑事情的角度差不多，有时候她们很聪明，但不会超过算清五毛钱的账。我很鄙视自己这一点，但不能控制。一开始我总以为是什么特别神秘的缘由，最后结果都是，我发现我们的生活就是坐在那，她可以做一晚上毛线球，我就在一旁刷手机，从下午到凌晨，之后我会打开窗户，如果有啤酒我也会开一瓶，站在窗前就好像发现了什么可悲的事情一样。其实一直如此，可能我三岁时就已经这样了，喜欢庸俗的女人。我们互相讲着社交网络上看来的笑话，就跟是自己身上发生的一样，再开怀大笑。有时我能笑得哭出来，但是没办法，我好像只能做这些事。比如她洗澡时会放三五年前的流行音乐，我听了也会很伤感，眼前浮现一个涂着星空眼影的过气女歌手，她一开口台下的人就开始哭，我听了也想哭，但其实我没什么好哭的。等她洗完澡走出来，我看着她，目光里都是，天啊这是世上最漂亮的女人了。就是这样的。起码今天就是这样的。”\n\n# 2022.4.10\nP173\n我所珍藏的东西，总是在触碰的时候就轻易瓦解成粉尘，这便是一种可以称为陷阱的东西。\n\nP184\n我越是去经历和感受些什么，她就会越来越远离我。\n\nP187\n也就是说，那些十几年前所期待的——虽然我并不知道在期待什么——都没有发生，构成我生活的每一部分都原封不动地矗立在这里。","tags":["阅读","文摘"],"categories":["解体与救赎"]},{"title":"Yolov5学习笔记9|YOLOv5-Face|改进原理解读与论文复现","url":"/2022/03/30/Yolov5学习笔记9/","content":"\n# Yolov5改进|YOLOv5-Face|改进原理解读与论文复现\n\n## YOLOv5Face的设计目标和主要贡献\n\n### 设计目标\n\nYOLOv5Face针对人脸检测的对YOLOv5进行了再设计和修改，考虑到大人脸、小人脸、Landmark监督等不同的复杂性和应用。YOLOv5Face的目标是为不同的应用程序提供一个模型组合，从非常复杂的应用程序到非常简单的应用程序，以在嵌入式或移动设备上获得性能和速度的最佳权衡。\n\n### 主要贡献\n\n1. 重新设计了YOLOV5来作为一个人脸检测器，并称之为YOLOv5Face。对网络进行了关键的修改，以提高平均平均精度(mAP)和速度方面的性能；\n2. 设计了一系列不同规模的模型，从大型模型到中型模型，再到超小模型，以满足不同应用中的需要。除了在YOLOv5中使用的Backbone外，还实现了一个基于ShuffleNetV2的Backbone，它为移动设备提供了最先进的性能和快速的速度；\n3. 在WiderFace数据集上评估了YOLOv5Face模型。在VGA分辨率的图像上，几乎所有的模型都达到了SOTA性能和速度。这也证明了前面的结论，不需要重新设计一个人脸检测器，因为YOLO5就可以完成它。\n\n## YOLOv5-Face的结构\n\n###  YOLOv5-Face模型架构\n\n![YOLOv5-Face架构图](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329211411.png)\n\n> YOLOv5Face是以YOLOv5作为Baseline来进行改进和再设计以适应人脸检测。这里主要是检测小脸和大脸的修改。\n\nYOLO5人脸检测器的网络架构如图1所示。它由Backbone、Neck和Head组成，描述了整体的网络体系结构。在YOLOv5中，使用了CSPNet Backbone。在Neck中使用了SPP和PAN来融合这些特征。在Head中也都使用了回归和分类。\n\n#### CBS Block\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329211622.png)\n\n在上图中重新定义了一个CBS Block，它由Conv、BN和SiLU激活函数组成。但其实架构和Yolov5的一样。\n\n对应的代码如下：\n\n```python\nclass Conv(nn.Module):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super(Conv, self).__init__()\n        # 卷积层\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        # BN层\n        self.bn = nn.BatchNorm2d(c2)\n        # SiLU激活层\n        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n    def fuseforward(self, x):\n        return self.act(self.conv(x))\n```\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329212205.png)\n\n\n\n在上图中显示了Head的输出标签，其中包括边界框(bbox)、置信度(conf)、分类(cls)和5-Point Landmarks。这些Landmarks是对YOLOv5的改进点，使其成为一个具有Landmarks输出的人脸检测器。如果没有Landmarks，最后一个向量的长度应该是6而不是16。\n\n**请注意，P3中的输出尺寸80×80×16，P4中的40×40×16，P5中的20×20×16，可选P6中的10×10×16为每个Anchor。实际的尺寸应该乘以Anchor的数量。**\n\n#### Stem Block\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220519.png)\n\n上图为stem，它同于取代Yolov5中原来的Focus层(实际上在yolov5-v5.0之后就没有Focus层了)。在Yolov5中引入Stem模块用于人脸检测时Yolov5-Face创新之一。\n\n```python\nclass StemBlock(nn.Module):\n    def __init__(self, c1, c2, k=3, s=2, p=None, g=1, act=True):\n        super(StemBlock, self).__init__()\n        # 3×3卷积\n        self.stem_1 = Conv(c1, c2, k, s, p, g, act)\n        # 1×1卷积\n        self.stem_2a = Conv(c2, c2 // 2, 1, 1, 0)\n        # 3×3卷积\n        self.stem_2b = Conv(c2 // 2, c2, 3, 2, 1)\n        # 最大池化层\n        self.stem_2p = nn.MaxPool2d(kernel_size=2,stride=2,ceil_mode=True)\n        # 1×1卷积\n        self.stem_3 = Conv(c2 * 2, c2, 1, 1, 0)\n\n    def forward(self, x):\n        stem_1_out  = self.stem_1(x)\n        stem_2a_out = self.stem_2a(stem_1_out)\n        stem_2b_out = self.stem_2b(stem_2a_out)\n        stem_2p_out = self.stem_2p(stem_1_out)\n        out = self.stem_3(torch.cat((stem_2b_out,stem_2p_out),1))\n        return out\n```\n\n用Stem模块替代网络中原有的Focus模块，**提高了网络的泛化能力，降低了计算复杂度，同时性能也没有下降**。\n\n```python\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, StemBlock, [64, 3, 2]],  # 0-P1/2\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],      # 2-P3/8\n   [-1, 9, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],      # 4-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],     # 6-P5/32\n   [-1, 1, SPP, [1024, [3,5,7]]],\n   [-1, 3, C3, [1024, False]],      # 8\n  ]\n```\n\nStem模块的图示中虽然都是用的CBS，但是看代码可以看出来第2个和第4个CBS是1×1卷积，第1个和第3个CBS是3×3，stride=2的卷积。配合yaml文件可以看到stem以后图像大小由640×640变成了160×160。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220426.png)\n\n在上图中，显示了一个CSP Block(C3)。CSP Block的设计灵感来自于DenseNet。但是，不是在一些CNN层之后添加完整的输入和输出，输入被分成 2 部分。其中一半通过一个CBS Block，即一些Bottleneck Blocks，另一半是经过Conv层进行计算：\n\n```python\nclass C3(nn.Module):\n    # CSP Bottleneck with 3 convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super(C3, self).__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n\n    def forward(self, x):\n        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n```\n\n#### Bottleneck Block\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220241.png)\n\n上图即为C3模块中的Bottleneck层。\n\n```python\nclass Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super(Bottleneck, self).__init__()\n        c_ = int(c2 * e)  # hidden channels\n        #第1个CBS模块\n        self.cv1 = Conv(c1, c_, 1, 1)\n        #第2个CBS模块\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        #元素add操作\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n```\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220924.png)\n\n上图为SPP Block、。YOLOv5Face在这个Block中把YOLOv5中的13×13,9×9,5×5的kernel size被修改为7×7,5×5,3×3，这个改进更适用于人脸检测并提高了人脸检测的精度。\n\n```python\nclass SPP(nn.Module):\n    # 这里主要是讲YOLOv5中的kernel=(5,7,13)修改为(3, 5, 7)\n    def __init__(self, c1, c2, k=(3, 5, 7)):\n        super(SPP, self).__init__()\n        c_ = c1 // 2  # hidden channels\n        # 对应第1个CBS Block\n        self.conv1 = Conv(c1, c_, 1, 1)\n        # 对应第2个 cat后的 CBS Block\n        self.conv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n        # ModuleList=[3×3 MaxPool2d,5×5 MaxPool2d,7×7 MaxPool2d]\n        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return self.conv2(torch.cat([x] + [m(x) for m in self.m], 1))\n```\n\n同时，YOLOv5Face添加一个stride=64的P6输出块，P6可以提高对大人脸的检测性能。（之前的人脸检测模型大多关注提高小人脸的检测性能，这里作者关注了大人脸的检测效果，提高大人脸的检测性能来提升模型整体的检测性能）。P6的特征图大小为10x10。\n\n> 同时，YOLOv5Face添加一个stride=64的P6输出块，P6可以提高对大人脸的检测性能。（之前的人脸检测模型大多关注提高小人脸的检测性能，这里作者关注了大人脸的检测效果，提高大人脸的检测性能来提升模型整体的检测性能）。P6的特征图大小为10x10。\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n###  输入改进\n\nYOLOv5Face作者发现一些目标检测的数据增广方法并不适合用在人脸检测中，包括上下翻转和Mosaic数据增广。**删除上下翻转可以提高模型性能**。**对小人脸进行Mosaic数据增广反而会降低模型性能**，但是**对中尺度和大尺度人脸进行Mosaic可以提高性能**。**随机裁剪有助于提高性能**。\n\n> 这里主要还是COCO数据集和WiderFace数据集尺度有差异，WiderFace数据集小尺度数据相对较多。\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n### Landmark回归\n\nLandmark是人脸的重要特征。它们可以用于人脸比对、人脸识别、面部表情分析、年龄分析等任务。传统Landmark由68个点组成。它们被简化为5点时，这5点Landmark就被广泛应用于面部识别。人脸标识的质量直接影响人脸对齐和人脸识别的质量。\n\n一般的物体检测器不包括Landmark。可以直接将其添加为回归Head。因此，作者将它添加到YOLO5Face中。Landmark输出将用于对齐人脸图像，然后将其发送到人脸识别网络。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330085638.png)\n\n用于Landmark回归的一般损失函数为L2、L1或smooth-L1。MTCNN使用的就是L2损失函数。然而，作者发现这些损失函数对小的误差并不敏感。为了克服这个问题，提出了Wing loss:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330085731.png)\n\nw: 正数w将非线性部分的范围限制在[-w,w]之间；\n\n$\\varepsilon $: 约束非线性区域的曲率，并且$C=\\omega-\\omega ln(1+\\frac{x}{\\varepsilon)}$是一个常数，可以平滑的连接分段的线性和非线性部分。$\\varepsilon$的取值是一个很小的数值，因为它会使网络训练变得不稳定，并且会因为很小的误差导致梯度爆炸问题。\n\n> 实际上，的Wing loss函数的非线性部分只是简单地采用ln(x)在[$\\frac{\\varepsilon}{\\omega},1+\\frac{\\varepsilon}{\\omega}$]之间的曲线，并沿X轴和Y轴将其缩放比例为w。另外，沿y轴应用平移以使wing(0)=0，并在损失函数上施加连续性。\n\n#### landmark的获取：\n\n```python\n#landmarks\nlks = t[:,6:14]\nlks_mask = torch.where(lks < 0, torch.full_like(lks, 0.), torch.full_like(lks, 1.0))\n#应该是关键点的坐标除以anch的宽高才对，便于模型学习。使用gwh会导致不同关键点的编码不同，没有统一的参考标准\nlks[:, [0, 1]] = (lks[:, [0, 1]] - gij)\nlks[:, [2, 3]] = (lks[:, [2, 3]] - gij)\nlks[:, [4, 5]] = (lks[:, [4, 5]] - gij)\nlks[:, [6, 7]] = (lks[:, [6, 7]] - gij)\n```\n\nWing Loss的计算如下：\n\n```python\nclass WingLoss(nn.Module):\n    def __init__(self, w=10, e=2):\n        super(WingLoss, self).__init__()\n        # https://arxiv.org/pdf/1711.06753v4.pdf   Figure 5\n        self.w = w\n        self.e = e\n        self.C = self.w - self.w * np.log(1 + self.w / self.e)\n \n    def forward(self, x, t, sigma=1):  #这里的x，t分别对应之后的pret，truel\n        weight = torch.ones_like(t) #返回一个大小为1的张量，大小与t相同\n        weight[torch.where(t==-1)] = 0\n        diff = weight * (x - t)\n        abs_diff = diff.abs()\n        flag = (abs_diff.data < self.w).float()\n        y = flag * self.w * torch.log(1 + abs_diff / self.e) + (1 - flag) * (abs_diff - self.C) #全是0，1\n        return y.sum()\n \nclass LandmarksLoss(nn.Module):\n    # BCEwithLogitLoss() with reduced missing label effects.\n    def __init__(self, alpha=1.0):\n        super(LandmarksLoss, self).__init__()\n        self.loss_fcn = WingLoss()#nn.SmoothL1Loss(reduction='sum')\n        self.alpha = alpha\n \n    def forward(self, pred, truel, mask): #预测的，真实的 600（原来为62*10）(推测是去掉了那些没有标注的值)\n        loss = self.loss_fcn(pred*mask, truel*mask)  #一个值（tensor）\n        return loss / (torch.sum(mask) + 10e-14)\n```\n\n#### **分析比较L1，L2和Smooth L1损失函数**\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092332.png)\n\n其中s是人脸关键点的ground-truth,函数f(x)就等价于：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092354.png)\n\n损失函数对x的导数分别为:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092423.png)\n\nL2损失函数，**当x增大时L2 loss对x的导数也增大，这就导致训练初期，预测值与ground-truth差异过大时，损失函数对预测值的梯度十分大，导致训练不稳定**。\n\nL1 loss的导数为常数，在训练后期，预测值与ground-truth差异很小时， 损失对预测值的导数的绝对值仍然为1，此时学习率(learning rate)如果不变，**损失函数将在稳定值附近波动，难以继续收敛达到更高精度**。\n\nsmooth L1损失函数，**在x较小时，对x的梯度也会变小，而在x很大时，对x的梯度的绝对值达到上限 1，也不会太大以至于破坏网络参数。smooth L1完美地避开了L1和L2损失的缺陷**。\n\n此外，根据fast rcnn的说法，\"… L1 loss that is less sensitive to outliers than the L2 loss used in R-CNN and SPPnet.\" 也就是**smooth L1让loss对于离群点更加鲁棒，即相比于L2损失函数，其对离群点、异常值（outlier）不敏感，梯度变化相对更小，训练时不容易跑飞**。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092522.png)\n\n上图描绘了这些损失函数的曲线图。需要注意的是，Smoolth L1损失是Huber损失的一种特殊情况，L2损失函数在人脸关键点检测中被广泛应用，然而，L2损失对异常值很敏感。\n\n#### 为什么是Wing Loss？\n\n上一部分中分析的所有损失函数在出现较大误差时表现良好。这说明神经网络的训练应更多地关注具有小或中误差的样本。为了实现此目标，提出了一种新的损失函数，即基于CNN的面部Landmark定位的Wing Loss。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092732.png)\n\n> 当NME在0.04的时候，测试数据比例已经接近1了，所以在0.04到0.05这一段，也就是所谓的large errros段，并没有分布更多的数据，说明各损失函数在large errors段都表现很好。\n>\n> 模型表现不一致的地方就在于small errors和medium errors段，例如，在NME为0.02的地方画一根竖线，相差甚远的。因此作者提出训练过程中应该更多关注samll or medium range errros样本。\n\n可以使用ln x来增强小误差的影响，它的梯度是$\\frac{1}{x}$,对于接近0的值就会越大,optimal step size为$x^2$，这样gradient就由small errors“主导”，step size由large errors“主导”。这样可以恢复不同大小误差之间的平衡。\n\n但是，为了防止在可能的错误方向上进行较大的更新步骤，重要的是不要过度补偿较小的定位错误的影响。这可以通过选择具有正偏移量的对数函数来实现。\n\n但是这种类型的损失函数适用于处理相对较小的定位误差。在wild人脸关键点检测中，可能会处理极端姿势，这些姿势最初的定位误差可能非常大，在这种情况下，损失函数应促进从这些大错误中快速恢复。这表明损失函数的行为应更像L1或L2。由于L2对异常值敏感，因此选择了L1。\n\n所以，对于小误差，它应该表现为具有偏移量的对数函数，而对于大误差，则应表现为L1。因此复合损失函数Wing Loss就诞生了。\n\n#### Yolov5-Face的后处理NMS\n\n其实本质上没有改变，这里仅仅给出对比的代码。\n\nYolov5的NMS代码如下：\n\n```python\ndef non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, labels=()):\n    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n    Returns:\n         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n    \"\"\"\n \n    nc = prediction.shape[2] -5  # number of classes\n```\n\nYolov5-Face的NMS代码如下：\n\n```\ndef non_max_suppression_face(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, labels=()):\n    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n    Returns:\n         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n    \"\"\"\n    # 不同之处\n    nc = prediction.shape[2] - 15  # number of classes\n```\n\n## 论文复现\n\n- 源码地址: https://github.com/deepcam-cn/yolov5-face\n\n- widerface数据集: https://drive.google.com/file/d/1tU_IjyOwGQfGNUvZGwWWM4SwxKp2PUQ8/view?usp=sharing\n\n> 下载后，解压缩位置放到yolov5-face-master项目里data文件夹下的widerface文件夹下。\n\n- 运行train2yolo.py和val2yolo.py\n\n> 把数据集转成yolo的训练格式，\n\n- 运行train.py\n\n## OpenCV-C++部署\n\n### 参数配置\n\n该部分主要是输入输出尺寸、Anchor以及Strides设置等。\n\n代码如下:\n\n```python\n const float anchors[3][6] = { {4,5,  8,10,  13,16}, \n                               {23,29,  43,55,  73,105},\n                               {146,217,  231,300,  335,433} };\n const float stride[3] = { 8.0, 16.0, 32.0 };\n const int inpWidth = 640;\n const int inpHeight = 640;\n float confThreshold;\n float nmsThreshold;\n float objThreshold;\n```\n\n### 模型加载以及Sigmoid的定义\n\n该部分主要设置ONNX模型的加载。\n\n```python\nYOLO::YOLO(Net_config config)\n{\n cout << \"Net use \" << config.netname << endl;\n this->confThreshold = config.confThreshold;\n this->nmsThreshold = config.nmsThreshold;\n this->objThreshold = config.objThreshold;\n strcpy_s(this->netname, config.netname.c_str());\n\n string modelFile = this->netname;\n modelFile += \"-face.onnx\";\n this->net = readNet(modelFile);\n}\n\nvoid YOLO::sigmoid(Mat* out, int length)\n{\n float* pdata = (float*)(out->data);\n int i = 0; \n for (i = 0; i < length; i++)\n {\n  pdata[i] = 1.0 / (1 + expf(-pdata[i]));\n }\n}\n```\n\n### 后处理部分\n\n这里对坐标的处理和Yolov5保持一致，但是由于多出来的Landmark，所以也多出了这一部分的处理:\n\n```python\n  if (box_score > this->objThreshold)\n     {\n      // 该部分与yolov5的保持一致\n      float face_score = sigmoid_x(pdata[15]);\n      float cx = (sigmoid_x(pdata[0]) * 2.f - 0.5f + j) * this->stride[n];  ///cx\n      float cy = (sigmoid_x(pdata[1]) * 2.f - 0.5f + i) * this->stride[n];   ///cy\n      float w = powf(sigmoid_x(pdata[2]) * 2.f, 2.f) * anchor_w;   ///w\n      float h = powf(sigmoid_x(pdata[3]) * 2.f, 2.f) * anchor_h;  ///h\n\n      int left = (cx - 0.5*w)*ratiow;\n      int top = (cy - 0.5*h)*ratioh;   \n\n      confidences.push_back(face_score);\n      boxes.push_back(Rect(left, top, (int)(w*ratiow), (int)(h*ratioh)));\n      // landmark的处理\n      vector<int> landmark(10);\n      for (k = 5; k < 15; k+=2)\n      {\n       const int ind = k - 5;\n       landmark[ind] = (int)(pdata[k] * anchor_w + j * this->stride[n])*ratiow;\n       landmark[ind + 1] = (int)(pdata[k + 1] * anchor_h + i * this->stride[n])*ratioh;\n      }\n      landmarks.push_back(landmark);\n     }  \n```\n\n## 参考文献\n\n[1].https://github.com/hpc203/yolov5-face-landmarks-opencv-v2\n[2].https://github.com/deepcam-cn/yolov5-face\n[3].YOLO5Face: Why Reinventing a Face Detector\n[4].https://zhuanlan.zhihu.com/p/375966269\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Deep_Learning学习笔记1","url":"/2022/03/25/Deep_Learning学习笔记1/","content":"\n#  Deep_Learning学习笔记——深度神经网络(DNN)实现手写数字识别\n\n深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。\n\n## 从感知机到神经网络\n\n感知机接收多个输入信号，输出一个信号。如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-8309413cef2521d53a8e0f8b82bc0e0c_r.jpg)\n\n输出和输入之间学习到一个线性关系，得到中间输出结果：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-91698dfe8a2cbf280d728fcda98dc6fb_r.jpg)\n$$\nx_i代表人们选择的输入信号，w_i为感知机的内部参数，称为权重，上图中的○通常称为“神经元”或“节点”。\n$$\n感知机的多个输入都有各自的权重，权重越大，对应信号的重要性就越高。\n\n接着是一个神经元激活函数：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-8d892ca795f7ef652b1b63a0f2335052_r.jpg)\n\n当输出1时，称此神经元被激活，其中w是体现输入信号重要性的参数，而偏置b是调整神经元被激活的容易程度的参数。有时将w，b统称为权重。\n\n这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。而神经网络则在感知机的模型上做了**扩展**，总结下主要有三点：\n\n1. **加入了隐藏层**：隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。\n2. **输出层的神经元也可以不止一个输出，可以有多个输出**，，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。\n3. （3）对激活函数做扩展，感知机的激活函数是sign(z) ,虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：\n\n$$\nf(z)=\\frac{1}{1+e^{-z}}\n$$\n\n还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。\n\n## DNN基本结构\n\n神经网络是基于感知机的扩展，而DNN可以理解为有很多隐藏层的神经网络。多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机(Multi-Layer perceptron,MLP)。\n\n从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-1afa0c7d95bea01c038d82deca9d683b_720w.jpg)\n\n层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系![](F:\\Blog\\picture\\equation1-1647673870094.svg) 加上一个激活函数 ![](F:\\Blog\\picture\\equation-1647661061238-1647673900849.svg)。\n\n**首先看线性关系系数w的定义。**以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性关系定义为![](F:\\Blog\\picture\\equation-1647673316596-1647673958389.svg).上标3代表线性系数w所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。你也许会问，为什么不是![](F:\\Blog\\picture\\equation-1647673316631-1647674001886.svg)呢？这主要是为了便于模型用于矩阵表示运算，如果是![](F:\\Blog\\picture\\equation-1647673316631-1647674022987.svg)而每次进行矩阵运算是![](F:\\Blog\\picture\\equation-1647673316732-1647674053223.svg)，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置，即直接为![](F:\\Blog\\picture\\equation-1647673316774-1647674086090.svg)。第i−1层的第k个神经元到第l层的第j个神经元的线性系数定义为![](F:\\Blog\\picture\\equation-1647673316822-1647674145381.svg)。注意，输入层是没有w参数的。\n\n**再看偏倚b的定义。**还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为$b^2_3$.其中，上标2代表所在的层数，下标3代表偏倚所在的神经元的索引。同样的道理，第三层的第一个神经元的偏倚应该表示为$a^3_1$ .输出层是没有偏倚参数的。\n\n## DNN反向传播算法\n\n在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。\n\n### 损失函数\n\n在神经网络中，衡量网络预测结果$y=F(x)$与真实值$y$之间差别的指标称为**损失函数**。损失函数值越小，表示神经网络的预测结果越接近真实值。神经网络进行分类和回归任务时会使用不同的损失函数，下面列出一些常用的分类损失和回归损失。\n\n### 分类损失函数\n\n1. Logistic损失:\n\n$$\nloss(\\widehat{y},y) = \\prod_{i=1}^{N}\\widehat{y}_i^{y_i}·{(1-\\widehat{y}_i)}^{1-y_i}\n$$\n\n2. 负对数似然损失\n\n3. 交叉熵损失\n\n### 回归损失函数\n\n1. 均方误差，也称L2损失\n2. 平均绝对误差，也称L1损失\n3. 均方对数差损失\n4. HUber损失\n5. Log-Cosh损失函数\n\n## DNN反向传播算法过程\n\n由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种，为了简化描述，这里我们以最基本的批量梯度下降法为例来描述反向传播算法。实际上在业界使用最多的是mini-Batch的梯度下降法。区别仅仅在于迭代时训练样本的选择。\n\n输入：总层数**L**，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长**a**，最大迭代次数**max**与停止迭代阈值$\\varepsilon$，输入的m个训练样本\n\n输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量。","tags":["deeplearning","DNN"],"categories":["深度学习"]},{"title":"Paper Reading 2|YoloF：You Only Look One-level Feature","url":"/2022/03/25/文献阅读2/","content":"\n# 文献阅读2-YoloF：You Only Look One-level Feature\n\n> 本文是旷视科技&中科院孙剑团队在单阶段目标检测方面一次突破性的创新，它针对单阶段目标检测中的FPN(特征金字塔)进行了深入的分析并得出：FPN最重要的成分是分而治之的处理思路缓解了优化难问题。针对FPN的多尺度特征、分而治之思想分别提出了Dilated编码器提升特征感受野，Uniform Matching进行不同尺度目标框的匹配；结合所提两种方案得到了本文的YOLOF，在COCO数据集上，所提方案取得了与RetinaNet相当的性能且推理速度快2.5倍；所提方法取得了与YOLOv4相当的性能且推理速度快13%。\n\n## Abstract\n\n本文对单阶段目标检测中的FPN进行了重思考并指出**FPN的成功之处在于它对目标检测优化问题的分而治之解决思路而非多尺度特征融合**。从优化的角度出发，作者引入了另一种方式替换复杂的特征金字塔来解决该优化问题：从而可以**仅仅采用一级特征进行检测**。基于所提简单而有效的解决方案，作者提出了YOLOF(You Only Look One-level Feature)。\n\nYOLOF有两个关键性模块：Dilated Encoder与Uniform Matching，它们对最终的检测带来了显著的性能提升。COCO基准数据集的实验表明了所提YOLOF的有效性，YOLOF取得与RetinaNet-FPN同等的性能，同时快2.5倍；无需transformer层，YOLOF仅需一级特征即可取得与DETR相当的性能，同时训练时间少7倍。以608×608大小的图像作为输入，YOLOF取得了44.3mAP的指标且推理速度为60fps@2080Ti，它比YOLOv4快13%。\n\n本文的贡献主要包含以下几点：\n\n- FPN的关键在于针对稠密目标检测优化问题的“分而治之”解决思路，而非多尺度特征融合；\n- 提出了一种简单而有效的无FPN的基线模型YOLOF，它包含两个关键成分(Dilated Encoder与Uniform Matching)以减轻与FPN的性能差异；\n- COCO数据集上的实验证明了所提方法每个成分的重要性，相比RetinaNet，DETR以及YOLOv4，所提方法取得相当的性能同时具有更快的推理速度。\n\n## Introduction\n\n本文主要针对单阶段检测器中的FPN的两个重要因素进行了研究，作者以RetinaNet为基线，通过解耦**多尺度特征融合**、**分而治之**进行实验设计。作者将FPN视作多输入多输出编码器(MiMo，见下图)，它对骨干网络的多尺度特征进行编码并为后接的解码器提供多尺度特征表达。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325110700.png)\n\n为进行更好的对比分析，作者设计了MiMo、SiMo、MiSo、SiSo等四种类型的解码器，见上图。令人惊艳的是：**SiMo编码器仅仅采用C5特征且不进行特征融合即可取得与MiMo编码器相当的性能**，且性能差异小于1mAP。相反，**MiSo编码器的性能则出现了显著下降**。这个现象意味着：\n\n- C5包含了充分的用于检测不同尺度目标的上下文信息，这促使SiMo编码器可以取得与MiMo相当的结果；\n- 多尺度特征融合带来的收益要远小于分而治之带来的收益，因此多尺度特征融合可能并非FPN最重要的影响因素；相反，分而治之将不同尺度的目标检测进行拆分处理，缓解了优化问题。\n\n##  Cost Analysis of MiMo Encoders\n\n如前所述FPN的成功在于它对于优化问题的解决思路，而非多尺度特征融合。为说明这一点，作者对FPN(即MiMo)进行了简单的分析。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325110918.png)\n\n以RetinaNet-ResNet50为基线方案，作者将检测任务的流水线分解为三个关键部分：骨干网络、Encoder以及Decoder。下图给出了不同部分的Flops对比，可以看到：\n\n- 相比SiMoEncoder，MiMoEncoder带来显著的内存负载问题(134G vs 6G)；\n- 基于MiMoEncoder的检测器推理速度明显要慢于SiSoEncoder检测器(13FPS vs 34FPS)；\n- 这个推理速度的变慢主要是因为高分辨率特征部分的目标检测导致，即C3特征部分。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648177868000.png)\n\n基于上述分析，作者期望寻找另一种解决优化问题的方案，且保持检测器检测、精确、快速。\n\n##  Method\n\n受上述目标驱动以及新发现：C5特征包含足够的信息进行大量目标检测，作者尝试用简单的SiSoEncoder替换复杂的MiCoEncoder。但是，这种简单的替换会带来显著性的性能下降(35.9mAP vs 23.7mAP)，见上图。对于这种情况 ，作者进行了仔细分析得出SiSoEncoder性能下降的两个重要原因：\n\n- The range of scales matching to the C5 feature's receptive field is limited\n- The imbalance problem on positive anchors\n\n接下来，作者将针对这两个问题进行讨论并提出对应的解决方案。\n\n### Limited Scale Range\n\n识别不同尺寸的目标是目标检测的一个根本挑战。一种常见的方案是采用多级特征。在MiMo与SiMoEncoder检测器中，作者构建了不同感受野的多级特征(C3-C7)并在匹配尺度上进行目标检测。然而，单级特征破坏了上述游戏规则，在SiSoEncoder中仅有一个输出特征。\n\n以下图(a)为例，C5特征感受野仅仅覆盖有限的尺度范围，当目标尺度与感受野尺度不匹配时就导致了检测性能的下降。为使得SiSoEncoder可以检测所有目标，作者需要寻找一种方案生成具有可变感受野的输出特征，以补偿多级特征的缺失。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648177983742.png)\n\n在C5特征的基础上，作者采用堆叠扩张卷积方式提升其感受野。尽管其覆盖的尺度范围可以在一定程度上扩大，但它仍无法覆盖所有的目标尺度。以上图(b)为例，相比图(a)，它的感受野尺度朝着更大尺度进行了整体的偏移。然后，作者对原始尺度范围与扩大后尺度范围通过相加方式进行组合，因此得到了覆盖范围更广的输出特征，见上图(c)。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648178037832.png)\n\n上图给出了采用本文所提SiSoEncoder结构示意图，作者称之为Dilated Encoder。它包含两个主要成分：Prejector与Residual Block。投影层采用1×1卷积，然后采用3×3卷积提取上下文语义信息(作用类似FPN)；然后堆叠四个不同扩张因子的残差模块以生成多感受野的输出特征(覆盖所有的目标尺度)。\n\n### Imbalance Problem on Positive Anchors\n\n正锚点的定义对于目标检测中的优化问题尤其重要。在基于锚点的检测方案中，正锚点的定义策略主要受锚点与真实box之间的IoU决定。在RetinaNet中，如果IoU大于0.5则锚点设为正。作者称之为*Max-IoU matching*。\n\n在MiMoEncoder中，锚点在多级特征上以稠密方式进行预定义，同时按照尺度生成特征级的正锚点。在分而治之的机制下，Max-IoU匹配使得每个尺度下的真实Box可以生成充分数量的正锚点。然而，**当作者采用SiSoEncoder时，锚点的数量会大量的减少(比如从100K减少到5K)**，导致了稀疏锚点。稀疏锚点进一步导致了采用Max-IoU匹配时的不匹配问题。以下图为例，大的目标框包含更多的正锚点，这就导致了正锚点的不平衡问题，进而导致了检测器更多关注于大目标而忽视了小目标。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648178166896.png)\n\n为解决上述正锚点不平衡问题，作者提出了Uniform Matching策略：**对于每个目标框采用k近邻锚点作为正锚点，这就确保了所有的目标框能够以相同数量的正锚点进行均匀匹配**。正锚点的平衡确保了所有的目标框都参与了训练且贡献相等。在实现方面，参考了Max-IoU匹配，作者对`Uniform matching`中的IoU阈值进行设置以忽略大IoU负锚点和小IoU正锚点。\n\n### YOLOF\n\n基于上述解决方案呢，作者提出了一种快速而直接的单级特征检测框架YOLOF，它由骨干网络、Encoder以及Decoder构成，整体结构如下图所示。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325111843.png)\n\n- BackBone。在所有模型中，作者简单的采用了ResNet与ResNeXt作为骨干网络，所有模型在ImageNet上与训练，输出C5特征该通道数为2048，下采样倍率为32；\n- Encoder。在这部分，作者参考FPN添加了两个投影层，将通道数降到512，然后堆叠四个不同扩张因子的残差模块；\n- Decoder。在这部分，作者采用了RetinaNet的主要设计思路，它包含两个并行的任务相关的Head分别用于分类和回归。作者仅仅添加两个微小改动：(1) 参考DETR中的FFN设计让两个Head的卷积数量不同，回归Head包含4个卷积而分类Head则仅包含两个卷积；(2) 作者参考AutoAssign在回归Head上对每个锚点添加了一个隐式目标预测。\n- Other Detail。正如前面所提到的YOLOF中的预定义锚点是稀疏的，这会导致目标框与锚点之间的匹配质量下降。作者在图像上添加了一个随机移动操作以缓解该问题，同时作者发现这种移动对于最终的分类是有帮助的。\n\n##  Experiments\n\n为说明所提方案的有效性，作者在MS COC数据集上与RetinaNet、DETR、YOLOv4进行了对比。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325113201.png)\n\n上表给出了所提方法与RetineNet在COCO数据集上的性能对比。从中可以看到：\n\n- YOLOF取得了与改进版RetinaNet+相当的性能，同时减少了57%的计算量，推理速度快了2.5倍；\n- 当采用相同骨干网络时，由于仅仅采用C5特征，YOLOF在小目标检测方面要比RetinaNet+弱一些(低3.1)；但在大目标检测方面更优(高3.3)；\n- 当YOLOF采用ResNeXt作为骨干网络时，它可以取得与RetinaNet在小目标检测方面相当的性能且推理速度同样相当。\n- 经由多尺度测试辅助，所提方法取得了47.1mAP的指标，且在小目标方面取得了极具竞争力的性能31.8mAP。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325113018.png)\n\n上图给出了所提方法与DETR的性能对比。从中可以看到：\n\n- YOLOF取得了与DETR相匹配的的性能；\n- 相比DETR，YOLOF可以从更深的网络中收益更多，比如ResNet50时低0.4，在ResNet10时多了0.2；\n- 在小目标检测方面，YOLOF要优于DETR；在大目标检测方面，YOLOF要弱于DETR。\n- 在收敛方面，YOLOF要比DETR快7倍，这使得YOLOF更适合于作为单级特征检测器的基线。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325113409.png)\n\n最后，作者再来看一下所提方法与YOLOv4的性能对比(注：这里采用了与YOLOv4类似的数据增强方法，并采用了三阶段训练方案，同时对骨干网络的最后阶段进行了调整)。从上表作者可以看到：\n\n- YOLOF-DC5取得了比YOLOv4快13%的推理速度，且性能高0.8mAP；\n- YOLOF-DC5在小目标检测方面弱于YOLOv4，而在大目标检测方面显著优于YOLOv4；\n- 这也就意味着：单级检测器具有极大的潜力获得SOTA速度-精度均衡性能。\n\n\n\n\n\n参考文献: [You Only Look One-level Feature](<https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_You_Only_Look_One-Level_Feature_CVPR_2021_paper.pdf>) (点击链接下载PDF)\n\n文献源码:https://github.com/megvii-model/YOLOF","tags":["yolov5","FPN"],"categories":["文献阅读"]},{"title":"FPN网络结构及源码分析","url":"/2022/03/24/Yolov5支线学习之FPN网络结构+源码分析/","content":"\n# FPN网络结构及源码分析\n\n> FPN即Feature Pyramid Networks，特征金字塔。\n\n特征金字塔是<u>*多尺度(muiti-scale)目标检测*</u>领域中的重要组成部分，但是由于此方法对计算和内存的需求，在FPN之前的深度学习任务都刻意回避了这类模型。在文献阅读2中，作者利用深度神经网络固有的多尺度、多层级的金字塔结构，使用一种 **自上而下的侧边连接** 在所有尺度上构建出高级语义特征图，构造了特征金字塔的经典结构。\n\n具体做法是:**把低分辨率、高语义信息的高层特征和高分辨率、低语义信息的低层特征自上而下进行融合，使得所有尺度下的特征都有丰富的语义信息。**\n\n其结构如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325142705.png)\n\n在FPN网络结构中，还有很多图像算法。\n\n## Featurized image pyramid\n\n> 生成不同尺寸的图片，每张图片生成不同的特征，分别进行预测，最后统计所有尺寸的预测结果。\n\n一种比较笨的多尺度方法，对输入图像设置不同的缩放比例实现多尺度。这样可以解决多尺度，但是相当于训练了多个模型（假设要求输入大小固定），即便允许输入大小不固定，但是也增加了存储不同尺度图像的内存空间。\n\n## Single feature map\n\n相当于早期的CNN模型，通过卷积层不断学习图像的高级语义特征。\n\n> 使用神经网络某一层输出的feature map进行预测，一般是网络最后一层feature map（例如Fast R-CNN、Faster R-CNN等）；然而靠近网络输入层的feature map包含粗略的位置信息，导致预测的目标狂bbox不准确，靠近最后网络最后一层的feature map会忽略小物体信息。\n\n## Pyramidal feature hierarchy\n\n> 使用不同层次的金字塔层feature map进行预测。SSD就是采用这种多尺度特征融合方法，从网络不同层抽取不同尺寸的特征做预测，没有增加额外的计算量。但是SSD没有使用足够底层的特征，SSD使用最底层的特征是VGG的conv4_3。\n\nSSD较早尝试了使用CNN金字塔形的层级特征，重用了前向过程计算出的多尺度特征图，因此这种形式是不消耗额外的资源的。但是SSD为了避免使用low-level的特征，放弃了浅层的特征图信息，直接从conv4_3开始建立金字塔，并且加入了一些新的层，但是这些低层级、高分辨率的特征图信息对检测小目标是非常重要的。\n\n## Feature Pyramid Network\n\n> 对最底层的特征进行向上采样，并与该底层特征进行融合，得到高分辨率、强语义的特征（即加强了特征的提取）。\n\nFPN为了能够自然地利用CNN层级特征的金字塔形式，同时生成在所有尺度上都具有强语义信息的特征金字塔，便以此为目的设计了top-down结构和lateral connection。这种金字塔结构以此融合具有高分辨率的浅层feature和具有丰富语义信息的深层feature。这样就实现了从单尺度的单张输入图像，快速构建在所有尺度上都具有强语义信息的特征金字塔，同时不产生明显的代价。\n\n上述四种的FPN的网络结构如下图所示：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325142959.png)\n\n## FPN特征金字塔\n\nFPN官方的backbone是ResNet。CNN的前馈计算就是自下而上的路径，特征图经过卷积核计算，通常是越变越小的，也有一些特征层的输出大小和输入大小一样。\n\n特征金字塔网络包括自底向上、自顶向下和横向连接。\n\nresnet特征提取（feature map）：自底向上\n\n最后一层feature map上采样：自顶向下\n\n特征融合：横向连接\n\n横向连接的两层特征在空间尺寸上要相同，主要是为了利用底层的定位细节信息（由于底部的feature map包含更多的定位细节，而顶部的feature map包含更多的目标特征信息）。","tags":["python","FPN"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记8——v6.0源码剖析——Head部分","url":"/2022/03/23/Yolov5学习笔记8/","content":"\n# Yolov5学习笔记8——v6.0源码剖析——Head部分\n\n## Yolov5s网络结构总览\n\n> 要了解head，就不能将其与前两部分割裂开。head中的主体部分就是三个Detect检测器，即利用基于网格的anchor在不同尺度的特征图上进行目标检测的过程。由下面的网络结构图可以很清楚的看出：当输入为640*640时，三个尺度上的特征图分别为：80x80、40x40、20x20。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325191212.png)\n\n## Detect解析\n\n### Detect源码\n\n```python\nclass Detect(nn.Module):\n    stride = None  # strides computed during build\n    onnx_dynamic = False  # ONNX export parameter\n\n    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) // 2  # number of anchors\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\n        self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n        self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n\n    def forward(self, x):\n        z = []  # inference output\n        for i in range(self.nl):\n            x[i] = self.m[i](x[i])  # conv\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n            if not self.training:  # inference\n                if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n                y = x[i].sigmoid()\n                if self.inplace:\n                    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                    y = torch.cat((xy, wh, y[..., 4:]), -1)\n                z.append(y.view(bs, -1, self.no))\n\n        return x if self.training else (torch.cat(z, 1), x)\n\n    def _make_grid(self, nx=20, ny=20, i=0):\n        d = self.anchors[i].device\n        if check_version(torch.__version__, '1.10.0'):  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility\n            yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)], indexing='ij')\n        else:\n            yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)])\n        grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n        anchor_grid = (self.anchors[i].clone() * self.stride[i]) \\\n            .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n        return grid, anchor_grid\n```\n\n### initial部分\n\n```python\ndef __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n    super().__init__()\n    self.nc = nc  # number of classes\n    self.no = nc + 5  # number of outputs per anchor\n    self.nl = len(anchors)  # number of detection layers\n    self.na = len(anchors[0]) // 2  # number of anchors\n    self.grid = [torch.zeros(1)] * self.nl  # init grid\n    self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n    self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n    self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n    self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n```\n\ninitial部分定义了Detect过程中的重要参数\n1. **nc:**类别数目\n2. **no:**每个anchor的输出，包含类别数nc+置信度1+xywh4，故nc+5\n3. **nl:**检测器的个数。以上图为例，我们有3个不同尺度上的检测器：[[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]，故检测器个数为3。\n4. **na:**每个检测器中anchor的数量，个数为3。由于anchor是w h连续排列的，所以需要被2整除。\n5. **grid:**检测器Detect的初始网格\n6. **anchor_grid:**anchor的初始网格\n7. **m：**每个检测器的最终输出，即检测器中anchor的输出no×anchor的个数nl。打印出来很好理解（60是因为我的数据集nc为15，coco是80）：\n\n### forward\n\n```python\ndef forward(self, x):\n    z = []  # inference output\n    for i in range(self.nl):\n        x[i] = self.m[i](x[i])  # conv\n        bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n        x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n        if not self.training:  # inference\n            if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n            y = x[i].sigmoid()\n            if self.inplace:\n                y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n            else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                y = torch.cat((xy, wh, y[..., 4:]), -1)\n            z.append(y.view(bs, -1, self.no))\n\n    return x if self.training else (torch.cat(z, 1), x)\n```\n\n在forward操作中，网络接收3个不同尺度的特征图，分别为：128×80×80、256×40×40、512×20×20\n\n```python\n    for i in range(self.nl):\n        x[i] = self.m[i](x[i])  # conv\n        bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n        x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).co\n```\n\n网络的循环次数为3，也就是依次在这3个特征图上进行网格化预测，利用卷积操作得到通道数为no×nl的特征输出。拿128x80x80举例，在nc=15的情况下经过卷积得到60x80x80的特征图，这个特征图就是后续用于格点检测的特征图。\n\n```python\n        if not self.training:  # inference\n            if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n```\n\n```python\ndef _make_grid(self, nx=20, ny=20, i=0):\n    d = self.anchors[i].device\n    if check_version(torch.__version__, '1.10.0'):  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)], indexing='ij')\n    else:\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)])\n    grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n    anchor_grid = (self.anchors[i].clone() * self.stride[i]) \\\n        .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n    return grid, anchor_grid\n```\n\n随后就是基于经过检测器卷积后的特征图划分网格，网格的尺寸是与输入尺寸相同的，如20x20的特征图会变成20x20的网格，那么一个网格对应到原图中就是32x32像素；40x40的一个网格就会对应到原图的16x16像素，以此类推。\n\n```python\ny = x[i].sigmoid()\nif self.inplace:\ny[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\ny[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\nelse:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\nxy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\nwh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\ny = torch.cat((xy, wh, y[..., 4:]), -1)\nz.append(y.view(bs, -1, self.no))\n```\n\n这部分代码是预测偏移的主体部分。\n\n`y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy`\n\n这一句是对x和y进行预测。x、y在输入网络前都是已经归一好的(0,1)，乘以2再减去0.5就是(-0.5,1.5)，也就是让x、y的预测能够跨网格进行。后边的`self.grid[i]) * self.stride[i]`就是将相对位置转为网格中的绝对位置。\n\n`y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh`\n\n这句是对宽和高进行预测的。\n\n`z.append(y.view(bs, -1, self.no))`\n\n最后再将结果填入z。","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记7——v6.0源码剖析——Neck部分","url":"/2022/03/22/Yolov5学习笔记7/","content":"\n\n\n\n# Yolov5学习笔记7——v6.0源码剖析——Neck部分\n\n> 在网络结构配置文件yolov5s.yaml中，并未将neck和head区分开来，而是直接以head命名，这也是方便在model/yolo.py中的加载。Yolov5学习笔记7只讨论head中的neck部分。\n\n## neck结构概览及参数\n\n### neck部分结构图\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325095329.png)\n\n### neck部分参数配置源码\n\n```python\n# YOLOv5 v6.0 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n  ]\n```\n\n可以看到，Neck部分的组件相较于Backbone较为单一，基本上就由CBS、Upsample、Concat和不带shortcut的CSP（C3)。\n\n### FPN和PAN\n\nNeck的网络结构设计也是沿用了**FPN+PAN的结构**。FPN就是使用一种 自顶向下的侧边连接在所有尺度上构建出高级语义特征图，构造了特征金字塔的经典结构；PAN的结构也不稀奇，FPN中间经过多层的网络后，底层的目标信息已经非常模糊了，因此PAN又加入了自底向上的路线，弥补并加强了定位信息。\n\n## Neck部分各模块\n\n### CBS模块\n\n在Backbone中，为了进一步提取图像中的信息，CBS在改变特征图通道的同时，也会控制卷积模块中的步长s下采样来改变特征图的尺寸。Neck中左侧采用FPN自顶向下设计的过程中，是特征图上采样的过程，因此这个时候再下采样就不合时宜了，所以在FPN中s=1；而到了右侧PAN再次自下而上提取位置信息时，就需要使用CBS继续下采样抽取高层次的语义信息，这也是CBS前后参数差异的原因。\n\n### nn.Upsample\n\n```python\n[-1, 1, nn.Upsample, [None, 2, 'nearest']],\n```\n\n使用的是Pytroch内置的上采样模块，需要指定上采样的倍数和方式。\n\n这里我们不指定size，上采样倍数为2，上采样方式为nearest，也就是最近填充。\n\n### Concat\n\n```python\n[[-1, 6], 1, Concat, [1]],  # cat backbone P4\n```\n\nConcat即拼接，接对象通过from传入，拼接的维度由args参数指定，此处即按照维度1(channel)拼接，其他维度不变。\n\n### CSP/C3\n\nBackbone需要更深层次的网络获取更多的信息，可以说backbone已经完成了主要特征信息的提取，所以在Neck阶段我们并不需要再一味地加深网络，采取不带残差的C3模块可能会更合适一些。","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记6——v6.0源码剖析——Backbone部分3","url":"/2022/03/21/Yolov5学习笔记6/","content":"\n\n# Yolov5学习笔记6——v6.0源码剖析——Backbone部分3\n\n## Backbone概览及参数\n\n### 源码如下\n\n```Python\n# Parameters\nnc: 80  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\nanchors:\n  - [10,13, 16,30, 33,23]  # P3/8\n  - [30,61, 62,45, 59,119]  # P4/16\n  - [116,90, 156,198, 373,326]  # P5/32\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n```\n\n> yolov5s的backbone部分如上，其网络结构使用yaml文件配置，通过./models/yolo.py解析文件加了一个输入构成的网络模块。与v3和v4所使用的config设置的网络不同，yaml文件中的网络组件不需要进行叠加，只需要在配置文件中设置number即可。\n\n#### Parameters\n\n```python\n# Parameters\nnc: 80  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\n```\n\n- **nc: 80**\n  代表数据集中的类别数目，例如MNIST中含有0-9共10个类.\n\n- **depth_multiple: 0.33**\n  用来控制模型的深度，仅在number≠1时启用。 如第一个C3层（c3具体是什么后续介绍）的参数设置为[-1, 3, C3, [128]]，其中number=3，表示在v5s中含有1个C3（3*0.33）；同理，v5l中的C3个数就是3（v5l的depth_multiple参数为1）。\n\n- **width_multiple: 0.50**\n  用来控制模型的宽度，主要作用于args中的ch_out。如第一个Conv层，ch_out=64，那么在v5s实际运算过程中，会将卷积过程中的卷积核设为64x0.5，所以会输出32通道的特征图。\n\n#### backbone\n\n```python\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n```\n\n- **from**：-n表示从前n层获得的输入，如-1表示从前一层获得输入\n- **number**：表示网络模块的数目，如[-1, 2, C3, [128] ]表示含有3个C3模块\n- **model**：表示网络模块的名称，具体细节可以在/models/common.py中查看，如Conv、C3、SPPF都是已经在common中定义好的模块。\n- **args**：表示向不同模块内传递的参数，即[ch_out, kernel, stride, padding, groups]，这里连ch_in都省去了，因为输入都是上层的输出（初始ch_in为3）。为了修改过于麻烦，这里输入的获取是从./models/yolo.py的`def parse_model(md, ch)`函数中解析得到的。\n\n#### 示例\n\n```python\n[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n```\n\n- input为：3×640×640\n\n- [ch_out，kernel，stride，padding]=[64, 6, 2, 2]\n\n  > 故新的通道数为64×0.5=32\n  >\n  > 根据特征图计算公式：Feature_new=(Feature_old-kernel+2xpadding)/stride+1可得：\n  >\n  > 新的特征图尺寸为：Feature_new=(640-6+2x2)/2+1=320\n\n```python\n[-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n```\n\n- input为：32×320×320\n\n- [ch_out, kernel, stride]=[128, 3, 2]\n\n  > 同理可得：新的通道数为64，新的特征图尺寸为160\n\n## Backbone组成\n\nv6.0版本的Backbone去除了Focus模块（便于模型导出部署），Backbone主要由CBL、BottleneckCSP/C3以及SPP/SPPF等组成，具体如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324165913.png)\n\n### CBS模块\n\n> CBS模块实际上就是Conv+BatchNorm+SiLU。\n\n- CBS模块框架图\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324170357.png)\n\n- CBS源码\n\n  ```python\n  class Conv(nn.Module):\n      # Standard convolution\n      def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n          super().__init__()\n          self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n          self.bn = nn.BatchNorm2d(c2)\n          self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n  \n      def forward(self, x):\n          return self.act(self.bn(self.conv(x)))\n  \n      def forward_fuse(self, x):\n          return self.act(self.conv(x))\n  ```\n\n这里配合CBS模块的源码，分析Conv()函数里的一些参数，作为pytorch中卷积操作的复习。\n\n从源码可以看出，Conv()包含7个参数，这些参数也是二维卷积Conv2d()中的重要参数。ch_in, ch_out, kernel, stride这4个参数前文已经提到过，是用来计算特征图尺寸的。主要分析后三个参数。\n\n#### padding\n\n> 从目前主流卷积操作来看，大多数的研究者不会通过kernel来改变特征图的尺寸，如googlenet中3x3的kernel设定了padding=1，所以当kernel≠1时需要对输入特征图进行填充。当指定p值时按照p值进行填充，当p值为默认时则通过autopad函数进行填充：\n\n```python\ndef autopad(k, p=None):  # kernel, padding\n    # Pad to 'same'\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n    return p\n```\n\n这里作者考虑到对不同的卷积操作使用不同大小的卷积核时padding也需要做出改变，所以这里在为p赋值时会首先检查k是否为int，如果k为列表则对列表中的每个元素整除。\n\n#### groups\n\n表示分组卷积，示意图如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324171353.png)\n\n> groups – 从输入通道到输出的阻塞连接数\n>\n> - groups=1 时，所有输入都卷积到所有输出。\n> - groups=2 时，该操作等效于并排具有两个凸层，每个凸层看到一半的输入通道，并产生一半的输出通道，随后两者都串联起来。\n> - groups= in_channels 时，每个输入通道都用自己的一组滤波器进行卷积，其大小为：⌊（out_channels）/（in_channels）⌋。\n\n#### act参数\n\n决定是否对特征图进行激活操作，SILU表示使用Sigmoid进行激活。\n\n*关于激活函数的内容在Yolov5学习笔记3中有提及*\n\n#### 补充：dilation参数\n\n在Conv2d()中有一个重要的参数——空洞卷积dilation\n\n```python\ndilation: _size_2_t = 1,\n```\n\n通俗解释就是控制kernel点（卷积核点）间距的参数，通过改变卷积核间距实现特征图及特征信息的保留，在语义分割任务中空洞卷积比较有效。\n\n### CSP/C3\n\n**注**：CSP即backbone中的C3，因为在backbone中C3存在shortcut，而在neck中C3不使用shortcut，所以backbone中的C3层使用CSP1_x表示，neck中的C3使用CSP2_x表示。\n\n#### CSP结构\n\n- 源码：\n\n  ```python\n  class C3(nn.Module):\n      # CSP Bottleneck with 3 convolutions\n      def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n          super().__init__()\n          c_ = int(c2 * e)  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = Conv(c1, c_, 1, 1)\n          self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n          self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n          # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])\n  \n      def forward(self, x):\n          return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n  ```\n\n> 从源码中可以看出：输入特征图一条分支先经过.cv1，再经过.m，得到子特征图1；另一分支经过.cv2后得到子特征图2。最后将子特征图1和子特征图2拼接后输入.cv3得到C3层的输出，如下图所示。\n>\n> **这里的CV就是前面的Conv2d+BN+SiLU**\n\n- 结构图\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324190458.png)\n\n> .m操作是用nn.Sequential将多个Bottleneck(也就是上图中的Res_u)串接到网络中，\n\n#### Bottleneck\n\n在Resnet出现之前，人们的普遍为网络越深获取信息也越多，模型泛化效果越好。然而随后大量的研究表明，网络深度到达一定的程度后，模型的准确率反而大大降低。这并不是过拟合造成的，而是由于反向传播过程中的梯度爆炸和梯度消失。也就是说，**网络越深，模型越难优化，而不是学习不到更多的特征。**\n\n为了能让深层次的网络模型达到更好的训练效果，残差网络中提出的残差映射替换了以往的基础映射。对于输入x，期望输出H(x)，网络利用恒等映射将x作为初始结果，将原来的映射关系变成F(x)+x。与其让多层卷积去近似估计H(x) ，不如近似估计H(x)-x，即近似估计残差F(x)。因此，ResNet相当于将学习目标改变为目标值H(x)和x的差值，后面的训练目标就是要将残差结果逼近于0。\n\n残差函数有什么好处呢？\n\n> - **梯度弥散方面。**加入ResNet中的shortcut结构之后，在反传时，每两个block之间不仅传递了梯度，还加上了求导之前的梯度，这相当于把每一个block中向前传递的梯度人为加大了，也就会减小梯度弥散的可能性。\n> - **特征冗余方面。**正向卷积时，对每一层做卷积其实只提取了图像的一部分信息，这样一来，越到深层，原始图像信息的丢失越严重，而仅仅是对原始图像中的一小部分特征做提取。这显然会发生类似欠拟合的现象。加入shortcut结构，相当于在每个block中又加入了上一层图像的全部信息，一定程度上保留了更多的原始信息。\n\n**在resnet中，人们可以使用带有shortcut的残差模块搭建几百层甚至上千层的网络，而浅层的残差模块被命名为Basicblock（18、34），深层网络所使用的的残差模块，就被命名为了Bottleneck（50+）。**\n\nBottleneck与Basicblock最大的区别是卷积核的组成。 Basicblock由两个3x3的卷积层组成，Bottleneck由两个1x1卷积层夹一个3x3卷积层组成：其中1x1卷积层降维后再恢复维数，让3x3卷积在计算过程中的参数量更少、速度更快。\n第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。\n**Bottleneck减少了参数量，优化了计算，保持了原有的精度。**\n\n- Bottleneck的源码如下：\n\n```python\nclass Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n```\n\n- 结构图如下:\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324191951.png)\n\n可以看到，CSP中的Bottleneck同resnet模块中的类似，先是1x1的卷积层（CBS)，然后再是3x3的卷积层，最后通过shortcut与初始输入相加。但是这里与resnet的不通点在于：CSP将输入维度减半运算后并未再使用1x1卷积核进行升维，而是将原始输入x也降了维，采取concat的方法进行张量的拼接，得到与原始输入相同维度的输出。其实这里能区分一点就够了：**resnet中的shortcut通过add实现，是特征图对应位置相加而通道数不变；而CSP中的shortcut通过concat实现，是通道数的增加。二者虽然都是信息融合的主要方式，但是对张量的具体操作又不相同.**\n\n### SSPF模块\n\n- 源码\n\n```python\nclass SPPF(nn.Module):\n    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher\n    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n\n    def forward(self, x):\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning\n            y1 = self.m(x)\n            y2 = self.m(y1)\n            return self.cv2(torch.cat([x, y1, y2, self.m(y2)], 1))\n```\n\n- 结构图\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324194944.png)\n\nSSPF模块将经过<u>CBS的x</u>、<u>一次池化后的y1</u>、<u>两次池化后的y2和3次池化后的self.m(y2)</u>先进行拼接，然后再<u>CBS提取特征</u>。 仔细观察不难发现，虽然SSPF对特征图进行了多次池化，但是**特征图尺寸并未发生变化**，通道数更不会变化，所以后续的4个输出能够在channel维度进行融合。这一模块的主要作用是**对高层特征进行提取并融合，在融合的过程中作者多次运用最大池化，尽可能多的去提取高层次的语义特征。**\n\n## Yolov5s的Backbone总览\n\n运行yolo.py，结合上述的分析，对输出的结果应该很容易理解了。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324195512.png)","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"基于DNN神经网络的手写数字识别","url":"/2022/03/21/Depp_Learning实战训练1/","content":"\n# 基于DNN神经网络的手写数字识别\n\n**导入相关的库**\n\n```python\nimport numpy as np\nimport paddle as paddle\nimport paddle.fluid as fluid\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\npaddle.enable_static()\n```\n\n**读取数据**\n\n```python\n# 使用多层感知器训练（DNN）模型，用于预测手写数字图片。\n\nBUF_SIZE=512\nBATCH_SIZE=128\n\n#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\ntrain_reader = paddle.batch(\n    paddle.reader.shuffle(paddle.dataset.mnist.train(),\n                          buf_size=BUF_SIZE),\n    batch_size=BATCH_SIZE)\n\n#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\ntest_reader = paddle.batch(\n    paddle.reader.shuffle(paddle.dataset.mnist.test(),\n                          buf_size=BUF_SIZE),\n    batch_size=BATCH_SIZE)\n\n#用于打印，查看mnist数据\ntrain_data=paddle.dataset.mnist.train();\nsampledata=next(train_data())\n# print(sampledata)\n```\n\n定义一个多层感知器**\n\n```python\ndef multilayer_perceptron(input):\n    # 第一个全连接层，激活函数为ReLU\n    hidden1 = fluid.layers.fc(input=input, size=100, act='relu')\n    # 第二个全连接层，激活函数为ReLU\n    hidden2 = fluid.layers.fc(input=hidden1, size=100, act='relu')\n    # 以softmax为激活函数的全连接输出层，输出层的大小必须为数字的个数10\n    prediction = fluid.layers.fc(input=hidden2, size=10, act='softmax')\n    return prediction\n```\n\n**定义图像和标签数据**\n\n```python\n# 输入的原始图像数据，大小为1*28*28\nimage = fluid.layers.data(name='image', shape=[1, 28, 28], dtype='float32')#单通道，28*28像素值\n# 标签，名称为label,对应输入图片的类别标签\nlabel = fluid.layers.data(name='label', shape=[1], dtype='int64')          #图片标签\n```\n\n**获取分类器**\n\n```python\npredict = multilayer_perceptron(image)\n```\n\n**损失函数**\n\n```python\n# 使用交叉熵损失函数,描述真实样本标签和预测概率之间的差值\ncost = fluid.layers.cross_entropy(input=predict, label=label)\n# 使用类交叉熵函数计算predict和label之间的损失函数\navg_cost = fluid.layers.mean(cost)\n# 计算分类准确率\nacc = fluid.layers.accuracy(input=predict, label=label)\n```\n\n**测试**\n\n```python\n# 获取测试程序\ntest_program = fluid.default_main_program().clone(for_test=True)\n#使用Adam算法进行优化, learning_rate 是学习率(它的大小与网络的训练收敛速度有关系)\noptimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.001)\nopts = optimizer.minimize(avg_cost)\n# （1）创建训练的Executor\n# 首先定义运算场所 fluid.CPUPlace()和 fluid.CUDAPlace(0)分别表示运算场所为CPU和GPU\n# Executor:接收传入的program，通过run()方法运行program。\n# 定义使用CPU还是GPU，使用CPU时use_cuda = False,使用GPU时use_cuda = True\nuse_cuda = False\nplace = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\nexe = fluid.Executor(place)\nexe.run(fluid.default_startup_program())\n#（2）告知网络传入的数据分为两部分，第一部分是image值，第二部分是label值\n# DataFeeder负责将数据提供器（train_reader,test_reader）返回的数据转成一种特殊的数据结构，使其可以输入到Executor中。\nfeeder = fluid.DataFeeder(place=place, feed_list=[image, label])\n\nall_train_iter=0\nall_train_iters=[]\nall_train_costs=[]\nall_train_accs=[]\n```\n\n可视化与模型训练\n\n```python\ndef draw_train_process(title,iters,costs,accs,label_cost,lable_acc):\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"iter\", fontsize=20)\n    plt.ylabel(\"cost/acc\", fontsize=20)\n    plt.plot(iters, costs,color='red',label=label_cost)\n    plt.plot(iters, accs,color='green',label=lable_acc)\n    plt.legend()\n    plt.grid()\n    plt.show()\n\nEPOCH_NUM = 2\nmodel_save_dir = \"CH4_File/model\"\nfor pass_id in range(EPOCH_NUM):\n    # 进行训练\n    for batch_id, data in enumerate(train_reader()):  # 遍历train_reader\n        train_cost, train_acc = exe.run(program=fluid.default_main_program(),  # 运行主程序\n                                        feed=feeder.feed(data),  # 给模型喂入数据\n                                        fetch_list=[avg_cost, acc])  # fetch 误差、准确率\n\n        all_train_iter = all_train_iter + BATCH_SIZE\n        all_train_iters.append(all_train_iter)\n\n        all_train_costs.append(train_cost[0])\n        all_train_accs.append(train_acc[0])\n\n        # 每200个batch打印一次信息  误差、准确率\n        if batch_id % 200 == 0:\n            print('Pass:%d, Batch:%d, Cost:%0.5f, Accuracy:%0.5f' %\n                  (pass_id, batch_id, train_cost[0], train_acc[0]))\n\n    # 进行测试\n    test_accs = []\n    test_costs = []\n    # 每训练一轮 进行一次测试\n    for batch_id, data in enumerate(test_reader()):  # 遍历test_reader\n        test_cost, test_acc = exe.run(program=test_program,  # 执行训练程序\n                                      feed=feeder.feed(data),  # 喂入数据\n                                      fetch_list=[avg_cost, acc])  # fetch 误差、准确率\n        test_accs.append(test_acc[0])  # 每个batch的准确率\n        test_costs.append(test_cost[0])  # 每个batch的误差\n\n    # 求测试结果的平均值\n    test_cost = (sum(test_costs) / len(test_costs))  # 每轮的平均误差\n    test_acc = (sum(test_accs) / len(test_accs))  # 每轮的平均准确率\n    print('Test:%d, Cost:%0.5f, Accuracy:%0.5f' % (pass_id, test_cost, test_acc))\n\n    # 保存模型\n    # 如果保存路径不存在就创建\nif not os.path.exists(model_save_dir):\n    os.makedirs(model_save_dir)\nprint('save models to %s' % (model_save_dir))\nfluid.io.save_inference_model(model_save_dir,   # 保存推理model的路径\n                              ['image'],        # 推理（inference）需要 feed 的数据\n                              [predict],        # 保存推理（inference）结果的 Variables\n                              exe)              # executor 保存 inference model\n\nprint('训练模型保存完成！')\ndraw_train_process(\"training\", all_train_iters, all_train_costs, all_train_accs, \"trainning cost\", \"trainning acc\")\n\ndef load_image(file):\n    im = Image.open(file).convert('L')                          # 将RGB转化为灰度图像，L代表灰度图像，像素值在0~255之间\n    im = im.resize((28, 28), Image.ANTIALIAS)                   # resize image with high-quality 图像大小为28*28\n    im = np.array(im).reshape(1, 1, 28, 28).astype(np.float32)  # 返回新形状的数组,把它变成一个 numpy 数组以匹配数据馈送格式。\n    # print(im)\n    im = im / 255.0 * 2.0 - 1.0                                 # 归一化到【-1~1】之间\n    return im\n\ninfer_path='CH4_File/data/infer_3.png'\nimg = Image.open(infer_path)\nplt.imshow(img)   #根据数组绘制图像\nplt.show()        #显示图像\n```\n\n**预测**\n\n```python\n# 加载数据并开始预测\nwith fluid.scope_guard(inference_scope):\n    #获取训练好的模型\n    #从指定目录中加载 推理model(inference model)\n    [inference_program,                                             # 推理Program\n     feed_target_names,                                             # 是一个str列表，它包含需要在推理 Program 中提供数据的变量的名称。\n     fetch_targets] = fluid.io.load_inference_model(model_save_dir, # fetch_targets：是一个 Variable 列表，从中我们可以得到推断结果。model_save_dir：模型保存的路径\n                                                    infer_exe)      # infer_exe: 运行 inference model的 executor\n    img = load_image(infer_path)\n\n    results = infer_exe.run(program=inference_program,              # 运行推测程序\n                   feed={feed_target_names[0]: img},                # 喂入要预测的img\n                   fetch_list=fetch_targets)                        # 得到推测结果,\n    # 获取概率最大的label\n    lab = np.argsort(results)                                       # argsort函数返回的是result数组值从小到大的索引值\n    #print(lab)\n    print(\"该图片的预测结果的label为: %d\" % lab[0][0][-1])             # -1代表读取数组中倒数第一列\n```\n\n**结果**\n\n训练过程可视化：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/训练曲线.png)\n\n识别如下图片：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/识别的图片.png)\n\n预测的结果为：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319162335.png)\n\n","tags":["deeplearning","DNN","手写数字识别"],"categories":["深度学习"]},{"title":"Yolov5学习笔记5——v5.0源码剖析——Backbone部分2","url":"/2022/03/20/Yolov5学习笔记5/","content":"\n# Yolov5学习笔记5——v5.0源码剖析——Backbone部分2\n\n用**netron**得到yolov5s的框架结构图如下，可以非常直观的得到关于backbone部分的网络结构图。\n\n**注**：所使用的代码版本为 2020年11月24日发布的Yolov5-master。\n\n## YOLOv5s结构图如下所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/yolov5s_2.png)\n\n## Backbone结构图\n\n- backbone的意义是：在不同图像细粒度上聚合并形成图像特征的卷积神经网络；\n\n- backbone所需的主要模块在common.py里面可以找到。\n\n从整体结构中我们抽出backbone部分学习：\n\nbackbone的结构图如下：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220323212340.png)\n\nNetron打开yolov5s.pt导出的BackBone部分的框架图如下：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220323212829.png)\n\nbackbone对应的代码为：\n\n```python\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, BottleneckCSP, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 9, BottleneckCSP, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, BottleneckCSP, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\n   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n  ]\n```\n\n从上述代码中我们可以看到backbone由如下组成：\n\nBACKBONE =FOCUS(1个)+CONV (1个)+BCSP(3个)+CONV (1个)+BCSP(9个)+CONV (1个)+SPP(1个)+BCSP(1个)\n\n- 注：这里的BCSP相当于CSP\n\n## Backbone源码解析\n\n### Focus模块\n\nFocus()函数在common.py中，对应的源码如下:\n\n```python\nclass Focus(nn.Module):\n    # Focus wh information into c-space\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super(Focus, self).__init__()\n        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\n\n    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n```\n\n根据前述代码我们知道，输入的图片尺寸为640×640×3，而Focus()函数的功能为：*将640 × 640 × 3的图像输入Focus结构，采用切片操作，先变成320 × 320 × 12的特征图，再经过3 × 3的卷积操作，输出通道32，最终变成320 × 320 × 32的特征图*\n\nFocus模块的结构图如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324110433.png)\n\n### CBL模块(CBH)\n\n该部分对应的源码如下：\n\n```python\nclass Conv(nn.Module):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super(Conv, self).__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.Hardswish() if act else nn.Identity()\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n    def fuseforward(self, x):\n        return self.act(self.conv(x))\n```\n\nCONV是一个标准的卷积模块。\n\n从源码第5 6 7行可以看出，激活函数变成了Hardwish()，实际上这里不应该叫CBL模块了，应该是CBH模块。\n\n- conv：来自于代码的torch.nn.Conv2d,是一个卷积操作\n- bn：来自于代码的torch.nn.BatchNorm2d：归一化处理，使batch里面的feature map 满足均值为1，方差为0 的正太分布\n- Hardswish：激活函数\n  故：CBL=CONV+BN+Hardswish\n\n### BottleneckCSP模块\n\n3个BCSP相当于是几个标准的Bottleneck的堆叠+几个标准卷积层。\n\n- BottleneckCSP的网络结构如下：\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324115258.png)\n\n- 残差组件Resunit的网络结构如下：\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324143720.png)\n\n- BottleneckCSP的源码如下：\n\n  ```python\n  class BottleneckCSP(nn.Module):\n      # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n      def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n          super(BottleneckCSP, self).__init__()\n          c_ = int(c2 * e)  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n          self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n          self.cv4 = Conv(2 * c_, c2, 1, 1)\n          self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n          self.act = nn.LeakyReLU(0.1, inplace=True)\n          self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n  \n      def forward(self, x):\n          y1 = self.cv3(self.m(self.cv1(x)))\n          y2 = self.cv2(x)\n          return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n  ```\n\n**注**：nn.sequential将所有的块链接在一起。self.bn = nn.BatchNorm2d(2 * c_)就是concat 块，cv2,cv3对应于图中的concat;\n\n- Resunit的源码如下：\n\n  ```python\n  class Bottleneck(nn.Module):\n      # Standard bottleneck\n      def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n          super(Bottleneck, self).__init__()\n          c_ = int(c2 * e)  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = Conv(c_, c2, 3, 1, g=g)\n          self.add = shortcut and c1 == c2\n  \n      def forward(self, x):\n          return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n  ```\n\n**注**：cv1、cv2对应于图中的CBL模块，add不变。\n\n### SPP某块\n\n> - SPP模块，就是常说的*空间金字塔池化模块*，分别采用5/9/13的最大池化，在进行concat融合，提高感受野。\n>\n> - SPP的输入时512×512×20，经过1×1的卷积层后输出256×20×20，然后经过并列的三个Maxpool进行下采样，将结果与其初始特征相加，输出1024×20×20，最后用512的卷积核将其恢复到512×20×20.\n\n- SPP模块的结构图如下：\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324143720.png)\n\n- SPP模块对应的代码如下：\n\n  ```python\n  class SPP(nn.Module):\n      # Spatial pyramid pooling layer used in YOLOv3-SPP\n      def __init__(self, c1, c2, k=(5, 9, 13)):\n          super(SPP, self).__init__()\n          c_ = c1 // 2  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n          self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n  \n      def forward(self, x):\n          x = self.cv1(x)\n          return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n  ```\n\n  \n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Paper Reading 1|YOLO-Z","url":"/2022/03/20/文献阅读1/","content":"\n# 文献阅读1-YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles\n\n本文的中文题目为：YOLO-Z:基于改进YOLOv5的自动驾驶车辆小目标检测算法。\n\n## 概述\n\n本研究探索了如何对YOLOv5进行修改，以提高其在检测较小目标时的性能，并在自动赛车中进行了特殊应用。为了实现这一点，作者研究了替换模型的某些结构会如何影响性能和推理时间。在这一过程中在不同的尺度上提出一系列的模型YOLO-Z，并得到高达6.9%的改善，相比原YOLOv5推理时间检测更小的目标时的成本就增加3ms。\n\n## 数据集(Dataset)\n\n本文所采用的数据集为基于自动驾驶赛车视角的带注释的锥体数据集。该数据集包括数字增强图像和具有挑战性天气条件的情况。数据集共有4类圆锥体(黄色、蓝色、橙色和大橙色)，接近4000张图像。\n\n数据集以65:15:20的比例分为训练、验证和测试。\n\n## 架构改进\n\nYOLOv5使用yaml文件来指导解析器如何构建模型。为了实现新的YOLOv5模型结构，作者为原有的YOLOv5的每个构建模块或层提供参数，并在必要时指导解析器如何构建它。换句话说，作者利用YOLOv5提供的基础和实验网络块，同时在需要模拟所需结构的地方实现额外的块。\n\n### Backbone部分\n\n模型的Backbone是用于获取输入图像并从中提取特征映射的组件。这是任何目标检测器的关键步骤，因为它是负责从输入图像提取上下文信息以及将该信息提取为模式的主要结构。\n\n作者尝试用2个Backbone替换YOLOv5中现有的Backbone。\n\n- ResNet是一种流行的结构，它引入残差连接来减少在深层神经网络中收益递减的影响。\n- DenseNet使用类似的连接，在网络中尽可能多地保存信息。实现这些结构需要将它们分解为基本块，并确保各层适当的通信。这包括确保正确的特征图尺寸，这有时需要为模型的宽度和深度略微修改缩放因子。\n\n**改进部分如下**\n\n- 使用ResNet50\n- 按比例缩小DenseNet\n- YOLOv5利用了Backbone和Neck之间的空间金字塔池化(SPP)层，改进的代码中没有用到。\n\n**实验结果**：对于小目标 DenseNet性能更好，增加的推理时间也相对较少，ResNet性能较差。\n\n### Neck部分\n\nNeck部分的作用是将Backbone提取的信息反馈到Head之前尽可能多地聚合这些信息。该结构通过防止小目标信息丢失，在传递小目标信息方面发挥了重要作用。它通过再次提高特征图的分辨率来做到这一点，这样来自Backbone的不同层的特征就可以被聚合，以提升整体的检测性能。\n\n**改进部分**：\n\n当前的PAN-Net替换为bi-FPN。\n\n虽然都保留了类似的特征，但复杂性不同，因此实现所需的层数和连接数也不同。\n\n### 其他修改\n\n作者提到为了提高小目标检测性能，YOLOv5的改进除了输入图像的大小之外，还可以**修改模型的深度和宽度**(废话)，以改变处理的主要方向；**Neck和Head的层连接方式**也可以手动改变，以便专注于检测特定的特征图。\n\n作者探索了涉及高分辨率特征映射的重定向连接的效果，以便将它们直接反馈到Neck和Head，做了如下修改：\n\n- **扩大Neck以适应额外的特征图**来实现\n- 通过**替换最低分辨率的特征图以适应新的特征图**来实现\n\n用这两种方式在Neck中整合。\n\n## 实验结果\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220321112728.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220321113416.png)\n\n\n\n### Backbone改进的影响\n\nDenseNet在推理时间(约3ms)相对较低的固定增加时，始终显示出显著的改进。作者的结论是：一般来说，DenseNet是一个更适合于小尺度目标检测。在较小比例的模型中，这可能是因为没有足够深的网络来获得ResNet骨干网的好处，而DenseNet在保存特征地图的细节方面做得很好。\n\n### Neck结构改进的影响\n\n使用FPN只在S尺度上优于双FPN。\n\n### Feature maps\n\n作者的实验表明重定向向颈部和头部提供的特征映射具有最显著的影响。在头部包含了更高分辨率的地图后，小对象最终占据了更多的像素，因此具有更大的影响力，而不是在脊柱的卷积阶段“丢失”。\n\n但对于超大规模的(即yolov5x)，在这种情况下，改进并不显著，保持较低分辨率的特征图实际上似乎对性能有害。\n\n### Influence of the number of anchors(锚点数)\n\n作者的实验表明：让YOLO根据所提供的数据集生成锚被证明在性能上是有效的，而且不会影响推理时间。在S模型下，3个锚点的表现优于5个锚点的表现，而在M模型下，差距减小。另一方面，L模型和X模型在5个锚点显示出更好的性能。\n\n**结论**：更复杂或更深入的模型可能确实受益于额外的锚点，或者换句话说，可能更有能力利用额外锚点提供的细节。\n\n### 其他因素\n\n- 更大的学习率被证明可以更好地利用模型\n- 较宽的模型(较高的宽度乘法器)对较小的尺度显示出积极的影响，而对较深的尺度(图6中既深又宽)则相反。\n- 类型的改变对推理速度有明显的负面影响，阻碍了它们的使用。\n\n### 改进模型\n\n作者通过使用上述修改方法的各种组合进行了其他测试，以寻找进一步偏离原始但同时又能进一步提高性能的模型——即YOLO-z，结论如下：\n\n- Neck的**FPN**结构往往优于双**FPN**\n- X量表，它似乎从这些变化中获益较少，即使使用不同的颈部结构，也不会像其他量表那样带来显著的改善。\n- 对于所有对象，在50% IoU的绝对mAP上，YOLO-Z模型的性能平均提高了2.7，而对于所有尺度上相同IoU的小对象，性能的绝对提高了5.9。这是以平均增加2.6ms的推理时间为代价的。\n\n## 讨论与结论\n\n在对YOLOv5进行调整以更好地检测更小的目标的方法的实验中，本文能够识别体系结构修改，这种修改在性能上比原始的检测器有明显改善，而且成本相对较低，因为新的模型保持了实时推理速度。\n\n所应用的技术，即自动赛车技术，可以从这样的改进中获益良多。在这项工作中，不仅显著提高了Baseline模型的性能，而且还确定了一些特定的技术，这些技术可以应用于任何其他应用程序，包括检测小或远的物体。\n\n最终结果是YOLO-Z系列的模型优于的YOLOv5类，同时保留一个推理时间等实时应用程序兼容的自动化赛车(见表2和图7)。特别是较小的目标是本研究的重点(图7中，中间)，而对于中等大小的目标(下图)，性能是稳定的。\n\n# 参考资料\n\n1. [YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles](<https://arxiv.org/pdf/2112.11798.pdf>)\n2. [深度学习论文及PyTorch实现](<https://blog.csdn.net/shanglianlm/article/details/122301921>)\n\n","tags":["yolov5","目标检测"],"categories":["文献阅读"]},{"title":"Yolov5学习笔记4——源码剖析——Head部分","url":"/2022/03/19/Yolov5学习笔记4/","content":"\n# Yolov5学习笔记4——源码剖析——Head部分\n\nDetect类对应yolov5的检查头(head)部分\n\nDetect类在yolo.py程序中的33行。\n\n## class Detect()代码分析\n\n```python\nclass Detect(nn.Module):\n    stride = None  # strides computed during build\n    onnx_dynamic = False  # ONNX export parameter\n\n    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) // 2  # number of anchors\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\n        self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n        self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n\n    def forward(self, x):\n        z = []  # inference output\n        for i in range(self.nl):\n            x[i] = self.m[i](x[i])  # conv\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n            if not self.training:  # inference\n                if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n                y = x[i].sigmoid()\n                if self.inplace:\n                    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                    y = torch.cat((xy, wh, y[..., 4:]), -1)\n                z.append(y.view(bs, -1, self.no))\n\n        return x if self.training else (torch.cat(z, 1), x)\n```\n\n### 初始化函数init()\n\n首先分析这个类的初始化函数：\n\n```python\ndef __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # 检测头\n    super().__init__()\n    self.nc = nc  # 类别数\n    self.no = nc + 5  # 输出的锚点数量\n    self.nl = len(anchors)  # 检测的层数\n    self.na = len(anchors[0]) // 2  # 锚点的数量\n    self.grid = [torch.zeros(1)] * self.nl  # 初始化网格\n    self.anchor_grid = [torch.zeros(1)] * self.nl  # 初始化锚点框\n    self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)=shape(3,3,2)\n    self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # 输出卷积结果\n    self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n```\n\nyolov5的检测头仍为**FPN结构**，所以self.m为3个输出卷积。这三个输出卷积模块的channel变化分别为128$\\longrightarrow$255|256$\\longrightarrow$255|512$\\longrightarrow$255。\nself.no为每个anchor位置的输出channel维度，每个位置都预测80个类（coco）+ 4个位置坐标xywh + 1个confidence score。所以输出channel为85。每个尺度下有3个anchor位置，所以输出85*3=255个channel。检测层数为3，锚点数量为85\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319214026.png)\n\n### forward()函数\n\n接下来看head部分的forward()函数：\n\n```python\ndef forward(self, x):\n    z = []  # inference output\n    for i in range(self.nl):\n        x[i] = self.m[i](x[i])  # conv\n        bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n        x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n        if not self.training:  # inference\n            if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n            y = x[i].sigmoid()\n            if self.inplace:\n                y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n            else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                y = torch.cat((xy, wh, y[..., 4:]), -1)\n            z.append(y.view(bs, -1, self.no))\n\n    return x if self.training else (torch.cat(z, 1), x)\n```\n\nx是一个列表的形式，分别对应着3个head的输入。它们的shape分别为：\n\n- [bs, 128, 32, 32]\n- [1, 256, 16, 16]\n- [1, 512, 8, 8]\n\n三个输入先后被送入了3个卷积，得到输出结果。\n\n```python\nx[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n```\n\n这里将x进行变换从：\n\nx[0]：(bs,255,32,32) => x(bs,3,32,32,85)\nx[1]：(bs,255,32,32) => x(bs,3,16,16,85)\nx[2]：(bs,255,32,32) => x(bs,3,8,8,85)\n\n### make_grid()函数\n\n```python\ndef _make_grid(self, nx=20, ny=20, i=0):\n    d = self.anchors[i].device\n    if check_version(torch.__version__, '1.10.0'):  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)], indexing='ij')\n    else:\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)])\n    grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n    anchor_grid = (self.anchors[i].clone() * self.stride[i]) \\\n        .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n    return grid, anchor_grid\n```\n\n这里的_make_grid()函数是准备好格点。所有的预测的单位长度都是基于grid层面的而不是原图。注意每一层的grid的尺寸都是不一样的，和每一层输出的尺寸w,h是一样的。\n\n```python\ny = x[i].sigmoid()\nif self.inplace:\n    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\nelse:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n    y = torch.cat((xy, wh, y[..., 4:]), -1)\nz.append(y.view(bs, -1, self.no))\n```\n\n这里是inference的核心代码，对应的是yolov5的bbox回归机制。yolov5的回归机制如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/587c92d2ea4f4be386365fce177112f6.png)\n\n相较于yolov3的回归机制，可以明显的发现box center的x，y的预测被乘以2并减去了0.5，所以这里的值域从yolov3里的(0，1)注意是开区间，变成了(-0.5， 1.5)。从表面理解是yolov5可以跨半个格点预测了，这样可以提高对格点周围的bbox的召回。当然还有一个好处就是也解决了yolov3中因为sigmoid开区间而导致中心无法到达边界处的问题。\n\n同样，在w，h的回归上，yolov5也有了新的变化，同样对比yolov3的源代码：\n\n```python\nx = torch.sigmoid(prediction[..., 0])  # Center x  #B A H W\ny = torch.sigmoid(prediction[..., 1])  # Center y  #B A H W\nw = prediction[..., 2]  # Width                    #B A H W\nh = prediction[..., 3]  # Height                   #B A H W\npred_conf = torch.sigmoid(prediction[..., 4])  # Conf\npred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n```\n\n很明显yolov3对于w，h没有做sigmoid，而在yolov5中对于x，y，w，h都做了sigmoid。其次yolov5的预测缩放比例变成了：(2*w_pred/h_pred) ^2。\n值域从基于anchor宽高的（0，+∞）变成了（0，4）。这可能目的在于使预测的框范围更精准，通过sigmoid约束，让回归的框比例尺寸更为合理。\n\n## class Model()代码分析\n\n接下来分析Model类里面的函数。主要分析它的前向传播过程，这里有两个函数：forward()和forward_once()。\n\n### forward()函数\n\n```python\ndef forward(self, x, augment=False, profile=False, visualize=False):\n    if augment:\n        return self._forward_augment(x)  # augmented inference, None\n    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n\ndef _forward_augment(self, x):\n    img_size = x.shape[-2:]  # height, width\n    s = [1, 0.83, 0.67]  # scales\n    f = [None, 3, None]  # flips (2-ud, 3-lr)\n    y = []  # outputs\n    for si, fi in zip(s, f):\n        xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))\n        yi = self._forward_once(xi)[0]  # forward\n        # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\n        yi = self._descale_pred(yi, fi, si, img_size)\n        y.append(yi)\n    y = self._clip_augmented(y)  # clip augmented tails\n    return torch.cat(y, 1), None  # augmented inference, train\n```\n\nself.forward()函数里面augment可以理解为控制TTA，如果打开会对图片进行**scale**和**flip**。默认是关闭的。\n\nscale_img的源码如下：\n\n#### scale_img()函数\n\n```python\ndef scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)\n    # Scales img(bs,3,y,x) by ratio constrained to gs-multiple\n    if ratio == 1.0:\n        return img\n    else:\n        h, w = img.shape[2:]\n        s = (int(h * ratio), int(w * ratio))  # new size\n        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\n        if not same_shape:  # pad/crop img\n            h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))\n        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean\n```\n\n通过普通的双线性插值实现，根据ratio来控制图片的缩放比例，最后通过pad 0补齐到原图的尺寸。\n\n### forward_once()函数\n\n```python\ndef _forward_once(self, x, profile=False, visualize=False):\n    y, dt = [], []  # outputs\n    for m in self.model:\n        if m.f != -1:  # if not from previous layer\n            x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n        if profile:\n            self._profile_one_layer(m, x, dt)\n        x = m(x)  # run\n        y.append(x if m.i in self.save else None)  # save output\n        if visualize:\n            feature_visualization(x, m.type, m.i, save_dir=visualize)\n    return x\n```\n\nself.foward_once()就是前向执行一次model里的所有module，得到结果。profile参数打开会记录每个模块的平均执行时长和flops用于分析模型的瓶颈，提高模型的执行速度和降低显存占用。\n\n本文分析了yolov5head部分的前向传播和inference的源码。\n\n参考资料：\n\n1.  [yolov5深度剖析+源码debug级讲解系列（三）yolov5 head源码解析](<https://blog.csdn.net/weixin_36714575/article/details/114238645>)\n\n2.  [YOLO全系列更新,YOLO的进化历程](https://www.likecs.com/show-204961615.html)","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记3——源码剖析——Backbone部分1","url":"/2022/03/18/Yolov5学习笔记3/","content":"\n# Yolov5学习笔记3——源码剖析——Backbone部分1\n\n## yolo.py源码解析\n\n该代码路径为\"**models/yolo.py**\"\n\n```python\ndef __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\n    super().__init__()\n    if isinstance(cfg, dict):\n        self.yaml = cfg  # model dict\n    else:  # is *.yaml\n        import yaml  # for torch hub\n        self.yaml_file = Path(cfg).name\n        with open(cfg, encoding='ascii', errors='ignore') as f:\n            self.yaml = yaml.safe_load(f)  # model dict\n\n    # Define model\n    ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\n    if nc and nc != self.yaml['nc']:\n        LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n        self.yaml['nc'] = nc  # override yaml value\n    if anchors:\n        LOGGER.info(f'Overriding model.yaml anchors with anchors={anchors}')\n        self.yaml['anchors'] = round(anchors)  # override yaml value\n    self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\n    self.names = [str(i) for i in range(self.yaml['nc'])]  # default names\n    self.inplace = self.yaml.get('inplace', True)\n\n    # Build strides, anchors\n    m = self.model[-1]  # Detect()\n    if isinstance(m, Detect):\n        s = 256  # 2x min stride\n        m.inplace = self.inplace\n        m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n        m.anchors /= m.stride.view(-1, 1, 1)\n        check_anchor_order(m)\n        self.stride = m.stride\n        self._initialize_biases()  # only run once\n\n    # Init weights, biases\n    initialize_weights(self)\n    self.info()\n    LOGGER.info('')\n```\n\n首先需要解析yaml配置文件，以yolov5s.yaml进行debug，可以看到解析后是一个dict形式：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319201158.png)\n\n- nc代表类别数量\n- depth_multiple是控制模型深度的参数。\n- width_multiple是一个控制模型宽度的参数。\n- anchors是预置的锚框，FPN每层设置3个，共有3*3=9个。\n- backbone是backbone网络的构建参数，根据这个配置可以加载出backbone网络。\n- head是yolo head网络的构建参数，根据这个配置可以加载出yolo head的网络。（其实可以认为这部分是neck+head）\n\n```python\n# 定义模型\nch = self.yaml['ch'] = self.yaml.get('ch', ch)  # 输入通道\nif nc and nc != self.yaml['nc']:\n    LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n    self.yaml['nc'] = nc  # 覆盖yaml的值\n```\n\n这里判断一下输入的channel和配置文件里的是否一致，不一致则以输入参数为准。\n\n```python\nself.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # 模型, 保存列表\n```\n\n然后进入核心的parse_model()函数。\n\n```python\ndef parse_model(d, ch):  # model_dict, input_channels(3)\n    \n    LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")\n    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n# 这部分很简单，读出配置dict里面的参数，na是判断anchor的数量,no是根据anchor数量推断的输出维度，比如对于coco是255。输出维度=anchor数量*（类别数量+置信度+xywh四个回归坐标）。\n\n    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n        m = eval(m) if isinstance(m, str) else m  # eval strings\n        for j, a in enumerate(args):\n            try:\n                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n            except NameError:\n                pass           \n# 这里开始迭代循环backbone与head的配置。f，n，m，args分别代表着从哪层开始，模块的默认深度，模块的类型和模块的参数。\n\n        n = n_ = max(round(n * gd), 1) if n > 1 else n  # depth gain\n# 网络用n*gd控制模块的深度缩放，比如对于yolo5s来讲，gd为0.33，也就是把默认的深度缩放为原来的1/3。深度在这里指的是类似CSP这种模块的重复迭代次数。而宽度一般我们指的是特征图的channel。一般控制模型的缩放，我们就会控制深度、宽度和resolution（efficientnet的思路）。\n\n        if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv,\n                 BottleneckCSP, C3, C3TR, C3SPP, C3Ghost]:\n            c1, c2 = ch[f], args[0]\n            if c2 != no:  # if not output\n                c2 = make_divisible(c2 * gw, 8)\n\n            args = [c1, c2, *args[1:]]\n            if m in [BottleneckCSP, C3, C3TR, C3Ghost]:\n                args.insert(2, n)  # number of repeats\n                n = 1\n# 对于以上的这几种类型的模块，ch是一个用来保存之前所有的模块输出的channle，ch[-1]代表着上一个模块的输出通道。args[0]是默认的输出通道。\n        elif m is nn.BatchNorm2d:\n            args = [ch[f]]\n        elif m is Concat:\n            c2 = sum(ch[x] for x in f)\n        elif m is Detect:\n            args.append([ch[x] for x in f])\n            if isinstance(args[1], int):  # number of anchors\n                args[1] = [list(range(args[1] * 2))] * len(f)\n        elif m is Contract:\n            c2 = ch[f] * args[0] ** 2\n        elif m is Expand:\n            c2 = ch[f] // args[0] ** 2\n        else:\n            c2 = ch[f]\n\n        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n        t = str(m)[8:-2].replace('__main__.', '')  # module type\n        np = sum(x.numel() for x in m_.parameters())  # number params\n        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n        LOGGER.info(f'{i:>3}{str(f):>18}{n_:>3}{np:10.0f}  {t:<40}{str(args):<30}')  # print\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n        layers.append(m_)\n        if i == 0:\n            ch = []\n        ch.append(c2)\n    return nn.Sequential(*layers), sorted(save)\n```\n\n逐步分析这个函数：\n\n```python\nLOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")\nanchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\nna = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\nno = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n```\n\n这部分很简单，读出配置dict里面的参数，na是判断anchor的数量,no是根据anchor数量推断的输出维度，比如对于coco是255。输出维度=anchor数量*（类别数量+置信度+xywh四个回归坐标）。\n\n```python\nfor i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n    m = eval(m) if isinstance(m, str) else m  # eval strings\n    for j, a in enumerate(args):\n        try:\n            args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n        except NameError:\n            pass\n```\n\n这里开始迭代循环backbone与head的配置。f，n，m，args分别代表着从哪层开始，模块的默认深度，模块的类型和模块的参数。\n\n```python\nn = n_ = max(round(n * gd), 1) if n > 1 else n  # depth gain\n```\n\n网络用n*gd控制模块的深度缩放，比如对于yolo5s来讲，gd为0.33，也就是把默认的深度缩放为原来的1/3。深度在这里指的是类似CSP这种模块的重复迭代次数。而宽度一般我们指的是特征图的channel。一般控制模型的缩放，我们就会控制深度、宽度和resolution（efficientnet的思路）。\n\n```python\nif m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv,\n         BottleneckCSP, C3, C3TR, C3SPP, C3Ghost]:\n    c1, c2 = ch[f], args[0]\n    if c2 != no:  # if not output\n        c2 = make_divisible(c2 * gw, 8)\n```\n\n对于以上的这几种类型的模块，ch是一个用来保存之前所有的模块输出的channle，ch[-1]代表着上一个模块的输出通道。args[0]是默认的输出通道\n\n上述第五行make_divisible()函数的源代码如下：\n\n```python\ndef make_divisible(x, divisor):\n    # Returns nearest x divisible by divisor\n    if isinstance(divisor, torch.Tensor):\n        divisor = int(divisor.max())  # to int\n    return math.ceil(x / divisor) * divisor\n```\n\n这里配合make_divisible()函数，是为了放缩网络模块的宽度（既输出的通道数），比如对于第一个模块“Focus”，默认的输出通道是64，而yolov5s里的放缩系数是0.5，所以通过以上代码变换，最终的输出通道为32。make_divisible()函数保证了输出的通道是8的倍数。\n\n```python\n\t\t\targs = [c1, c2, *args[1:]]\n\t\t\tif m in [BottleneckCSP, C3, C3TR, C3Ghost]:\n    \t\t\targs.insert(2, n)  # number of repeats\n   \t\t\t\tn = 1\n```\n经过以上处理，args里面保存的前两个参数就是module的输入通道数、输出通道数。只有BottleneckCSP和C3这两种module会根据深度参数n被调整该模块的重复迭加次数。\n\n```python\nelif m is nn.BatchNorm2d:\n    args = [ch[f]]\nelif m is Concat:\n    c2 = sum(ch[x] for x in f)\nelif m is Detect:\n    args.append([ch[x] for x in f])\n    if isinstance(args[1], int):  # number of anchors\n        args[1] = [list(range(args[1] * 2))] * len(f)\nelif m is Contract:\n    c2 = ch[f] * args[0] ** 2\nelif m is Expand:\n    c2 = ch[f] // args[0] ** 2\nelse:\n    c2 = ch[f]\n```\n\n以上是其他几种类型的Module。\n如果是nn.BatchNorm2d则通道数保持不变。\n如果是Concat则f是所有需要拼接层的index，则输出通道c2是所有层的和。\n如果是Detect则对应检测头，这部分后面再详细讲。\nContract和Expand目前未在模型中使用。\n\n```python\nm_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n```\n\n这里把args里的参数用于构建了module m，然后模块的循环次数用参数n控制。整体都受到宽度缩放，C3模块受到深度缩放。\n\n```python\nt = str(m)[8:-2].replace('__main__.', '')  # module type\nnp = sum(x.numel() for x in m_.parameters())  # number params\nm_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\nLOGGER.info(f'{i:>3}{str(f):>18}{n_:>3}{np:10.0f}  {t:<40}{str(args):<30}')  # print\n```\n\n这里做了一些输出打印，可以看到每一层module构建的编号、参数量等情况，如下所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319203847.png)\n\n```python\nsave.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n    layers.append(m_)\n    if i == 0:\n        ch = []\n    ch.append(c2)\nreturn nn.Sequential(*layers), sorted(save)\n```\n\n最后把构建的模块保存到layers里，把该层的输出通道数写入ch列表里。\n待全部循环结束后再构建成模型。至此模型就全部构建完毕了。\n\n再回到yolo.py里刚刚调用parse_model的位置继续学习init()函数的学习。\n\n```python\nm = self.model[-1]  # Detect()\nif isinstance(m, Detect):\n    s = 256  # 2x min stride\n    m.inplace = self.inplace\n    m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n    m.anchors /= m.stride.view(-1, 1, 1)\n    check_anchor_order(m)\n    self.stride = m.stride\n    self._initialize_biases()  # only run once\n\n# Init weights, biases\ninitialize_weights(self)\nself.info()\nLOGGER.info('')\n```\n\n这里通过调用一次forward()函数，输入了一个[1, 3, 256, 256]的tensor(ch=3)，然后得到FPN输出结果的维度。然后求出了下次采样的倍数stride：8，16，32。\n最后把anchor除以以上的数值，将anchor放缩到了3个不同的尺度上。anchor的最终shape是[3,3,2]。\n至此init()函数已经完整的过了一遍。\n\n## 其他Modules中的源码解析\n\n在网络构建的过程中涉及到了多种Modules，这些Modules默认在models文件夹下面的common.py文件里我们下面还过一下这些函数。\n\n### 普通卷积Conv\n\n```python\nclass Conv(nn.Module):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n    def forward_fuse(self, x):\n        return self.act(self.conv(x))\n```\n\n普通的卷积，这里调用了autopad()函数计算了same-padding所需要的padding数量。\n默认的激活函数是SiLU()，即Sigmoid激活函数。各种激活函数见下图。\nSiLU函数形式：f(x)=x⋅σ(x)\n导函数形式： f'(x)=f(x)+σ(x)(1−f(x))\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1b4516bab0d8e3664e773d091d59ea32.png)\n\n**yolo5的作者使用了 Leaky ReLU 和 Sigmoid 激活函数。yolo5中中间/隐藏层使用了 Leaky ReLU 激活函数，最后的检测层使用了 Sigmoid 形激活函数。而YOLO V4使用Mish激活函数。**\n\n### BottleNeck结构\n\n```python\nclass Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n```\n\n可以看出BottleNeck结构默认是先1x1卷积缩小channel为原来的1/2，再通过3x3卷积提取特征。如果输入通道c1和3x3卷积输出通道c2相等，则进行残差输出。shortcut参数控制是否进行残差连接。\n\n### BottleNeckCSP和C3\n\n```python\nclass BottleneckCSP(nn.Module):\n    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n        self.cv4 = Conv(2 * c_, c2, 1, 1)\n        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n        self.act = nn.SiLU()\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n\n    def forward(self, x):\n        y1 = self.cv3(self.m(self.cv1(x)))\n        y2 = self.cv2(x)\n        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n\n\nclass C3(nn.Module):\n    # CSP Bottleneck with 3 convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])\n\n    def forward(self, x):\n        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n```\n\n在common.py里实现了两种csp结构：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20210228161030675.png)\n\nBottleneckCSP就完全对应着上面的结构。但是作者在yoloV5 4.0的版本中将这部分结构改成了C3。C3的结构如下图：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20210228161503472.png)\n\n残差之后的Conv被去掉了，激活函数从上面的LeakyRelu变为了SiLU。\n\n### SPP\n\n```python\nclass SPP(nn.Module):\n    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729\n    def __init__(self, c1, c2, k=(5, 9, 13)):\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n\n    def forward(self, x):\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning\n            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n```\n\nSPP模块将输入通道减半，然后分别做kernel size为5，9，13的maxpooling，最后将结过拼接，包含原始输入的四组结果合并后通道应该是原来的2倍。\n\n### Focus\n\n```python\nclass Focus(nn.Module):\n    # Focus wh information into c-space\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\n        # self.contract = Contract(gain=2)\n\n    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n        # return self.conv(self.contract(x))\n```\n\n把feature map 切成四等分，然后叠加起来。最后的结果是通道数变为原来的四倍，resolution为原来的1/4（H，W分别减半）。最后通过一个卷积调整通道数为预先设置。\n\n\n\n参考文章：\n\n1. [yolov5深度剖析+源码debug级讲解系列（二）backbone构建](<https://blog.csdn.net/weixin_36714575/article/details/114211796?spm=1001.2014.3001.5501>)\n\n2. [yolo5的改进策略](<https://blog.csdn.net/l641208111/article/details/109286497>)\n\n3. [深入浅出Yolo系列之Yolov5核心基础知识完整讲解]([深入浅出Yolo系列之Yolov5核心基础知识完整讲解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/172121380))","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"实用软件、脚本和工具","url":"/2022/03/17/实用软件、插件和脚本/","content":"\n# 实用软件、脚本和工具。\n\n## Tampermonkey油猴插件\n\n油猴浏览器插件实际上是一个用户脚本管理器，主要依靠各大社区编写的扩展脚本（JavaScript代码）运行在浏览器上，来改变被访问网页的功能，提升我们的网页浏览体验 。\n\n### 插件的安装\n\n**方法一**\n\n①途径——浏览器上打开[谷歌网上应用店](https://chrome.google.com/webstore/search/tampermonkey?hl=zh-CN)搜索 ,然后点击**添加至Chrome**。(谷歌浏览器专用，一般需要科学上网。)\n\n②途径——或者直接进入[Tampermonkey官网](https://www.tampermonkey.net/)**下载**然后添加到浏览器中。\n\n![](F:\\Blog\\picture\\20220317232319.png)\n\n以上，**Tampermonkey Stable**为正式版，**Tampermonkey Beta**为测试版\n\n**方法二**\n\n由**方法一**可得知Tampermonkey是附属于google上的，考虑到文明上网的普及问题，这里我们也可以在其他渠道获取[Tampermonkey的crx文件](https://wmhl.lanzoui.com/ib8glab)，然后解压提取出来。\n\n然后进入浏览器**设置**→**扩展程序**，进入后再打开右上角的**开发者模式**并保持该窗口的开启。之后找到被解压后的`tampermonkey.crx`文件，将其拖动到**扩展程序**界面，释放并同意安装。\n\n![](F:\\Blog\\picture\\20220317233113.png)\n\n### **获取脚本的方式**\n\n我常用[GreasyFork](https://greasyfork.org/zh-CN/scripts/28497-%E7%BD%91%E9%A1%B5%E9%99%90%E5%88%B6%E8%A7%A3%E9%99%A4-%E6%94%B9)，用的人也多，最重要的是支持中文！想用什么脚本吗，或者脚本应用在什么网站都可以直接搜索。\n\n### 我的常用脚本。\n\n|                             脚本                             |                             备注                             |\n| :----------------------------------------------------------: | :----------------------------------------------------------: |\n| [网页限制解除(改)](https://greasyfork.org/zh-CN/scripts/28497-%E7%BD%91%E9%A1%B5%E9%99%90%E5%88%B6%E8%A7%A3%E9%99%A4-%E6%94%B9) | 通杀大部分网站,可以解除禁止复制、剪切、选择文本、右键菜单的限制。 |\n| [Github增强-高速下载](https://greasyfork.org/scripts/412245) | 高速下载 Git Clone/SSH、Release、Raw、Code(ZIP) 等文件、项目列表单文件快捷下载 (☁) |\n| [知网PDF下载助手](https://greasyfork.org/zh-CN/scripts/389343) | 添加知网文献PDF下载按钮，支持搜索列表、详情页，下载论文章节目录，批量下载文献，一键切换CAJ和PDF格式 |\n| [免登录去弹窗](https://greasyfork.org/zh-CN/scripts/428960)  | CSDN/知乎/哔哩哔哩/简书免登录去除弹窗广告+去除所有广告+界面优化 |\n| [蓝奏云网盘增强](https://greasyfork.org/zh-CN/scripts/419224) | 刷新不回根目录、后退返回上一级、右键文件显示菜单、点击直接下载文件、点击空白进入目录、自动显示更多文件、一键复制所有分享链接、自定义分享链接域名、自动打开/复制分享链接、带密码的分享链接自动输密码、拖入文件自动显示上传框、输入密码后回车确认 |\n|  [电子书下载1](https://greasyfork.org/zh-CN/scripts/438563)  |       直接下载全国图书馆参考咨询联盟为PDF，有目录书签        |\n|  [电子书下载2](https://greasyfork.org/zh-CN/scripts/432075)  | 查询全国图书馆参考咨询联盟、读秀、超星、龙岩是否有书互助,自动获取435w无重全文PDF,（全网独家）不需要某度会员高速下载PDF！另（独家）全程免注册、免脚本、手机即可搜,下,看，复制浏览器打开： http://172.247.14.184/ |\n|  [VIP视频网站](https://greasyfork.org/zh-CN/scripts/418804)  | 解锁B站大会员番剧、B站视频解析下载；全网VIP视频免费破解去广告；全网音乐直接下载；油管、Facebook等国外视频解析下载 |\n|                              ……                              |               有需求就进脚本网站找对应脚本即可               |\n\n## 一些其他插件\n\n### ublock origin插件\n\n[ublock origin](https://chrome.zzzmh.cn/info?token=cjpalhdlnbpafiamejdnhcphjbkeiagm)是一款高效的网络请求过滤工具，占用极低的内存和 CPU，占用极低的内存和CPU，和其他常见的过滤工具相比，它能够加载并执行上千条过滤规则。\n\n### IGG谷歌访问助手\n\n[IGG谷歌访问助手](https://chrome.zzzmh.cn/info?token=ncldcbhpeplkfijdhnoepdgdnmjkckij)免费为广大科研及医务工作者、高校学生提供谷歌学术文献、期刊等资料产品的查询与加速访问。\n您可以从一个位置搜索众多学科和资料来源：来自学术著作出版商、专业性社团、预印本、各大学及其他学术组织的经同行评论的文章、论文、图书、摘要和文章。可帮助您在整个学术领域中确定相关性最强的研究。\n\n### Abcd PDF\n\n[Abcd PDF](https://chrome.zzzmh.cn/info?token=mcgnagemenncafhpabimmooimpngdcpn)可以在线将PDF转换为 Word、Excel和PPT。可在线编辑Word和PDF。100%免费，提高您的生产力。\nAbcd PDF 扩展是 100% 免费的多合一 PDF 工具\n\n### Infinity新标签页\n\n[Infinity新标签页](https://chrome.zzzmh.cn/info?token=dbfmnekepjoapopniengjbcpnbljalfg)是一款基于html5的Chrome扩展程序，它重新定义了您的Chrome新标签页。相比Chrome自带的新标签页，您可以通过Infinity自定义添加自己喜爱的网站，我们重绘了上千图标，当然您也可以自定义这些网站的图标。除此之外，您还可以更新新标签页的背景图片，既可以使用您自己的图片，也可以使用自动更换图片。集成了天气，待办事项，笔记等功能，甚至还能显示你的Gmail邮件数量和通知。\n\n### 彩云小译\n\n[彩云小译](https://chrome.zzzmh.cn/info?token=jmpepeebcbihafjjadogphmbgiffiajh)双语对照网页翻译插件，针对浏览器开发的一款网页翻译工具，一键高效获取母语阅读体验。\n\n> 上文中提到的所有插件的某度链接如下：\n>\n> ​\t链接: https://pan.baidu.com/s/17yGQF_krywzgEFEli60zsg?pwd=wz83 \n>\n> ​\t提取码: wz83\n\n## 一些软件\n\n### Everything\n\n\"[Everything](https://www.voidtools.com/zh-cn/)\" 是 Windows 上一款搜索引擎，它能够基于文件名快速定文件和文件夹位置。\n不像 Windows 内置搜索，\"Everything\" 默认显示电脑上每个文件和文件夹 (就如其名 \"Everything\")。\n您在搜索框输入的关键词将会筛选显示的文件和文件夹。\n\n### 流量盘\n\n流量盘是一款高速下载某度云资源的软件。\n\n某度链接：\n\n​\t链接: https://pan.baidu.com/s/1Ukaw2MCfg7uJJ_t32fwOmA \n\n​\t提取码: ivky \n\n\n\n未完待续……","categories":["效率"]},{"title":"YOLOv5学习笔记2","url":"/2022/03/17/Yolov5学习笔记2/","content":"\n\n\n# Yolov5学习笔记2——代码框架\n\n打开Yolov5的代码，可以看到有许多文件夹和很多的子文件。本文主要弄清楚Yolov5的代码框架。\n\n## data文件夹\n\n该文件夹下主要存放项目运行所需要的数据，包括一些超参、默认输入的图片等。\n\n### hyps文件夹\n\n该文件夹下存放的都是训练参数\n\n1. `hyp.Objects365.yaml`——项目在进行Objects365训练的超参数。\n2. `hyp.scratch-high.yaml`——里面的参数为对COCO数据集从零开始进行高增强训练的超参数。\n3. `hyp.scratch-low.yaml`——对COCO数据集从零开始进行低增强训练的超参数。\n4. `hyp.scratch-med.yaml`——对COCO数据集从零开始进行中等增强训练的超参数。\n5. `hyp.VOC.yaml`——对VOC数据集进行训练时的超参数。\n\n一般这些参数都不需要改动，这是作者团队大量训练记录的最好的参数结果。\n\n### images文件夹\n\n该文件夹下存放了代码默认执行detect.py时检测的图片，执行后会在目录中新建一个**runs**文件夹并将检测的结果存放在/runs/detect文件夹下。\n\n### scripts文件夹\n\n脚本文件夹。提供了下载权重文件夹、COCO数据集、COCO128数据集的方法。直接执行该脚本文件就可以直接下载`yolov5x.pt`文件和训练的数据集。\n\n**注意**：如果想要运行脚本文件的话，首先要确定`PyCharm`有没有安装`PowerShell`平台，一般`PyCharm`会提示你安装的。\n\n当然我还是比较推荐直接从`github`官网中下载。而数据集当然是自制。\n\n### 其他.yaml文件\n\n这些文件主要是对一些数据集做一个补充说明。如存放的路径、训练的路径、测试的路径等，以及这些数据集有多少张图片，定义了多少个类别等。当然代码中也写了下载这些数据集的方法。\n\n1. `Argoverse.yaml`——Argo AI提供的Argoverse-HD数据集(环形前置中央摄像头)\n\n2. `coco.yaml`——由微软提供的COCO 2017数据集，网址为：http://cocodataset.org\n3. `coco128.yaml`——由作者所在公司Ultralytics提供的COCO128数据集(来自COCO train2017的前128张图片)，网址为：https://www.kaggle.com/ultralytics/coco128\n4. `GlobalWheat2020.yaml`——由萨斯喀彻温大学提供  全球小麦2020数据集网址为：http://www.global-wheat.com/\n5. `Objects365.yaml`——提供了365个检测对象的数据集，几乎涵盖了生活中的各种常见物体。\n6. `SKU-110K.yaml`——由Trax retail提供的SKU-110K零售项目数据集网址为：https://github.com/eg4000/SKU110K_CVPR19\n7. `VisDrone.yaml`——由天津大学提供的VisDrone2019-DET数据集(无人机拍摄的图片)https://github.com/VisDrone/VisDrone-Dataset\n8. `VOC.yaml`——由牛津大学提供的PASCAL VOC数据集http://host.robots.ox.ac.uk/pascal/VOC\n9. `xView.yaml`——由美国国家地理空间情报局(NGA)提供的DIUx xView 2018挑战赛中的数据集。https://challenge.xviewdataset.org\n\n## model文件夹——模型文件\n\n在本文件夹中主要存放了各种Yolo算法的模型文件。在这些模型文件中定义了如下参数：\n\n|      参数      |         解释         |\n| :------------: | :------------------: |\n|       nc       | 训练和检测的类别数量 |\n| depth_multiple |       网络深度       |\n| width_multiple |       网络宽度       |\n|    anchors     |      锚点框参数      |\n|    backbone    |     骨干网络参数     |\n|      head      |        检测头        |\n\n**下面以yolov5s.yaml为例，对里面的相关参数进行详细解释。**\n\n###  .yaml介绍\n\n1. YAML(YAML Ain`t Markup language)文件， **它不是一个标记语言**。配置文件有xml、properties等，但 **YAML是以数据为中心 **，更适合做配置文件。\n2. YAML的语法和其他高级语言类似，并且可以 **简单表达清单、散列表，标量**等数据形态。\n3. 它使用 **空白符号缩进 **和大量依赖外观的特色，特别适合用来表达或编辑数据结构、各种配置文件、倾印调试内容、文件大纲。 [yaml介绍](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.runoob.com%2Fw3cnote%2Fyaml-intro.html)\n4. 大小写敏感；缩进不允许使用tab，只允许空格；缩进的空格数不重要，只要相同层级的元素左对齐即可；’#'表示注释；使用缩进表示层级关系。\n\n**注意**，在`yaml`文件中空格数其实也是重要的！在建立YAML 对象时，对象键值对使用冒号结构表示 `key: value`， **冒号后面要加一个空格**。\n\n### parameters\n\n```python\n# Parameters\nnc: 80  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\n```\n\n1. nc： **类别数**，你的类别有多少就填写多少。从1开始算起，不是0-14这样算。\n2. depth_multiple：控制 **模型的深度**。\n3. width_multiple：控制 **卷积核的个数**。\n\n> **depth_multiple **是用在 **backbone **中的 **number≠1的情况下， **即在Bottleneck层使用，控制模型的深度，yolov5s中设置为0.33，假设yolov5l中有三个Bottleneck，那yolov5s中就只有一个Bottleneck。\n> 因为一般 **number=1 **表示的是 **功能背景的层 **，比如说下采样Conv、Focus、SPP（空间金字塔池化）。\n> ——————————————————————————————————————\n> **width_multiple **主要是用于设置arguments，例如yolov5s设置为0.5，Focus就变成[32, 3]，Conv就变成[64, 3, 2]。\n> 以此类推，卷积核的个数都变成了设置的一半。\n\nyolov5提供了s、m、l、x四种，所有的`.yaml`文件都设置差不多，只有上面2和3的设置不同，作者团队很厉害，只需要修改这两个参数就可以调整模型的网络结构。\n\n### anchors\n\n```python\nanchors:\n  - [10,13, 16,30, 33,23]  # P3/8\n  - [30,61, 62,45, 59,119]  # P4/16\n  - [116,90, 156,198, 373,326]  # P5/32\n```\n\n首先，anchor box就是从训练集中真实框（ground truth）中统计或聚类得到的几个不同尺寸的框。避免模型在训练的时候盲目的找，有助于模型快速收敛。假设每个网格对应k个anchor，也就是模型在训练的时候，它只是会在每一个网格附近找出这k种形状，不会找其他的。\n\nanchor其实就是对预测的对象范围进行约束，并加入了尺寸先验经验，从而实现多尺度学习的目的。\n\n而对于yolov5l来说，输出为3个尺度的特征图，分别为13×13、26×26、52×52，对应着9个anchor，每个尺度均分3个anchor。\n\n> 最小的13×13的特征图上由于其感受野最大，应该使用大的anchor(116x90)，(156x198)，(373x326)，这几个坐标是针对原始输入的，即416×416的，因此要除以32把尺度缩放到13×3下使用，适合较大的目标检测。中等的26×26特征图上由于其具有中等感受野故应用中等的anchor box (30x61)，(62x45)，(59x119)，适合检测中等大小的目标。较大的33×23特征图上由于其具有较小的感受野故应用最小的anchor box(10x13)，(16x30)，(33x23)，适合检测较小的目标。具体使用就是每个grid cell都有3个anchor box。\n\n根据检测层来相应增加anchors。\n\n### backbone\n\n```python\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n```\n\n1. Bottleneck 可以译为“瓶颈层”。\n2. from列参数： **-1**代表是从上一层获得的输入 ， **-2**表示从上两层获得的输入 （head同理）。\n3. number列参数： 1表示只有一个，3表示有三个相同的模块。\n4. SPPF、Conv、Bottleneck、BottleneckCSP的代码可以在 `./models/common.py`中获取到。\n5. [64, 6, 2, 2]解析得到[3, 32, 3] ，输入为3（RGB），输出为32，卷积核k为3；<!--存疑，暂时没有太搞懂args里的参数-->\n6. [128, 3, 2]这是固定的，128表示输出128个卷积核个数。根据[128, 3, 2]解析得到[32, 64, 3, 2] ，32是输入，64是输出（128×0.5=64），3表示3×3的卷积核，2表示步长为2。\n7. 主干网是图片从大到小，深度不断加深。\n8. `args`这里的输入都省去了，因为输入都是上层的输出。为了修改过于麻烦，这里输入的获取是从./models/yolo.py的 `def parse_model(md, ch)`函数中解析得到的。\n\n### head\n\n**head检测头**：一般表示的是经过主干网后输出的特征图，特征图输入head中进行检测，包括类别和位置的检测。\n\n```python\n# YOLOv5 v6.0 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]\n```\n\n运行models文件夹下yolo.py文件，得到的解析图如下所示：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220317165308.png)\n\n与上面的模型似乎不太一样。\n\n后面弄清楚了再来补充。\n\n### common.py\n\n该部分是backbone各个模块参数讲解。\n\n## utils文件夹——工具包\n\n---------------------------------------------------------------------------------------------------------------------\n\n这三个文件夹都用的很少，需要用到时再做了解。\n\n### aws文件夹\n\n里面是一些跟其他语言对接的文件\n\n### flask_rest_api\n\n存放了做后端API的一些例程代码和封装好的函数\n\n### loggers\n\n终端需要打印任务信息的接口函数。\n\n------------------------------------------------------------------------------\n\n### 工具包函数\n\n|      文件名      |            备注            |\n| :--------------: | :------------------------: |\n|  activations.py  |          激活函数          |\n| augmentations.py |        图片增强函数        |\n|  autoanchor.py   |      自动锚点工具函数      |\n|   autobatch.py   |      自动批量处理工具      |\n|  benchmarks.py   |             /              |\n|   callbacks.py   |          回调函数          |\n|   datasets.py    | 用于数据加载和数据集的工具 |\n|   downloads.py   |          下载工具          |\n|    general.py    |        通用工具函数        |\n|     loss.py      |      计算损失函数工具      |\n|    metrics.py    |        模型验证函数        |\n|     plots.py     |         可视化工具         |\n|  torch_utils.py  |     `PyTorch`相关工具      |\n\n\n\n## 主目录下其他.py代码\n\n### detect.py\n\n对图像、视频、路径、流媒体等进行推理检测。  \n\n### export.py\n\n导出`YOLOv5 PyTorch`模型到其他格式。如ONNX、OpenVINO、Core ML以及TensorFlow相关的格式。\n\n### train.py\n\n训练程序，在自定义数据集上训练YOLOv5模型。\n\n\n\n### val.py\n\n在自定义数据集上验证经过训练的YOLOv5模型的准确性 。\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"数学建模学习笔记1","url":"/2022/03/16/数学建模学习1/","content":"\n\n# 数学建模学习笔记1——AHP层次分析法\n\n## 介绍\n\n**作为建模比赛中最基础的模型之一，其主要用于解决评价类问题。(例如：选择哪种方案更好、哪种运动员或者员工表现的更优秀)**\n\nAHP的主要特点是通过建立递阶层次结构，把人类的判断转化到若干因素两两之间重要度的比较上，从而把难于量化的定性判断转化为可操作的重要度的比较上面。在许多情况下，决策者可以直接使用AHP进行决策，极大地提高了决策的有效性、可靠性和可行性，但其本质是一种思维方式，它把复杂问题分解成多个组成因素，又将这些因素按支配关系分别形成递阶层次结构，通过两两比较的方法确定决策阀杆相对重要度的总排序。整个过程体现了人类决策思维的基本特征，即分解、判断、综合。克服了其他方法回避决策者主观判断的缺点。 \n\n## 原理\n\n### 方法一：使用打分法解决评价问题\n\n需要完成权重表格：\n\n|       | 指标权重 | 方案1 | 方案2 | ……   |\n| ----- | -------- | ----- | ----- | ---- |\n| 指标1 |          |       |       |      |\n| 指标2 |          |       |       |      |\n| 指标3 |          |       |       |      |\n| ……    |          |       |       |      |\n\n同颜色的单元格的和为1，它们表示的针对某一因素所占的权重。\n\n#### 例题1：小明想去旅游，初步选择苏杭、北戴河和桂林三地之一作为目标景点。请你确定评价指标、形成评价体系来为小明选择最佳方案。\n\n**分析：**解决评价类问题，首先要想到一下三个问题：\n\n① 我们评价的目标是什么？\n\n② 为了达到这个目标有哪几种可选的方案？\n\n③ 评价的准则或指标是什么？(我们根据什么东西来评价好坏)\n\n一般而言前两个问题的答案是显而易见的，第三个问题的答案需要我们根据题目汇总的*背景材料、常识以及网上搜集到的参考资料*进行结合，从中筛选出最合适的指标。\n\n##### 一致矩阵\n\n$$\na_{ij}=\\frac{i的重要程度}{j的重要程度}\n$$\n\n$$\na_{jk}=\\frac{j的重要程度}{k的重要程度}\n$$\n\n$$\na_{ik}=\\frac{i的重要程度}{k的重要程度}=a_{ij} \\times a_{jk}\n$$\n\n##### 一致性检验\n\n**原理：**检验我们构造的判断矩阵和一致矩阵是否有太大的差别。\n\n##### 一致性检验的步骤\n\n第一步：计算一致性指标CI：\n\n$$\nCI = \\frac{\\lambda_{max}-n}{n-1}\n$$\n第二步：计算查找对应的平均随机一致性指标RI\n\n|  n   | 1    | 2    |  3   |  4   | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |\n| :--: | ---- | ---- | :--: | :--: | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n|  RI  | 0    | 0    | 0.52 | 0.89 | 1.12 | 1.26 | 1.36 | 1.41 | 1.46 | 1.49 | 1.52 | 1.54 | 1.56 | 1.58 | 1.59 |\n\n平均随机一致性RI的表格中n最多是15。\n\n第三步：计算一致性比例CR\n$$\nCR= \\frac{CI}{RI}\n$$\n若CR＜0.1，则可认为判断矩阵的一致性可以接受；否则需要对判断矩阵进行修正。\n\n第四步：计算各层元素度系统目标的合成权重，并进行排序。\n\n##### 层次分析法的一些局限性\n\n1. 评价的决策层不能太多，太多的话n会很大，判断矩阵和一致矩阵的差异也会很大。\n\n2. 如果决策层中指标的数据是已知的，那么我们如何利用这些来使得评价评价准确呢？\n\n   \n\n\n\n","tags":["matlab"],"categories":["数学建模"]},{"title":"IOU相关知识","url":"/2022/03/15/YOLOv5支线学习之IOU相关知识/","content":"\n# IOU相关知识\n\nIoU 作为目标检测算法性能 mAP 计算的一个非常重要的函数。\n\n## 1. 什么是IOU\n\nIoU 的全称为交并比（Intersection over Union），通过这个名称我们大概可以猜到 IoU 的计算方法。IoU 计算的是 “预测的边框” 和 “真实的边框” 的交集和并集的比值。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/IOU计算公式.png)\n\n一般约定，在计算机检测任务中，如果IoU≥0.5，就说检测正确。当然0.5只是约定阈值，你可以将IoU的阈值定的更高。IoU越高，边界框越精确。\n\n**举例如下：**绿色框是准确值，红色框是预测值。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1.jpg)\n\n## 2. 代码实现\n\n```python\ndef calculateIoU(candidateBound, groundTruthBound):\n    cx1 = candidateBound[0]\n    cy1 = candidateBound[1]\n    cx2 = candidateBound[2]\n    cy2 = candidateBound[3]\n \n    gx1 = groundTruthBound[0]\n    gy1 = groundTruthBound[1]\n    gx2 = groundTruthBound[2]\n    gy2 = groundTruthBound[3]\n \n    carea = (cx2 - cx1) * (cy2 - cy1) #C的面积\n    garea = (gx2 - gx1) * (gy2 - gy1) #G的面积\n \n    x1 = max(cx1, gx1)\n    y1 = max(cy1, gy1)\n    x2 = min(cx2, gx2)\n    y2 = min(cy2, gy2)\n    w = max(0, abs(x2 - x1))\n    h = max(0, abs(y2 - y1))\n    area = w * h #C∩G的面积\n \n    iou = area / (carea + garea - area)\n \n    return iou\n```\n\n## 3. 原理解析\n\n计算两个图片的交集，首先想到的是考虑两个图片边框的相对位置，然后按照它们的相对位置分情况讨论。\n\n相对位置无非以下几种：\n\n*左上   左下   右上   右下   包含   互不相交*\n\n如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/2.jpg)\n\n但在实际上这样写代码是做不到的。\n\n换个角度思考：两个框交集的计算的实质是两个集合交集的计算，因此我们可以将两个框的交集的计算简化为：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/3.jpg)\n\n通过简化，我们可以清晰地看到，交集计算的关键是交集上下界点（图中蓝点）的计算。\n\n我们假设集合 A 为 [x 1 x_{1}*x*1，x 2 x_{2}*x*2]，集合 B 为 [y 1 y_{1}*y*1，y 2 y_{2}*y*2]。然后我们来求AB交集的上下界限。\n\n交集计算的逻辑\n\n- 交集下界 z 1 z_{1}*z*1：max ( x 1 , y 1 ) \\text{max}(x_{1}, y_{1})max(*x*1,*y*1)\n- 交集上界 z 2 z_{2}*z*2：min ( x 2 , y 2 ) \\text{min}(x_{2}, y_{2})min(*x*2,*y*2)\n- 如果 z 2 − z 1 z_{2}-z_{1}*z*2−*z*1 小于0，则说明集合 A 和集合 B 没有交集。\n\n在YOLOv5的项目代码中，作者使用如下代码计算iou。\n\n代码路径为：/yolov5-master/build/lib.win-amd64-3.9/utils/metrics.py\n\n```python\ndef bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n    box2 = box2.T\n\n    # Get the coordinates of bounding boxes\n    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n    else:  # transform from xywh to xyxy\n        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n\n    # Intersection area\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n\n    # Union Area\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n    union = w1 * h1 + w2 * h2 - inter + eps\n\n    iou = inter / union\n    if CIoU or DIoU or GIoU:\n        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n                with torch.no_grad():\n                    alpha = v / (v - iou + (1 + eps))\n                return iou - (rho2 / c2 + v * alpha)  # CIoU\n            return iou - rho2 / c2  # DIoU\n        c_area = cw * ch + eps  # convex area\n        return iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n    return iou  # IoU\n```\n\n\n\n","categories":["计算机视觉"]},{"title":"YOLOv5学习笔记1","url":"/2022/03/15/Yolov5学习笔记1/","content":"\n# Yolov5学习笔记-预测部分\n\n## 一些使用tips：\n\n### detect.py运行指令——命令行方式\n\n```python\npython detect.py --source 0  # webcam\n                          img.jpg  # image\n                          vid.mp4  # video\n                          path/  # directory\n                          path/*.jpg  # glob  匹配该文件夹下的所有jpg图片\n                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n```\n\n格式如上，“--”后面为传入的参数，通过命令行的方式往执行的.py文件中传入参数。\n\n以detect.py为例，运行detect.py文件肯定是要有默认的输入参数的，在我所使用的YOLOv5代码下，以右键运行detect.py文件，默认检测的是目录\"\"/yolov5-master/data/images\"下的两张图片。\n\n通过上述代码的命令行指令，可以给detect.py的输入指定图片、视频或者某一个路径下的所有文件或某一个路径下的所有图片，当然还有视频或直接调用电脑或手机的摄像头实现实时检测。\n\n### 运行detect.py需要输入的参数\n\n示例代码：\n\n```python\npython detect.pu --source data/images --weights yolov5s.pt --conf 0.25\n```\n\n|       参数       |   解释   |\n| :--------------: | :------: |\n|      source      | 输入来源 |\n|     weights      |   权重   |\n| conf(confidence) |  失信度  |\n\n运行结果如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/detect文件运行截图.png)\n\n## detect.py相关代码学习\n\n### 参数&解释\n\n接上表继续学习该代码中的参数\n\n|           参数           |                 解释                 |\n| :----------------------: | :----------------------------------: |\n| --imgsz/--img/--img-size |            输入的图片尺寸            |\n|       --conf-thres       |                置信度                |\n|       --iou-thres        |                交并比                |\n|        --view-img        |             实时显示结果             |\n|        --save-txt        |             保存检测结果             |\n|       --save-conf        |              保存置信度              |\n|       --save-crop        |             保存预测结果             |\n|        --classes         |            可以检测的类别            |\n|      --agnostic-nms      |               数据增强               |\n|        --augment         |               增强检测               |\n|        --project         |            结果保存的位置            |\n|          --name          |            保存结果的名字            |\n|        --exist-ok        | 依然保存在默认文件夹，而不新增文件夹 |\n|                          |                                      |\n\n### 代码解析\n\n```python\nparser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n```\n\n虽然指定了图片尺寸，但比如输入1200*800的图片，输出依然为该尺寸，只是在检测过程中会对图片进行裁剪、分割。(代码后面会还原图片尺寸)。\n\n```python\nparser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n```\n\n默认值为0.25，表示只有当置信度大于0.25时，才会去标注它。值越小，检测标注的东西越多，但可信度就会降低，极容易出现误判。\n\n```python\nparser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n```\n\n有关IOU的知识在另一篇blog中有详细解释。**default=1表示框与框完全重合才能合并，结果中会有多个框出现。default=0表示只要框与框有交集部分就可以合并，故结果中没有重合的框。**\n\n设置默认值default=1，表示只有当检测框与标注框完全重合时才会合并，因此运行后检测的效果会看到许多框框，这种完全重合的情况是很难满足的。在不同框下，一个物体被多次重复检测。如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/3.png)\n\n当默认值default=0时，检测效果如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/IOU=0.png)\n\n```python\nparser.add_argument('--view-img', action='store_true', help='show results')\n```\n\n在运行detect.py时，若加上上行代码参数，则表示在运行此代码时会在检测的同时显示检测的效果。\n\n一般执行时是默认没有的，若需要显示，有两种方法：一种通过命令行，输入如下代码：\n\n```python\nconda activate pytorch\npython detect.py --view-img\n```\n\n我们不喜欢在命令行写指令，那么可以编辑该文件的运行配置，在[形参Parameters]位置上添加参数，如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/4.png)\n\n```python\nparser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n```\n\n保存检测的结果，内容为检测标注的值。\n\n```\nparser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n```\n\n例如设置classes=0并运行代码，则表示之间**人**这个类别。\n\n### 查看detect.py参数的默认值\n\n通过在调试可以看到所有参数的默认值\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/5.png)\n\n\n\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"博客编写发布指南","url":"/2022/03/14/博客编写发布指南/","content":"\n## 使用的软件\n### WebStorm\n用来管理博客相关源代码以及博客的部署和文章的提交\n### Typora\n用于博客写作\nTypora常用快捷键和用法：\n标题：Ctrl+1、2、3...对应一、二、三...级标题（光标定位到需要设置为标题的行，按快捷键）\n加粗：Ctrl+B（选中要加粗的文本，按快捷键）\n斜体：Ctrl+I（选中要设置斜体的文本，按快捷键）\n下划线：Ctrl+U（选中要加下划线的文本，按快捷键）\n删除线：Alt+Shift+5（选中要加删除线的文本，按快捷键）\n代码片段：Ctrl+Shift+`（选中要设置为代码片段的文本，按快捷键）\n代码块：Ctrl+Shift+K（任意位置按快捷键，选择编程语言然后在代码块中输入代码）\n切换到下一行：Ctrl+Enter（任意位置按快捷键，在代码块中可以跳出代码块另起一行）\n链接：Ctrl+K（先复制链接，然后选中要加链接的文本，按快捷键。Ctrl+左键点击文本可跳转到对应链接）\n取消格式：再次按相同的快捷键即可\n有序列表：数字+点+空格\n任务列表：加号或减号+空格 \n切换到列表下一行：Space+Enter\n嵌套列表：按Tab键\n退出列表：按 Shift+Tab\n插入表格：Ctrl+T\n引用：输入>后面加空格，或者Ctrl+Shift+Q\n\n## Hexo文章管理\n### 创建的命令\n\n``` -bash\n$ hexo new <title>\n$ hexo new \"我的第一篇文章\"\n```\n\n\n### 布局\n· 创建md文件时，我们可以指定布局\n``` -bash\n$ hexo new [layout] <title>\n$ hexo new page \"我的页面\"\n```\n· 布局有三种\n    ①post(文章)\n    ②draft(草稿)\n    ③page(页面)\n· 如果没有指定布局类型，则为默认布局post，可以在站点配置文件修改 default_layout 参数来修改默认布局。\n\n## 文章编写格式\n|         写法          |                             解释                             |\n| :-------------------: | :----------------------------------------------------------: |\n|         title         |                       【必需】文章标题                       |\n|         date          |                     【必需】文章创建日期                     |\n|        updated        |                     【可选】文章更新日期                     |\n|         tags          |                       【可选】文章标籤                       |\n|      categories       |                       【可选】文章分类                       |\n|       keywords        |                      【可选】文章关键字                      |\n|      description      |                       【可选】文章描述                       |\n|        top_img        |                     【可选】文章顶部图片                     |\n|         cover         | 【可选】文章缩略图 (如果没有设置 top_img, 文章页顶部将显示缩略图，可设为 false / 图片地址 / 留空) |\n|       comments        |             【可选】显示文章评论模块 (默认 true)             |\n|          toc          |    【可选】显示文章 TOC (默认为设置中 toc 的 enable 配置)    |\n|      toc_number       |  【可选】显示 toc_number (默认为设置中 toc 的 number 配置)   |\n|       copyright       | 【可选】显示文章版权模块 (默认为设置中 post_copyright 的 enable 配置) |\n|   copyright_author    |               【可选】文章版权模块的`文章作者`               |\n| copyright_author_href |             【可选】文章版权模块的`文章作者`链接             |\n|     copyright_url     |             【可选】文章版权模块的`文章连结`链接             |\n|    copyright_info     |             【可选】文章版权模块的`版权声明`文字             |\n|        mathjax        | 【可选】显示 mathjax (当设置 mathjax 的 per_page: false 时，才需要配置，默认 false) |\n|         katex         | 【可选】显示 katex (当设置 katex 的 per_page: false 时，才需要配置，默认 false) |\n|        aplayer        | 【可选】在需要的页面加载 aplayer 的 js 和 css, 请参考文章下面的`音乐` 配置 |\n|   highlight_shrink    | 【可选】配置代码框是否展开 (true/false)(默认为设置中 highlight_shrink 的配置) |\n|         aside         |                【可选】显示侧边栏 (默认 true)                |\n|         hide          |                       【可选】隐藏文章                       |\n|        sticky         |                【可选】文章置顶，值越大越靠上                |\n\n","categories":["生产力"]}]