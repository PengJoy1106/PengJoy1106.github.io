[{"title":"Paper Reading 5|YOLOv7:Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors","url":"/2022/07/28/文献阅读5/","content":"\n# 摘要\n\nYOLOv7在5FPS到160FPS范围内的速度和精度都超过了所有已知的目标检测算法，在GPU V100上具有30FPS的帧率，在所有已知的30FPS以上的实时目标检测器中，YOLOv7的准确率最高，达到56.8% AP。\n\n# Introduction\n\n- 实时目标检测是计算机视觉领域中的一个重要课题，是计算机视觉系统中必不可少的组成部分。在多目标跟踪、自动驾驶、机器人、医学图像分析等领域都有体现。YOLOV7主要希望它能够同时支持移动GPU和从边缘到云端的GPU设备。\n- 近年来，针对不同边缘设备的实时目标检测仍在不断发展。\n  - **MCUNet**和**NanoNet**——专注于生产低功耗单片机和提高边缘CPU上的推理速度；\n  - **YOLOX**和**YOLOR**等——专注于提高各种GPU的 推理速度；\n  - **实时目标检测器**——主要集中在高效体系结构的设计上；\n    - 可以在CPU上使用的实时目标检测器——**MobileNet**，**ShuffleNet**或**GhostNet**\n    - 针对GPU开发的实时目标检测器——**ResNet**、**DarkNet**或**DLA**(Deep Layer Aggregation,一种深度网络特征融合方法，CVPR2018)\n- 本文所提出的方法，其发展方向不同于目前主流的实时目标检测器。除了架构优化之外，该方法是将重点放在训练过程的优化上，即一些优化的模块和优化方法，可能会增强训练成本以提高目标检测的准确性，但不增加推理成本。这种模块和优化方法称为可训练的免费包(bag-of-freebies)。\n- 近年来，模型重新参数化(model re-parameterization)和动态标签分配(dynamic label assignment)已成为网络训练和目标检测中的重要课题。在上述新概念提出后，目标检测器的训练又发展出许多新问题，本论文中作者介绍了一些发现的新问题，并设计了解决这些问题的有效方法。\n\n# 本文的贡献总结：\n\n(1)  我们设计了几种可训练的bag-of-freebies方法，使得实时目标检测可以在不增加推理成本的情况下大大提高检测精度；\n(2)  对于目标检测方法的演进，我们发现了两个新问题，即重新参数化的模块如何替换原始模块，以及动态标签分配策略如何处理分配给不同输出层的问题。此外，我们还提出了解决这些问题所带来的困难的方法；\n(3)  我们提出了实时目标检测器的“扩展”和“复合缩放”方法，可以有效地利用参数和计算；\n(4)  我们提出的方法可以有效减少最先进的实时目标检测器约40%的参数和50%的计算量，并且具有更快的推理速度和更高的检测精度。\n\n# 相关工作\n\n## 实时物体检测器\n\n目前最先进的实时目标检测器主要基于 YOLO 和 FCOS .能够成为最先进的实时目标检测器通常需要以下特性：\n(1)更快更强的网络架构；\n(2) 更有效的特征整合方法；\n(3) 更准确的检测方法;\n(4) 更稳健的损失函数 ；\n(5) 一种更有效的标签分配方法 ；\n(6) 更有效的训练方法。\n在本文中，将针对与上述 (4)、(5) 和 (6) 相关的最先进方法衍生的问题设计新的可训练免费赠品方法。\n\n## 模型重新参数化\n\n模型重新参数化是在推理阶段将多个计算模块合并为一个。有两种方法：\n\n- 模块级集成：模块级重新参数化是最近比较热门的研究问题。这种方法在训练时将一个模块拆分为多个相同或不同的模块分支，在推理时将多个分支模块整合为一个完全等效的模块。\n\n- 模型级集成：模型级重新参数化有两种做法来获得最终的推理模型。一种是用不同的训练数据训练多个相同的模型，然后对多个训练模型的权重进行平均。另一种是对不同迭代次数的模型权重进行加权平均。\n\n## 模型缩放\n\n模型缩放是一种放大或缩小已设计模型并使其适合不同计算设备的方法。模型缩放方法通常使用不同的缩放因子，例如分辨率（输入图像的大小）、深度（层数）、宽度（通道数）和阶段（特征金字塔的数量），从而在网络参数量、计算量、推理速度和准确率之间取得良好的折衷。\n\nNAS(Network architecture search，网络架构搜索)是一种常用的模型扩展方法。\n\n- 优点：从搜索空间中自动搜索合适的缩放因子，而不需要定义太复杂的规则；\n- 缺点：需要非常昂贵的计算来完成模型缩放因子的搜索。\n\n通过查阅文献发现几乎所有的模型缩放方法都是独立分析单个缩放因子的，甚至复合缩放类别中的方法也是独立优化缩放因子的。这是因为大多数流行的NAS架构都处理不太相关的伸缩因子。\n\n# 网络结构\n\n## 扩展的高效层聚合网络\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220728162225.png)\n\n在大多数关于高效建筑结构设计的文献中，主要考虑的因素在于参数的数量、计算量和计算密度，有些还涉及到内存访问代价等，分析**输入/输出信道比、体系结构分支数量和单元操作对网络推理速度的影响**。上图(b)中的CSPVoVNet设计时VoVNet的变体，CSPVoVNet的架构除了考虑上述的基本设计问题外，还对梯度路径进行了分析，使不同层的权值能够学习到更多样化的特征，这样的梯度分析方法使推断更快、更准确。上图(c)中的ELAN考虑了以下设计策略——**通过控制最短最长梯度路径，深度网络可以有效学习和收敛**。在本文中，作者提出了基于ELAN的Extended-ELAN(E-ELAN)，其主要架构如上图(d)所示。\n\n## Partial -> Partial Convolutional (部分卷积)\n\n部分卷积的概念由英伟达在ECCV2018发表的论文《Image Inpainting for Irregular Holes Using Partial Convolutions》中提出，最初用于图像修复。\n\n哪天想起来了爷再看。\n\n## 基于串联的模型的模型缩放\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220728171808.png)\n\n模型缩放的主要目的是调整模型的一些属性，生成不同尺度的模型，以满足不同推理速度的需要。例如，EfficientNet的缩放模型考虑了宽度、深度和分辨率；scaled-YOLOv4的缩放模型是调整阶段数。上述方法主要应用于PlainNet、ResNet等架构中，当这些架构在执行放大或缩小时，每一层的入度和出度不会发生变化，因此我们可以独立分析每个缩放因子对参数量和计算量的影响。然而，如果将这些方法应用到基于连接的体系结构中，我们将发现当对深度进行放大或缩小时，位于基于连接的计算块之后的转换层的程度将减少或增加，如上图的 (a)和(b)所示。\n\n从上述现象可以推断，对于基于串联的模型，我们不能单独分析不同的缩放因子，而必须综合考虑。以scaling-up depth为例，这样的动作会导致transition layer的输入通道和输出通道的比例发生变化，这可能会导致模型的硬件使用率下降。因此，我们必须为基于级联的模型提出相应的复合模型缩放方法。当我们缩放计算块的深度因子时，我们还必须计算该块的输出通道的变化。然后，我们将对过渡层进行等量变化的宽度因子缩放，结果如上图（c）所示。我们提出的复合缩放方法可以保持模型在初始设计时的特性并保持最佳结构。\n\n# 可训练的bag-of-freebies\n\n## 重新参数化卷积\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220729142928.png)\n\n- **问题**：虽然RepConv在VGG上已经取得了优异的性能，但将其应用在ResNet和DenseNet等架构时，其精度会显著降低；\n\n- **原因**：RepConv实际上是在一个卷积层中结合了3×3卷积、1×1卷积和恒等连接。但 RepConv 中的恒等连接破坏了 ResNet 中的残差和 DenseNet 中的连接，因此文中提出了RepConvN；\n\n- **解决方法**：用梯度流传播路径来分析重新参数化卷积如何与不同的网络相结合，并据此设计了规划的重新参数化卷积，即RepConvN。\n\n- **具体描述**：当一个带有残差或连接的卷积层被重新参数化的卷积取代时，应该没有恒等连接。\n\n上图显示了在PlainNet和ResNet中使用的“Planned re-parameterized convolution”的示例。\n\n## 标签分配（粗为辅助，细为主要损失）\n\n*深度监督是一种常用的深度网络训练技术。其主要思想是在深度神经网络的某些中间隐藏层加了一个辅助的分类器作为一种网络分支来对主干网络进行监督。*\n\nYOLOv7将负责最终输出的头部称为**引导头**(lead head)，辅助训练的头部称为 **辅助头**(auxiliary head).\n\n- 问题：\n\n  - 过去：标签分配通常直接指向ground truth，根据给定的规则生成硬标签；\n\n  - 近年：以目标检测为例，利用网络预测输出的质量和分布，再结合真实信息考虑，使用一些计算和优化方法生成可靠的软标签。如YOLO利用bounding box回归预测IoU和ground truth作为软标签；\n\n  - 目前：针对“如何将软标签分配给辅助头和引导头”的问题还没有相关文献进行探讨；\n\n本文作者将网络预测结果和地面真实值一起考虑，然后分配软标签的机制称为**标签分类器**\n\n- 解决方法：\n  - 流行方法：是将辅助头和引导头分开，利用各自的预测结果和ground truth执行标签分配。\n  - 本文方法：通过引线头预测同时引导辅助引线头和引线头的标签分配新方法。也就是说，我们以导头预测为指导，生成粗到细的层次标签，分别用于辅助导头学习和导头学习。\n\n这两种深度监管标签分配策略如下图所示：\n\n![](https://blog-1310087999.cos.ap-nanjing.myqcloud.com/20220729151737.png)\n\n1. 导头引导标签分配器：根据根据引导头的预测结果和ground truth进行计算，通过优化过程生成软标签。这套标签将作为辅助头和导头的目标获取训练模型。*这样做的原因是lead head具有较强的学习能力*，因此由它生成的软标签应该更能代表源数据与目标数据之间的分布和相关性。*此外，可以把这种学习看作一种广义剩余学习，通过让较浅的辅助头直接学习引导头已经学习过的信息，引导头将更能专注于学习尚未学习到的剩余信息*。\n2. 粗到细导头引导标签分配器：利用引导头的预测结果和ground truth生成软标签，**但是**，在这个过程中生成了两组不同的软标签，即粗标签和细标签，其中细标签与引导头引导标签分配器生成的软标签相同，而粗标签是通过降低正样本分配过程的约束没让更多的网络作为正目标来生成的。*这是因为辅助头的学习能力不如导头强*，为了避免丢失学习的信息，重点将优化辅助头在对象检测任务中的回忆。对于引导头的输出，可以从高查全率的结果中过滤出高精度的结果作为最终输出。\n\n*ground truth：基准真相，是一个相对概念。它是指相对于新的**测量**方式得到的测量值，作为基准的，由已有的、可靠的测量方式得到的测量值（即**经验证据**）。人们往往会利用基准真相，对新的测量方式进行**校准**，以降低新测量方式的误差和提高新测量方式的准确性。机器学习领域借用了这一概念。使用训练所得模型对样本进行推理的过程，可以当做是一种广义上的测量行为。因此，在**有监督学习**中，ground truth 通常指代样本集中的标签。可以理解为**真实信息***。\n\n\n\n## 其他可训练的bag-of-freebies\n\n1. 批处理归一化：将批处理归一化直接连接到卷积层；\n2. YOLOR中的隐形知识结合卷积特征映射的加法和乘法方式：通过推理阶段的预计算，将YOLOR中的隐性知识简化为向量，该向量可以与前一层或后一层的偏差和权重相结合；\n3. EMA模型：EMA是mean teacher中使用的一种技术，在本文中使用EMA模型作为最终推断模型。\n","tags":["Object Detection","YOLOv7"],"categories":["文献阅读"]},{"title":"数学建模学习笔记5|数据包络分析方法","url":"/2022/04/22/数学建模学习笔记5/","content":"\n# 数据包络分析方法\n\n数据包络分析方法（Data Envelopment Analysis，DEA）是运筹学、管理科学与数理经济学交叉研究的一个新领域。它是根据多项投入指标和多项产出指标，利用线性规划的方法，对具有可比性的同类型单位进行相对有效性评价的一种数量分析方法。DEA 方法及其模型已广泛应用于不同行业及部门，并且在处理多指标投入和多指标产出方面，体现了其得天独厚的优势。\n\n## DEA 模型\n\n开发出一种技术，通过明确地考虑多种投入的运用和多种产出的产生，它能够用来比较提供相似服务的多个服务单位之间的效率，这项技术被称为数据包络线分析（DEA）。它避开了计算每项服务的标准成本，因为它可以把多种投入和多种产出转化为效率比率的分子和分母，而不需要转换成相同的货币单位。因此，用 DEA 衡量效率可以清晰地说明投入和产出的组合，从而，它比一套经营比率或利润指标更具有综合性并且更值得信赖。\n\nDEA 是一个线形规划模型，表示为产出对投入的比率。通过对一个特定单位的效率和一组提供相同服务的类似单位的绩效的比较，它试图使服务单位的效率最大化。在这个过程中，获得 100% 效率的一些单位被称为相对有效率单位，而另外的效率评分低于 100% 的单位被称为无效率单位。\n\n这样，企业管理者就能运用 DEA 来比较一组服务单位，识别相对无效率单位，衡量无效率的严重性，并通过对无效率和有效率单位的比较，发现降低无效率的方法。\n\n## DEA 模型建立\n\n#### 1) 定义变量\n\n设 $Ek(k=1，2，……， K)$ 为第 $k$ 个单位的效率比率，这里 $K$ 代表评估单位的总数。\n\n设 $uj(j=1，2，……， M)$ 为第 $j$ 种产出的系数，这里 $M$ 代表所考虑的产出种类的总数。变量 $uj$ 用来衡量产出价值降低一个单位所带来的相对的效率下降。\n\n设 $vI(I=1，2，……，N)$ 为第 $I$ 种投入的系数，这里 $N$ 代表所考虑的投入种类的综合素。变量 $vI$ 用来衡量投入价值降低一个单位带来的相对的效率下降。\n\n设 $Ojk$ 为一定时期内由第 $k$ 个服务单位所创造的第 $j$ 种产出的观察到的单位的数量。\n\n设 $Iik$ 为一定时期内由第 $k$ 个服务单位所使用的第 $i$ 种投入的实际的单位的数量。\n\n#### 2) 目标函数\n\n　　目标是找出一组伴随每种产出的系数 $u$ 和一组伴随每种投入的系数 $ν$，从而给被评估的服务单位最高的可能效率。 $$ \\max E_{e}=\\frac{u_{1} O_{i e}+u_{2} O_{2 e}+\\ldots \\ldots+u_{M} O_{M e}}{v_{1} I_{1 e}+v_{2} I_{2 e}+\\ldots \\ldots+v_{N} I_{N e}}\\tag{*} $$ 式中，$e$ 是被评估单位的代码。 这个函数满足这样一个约束条件，当同一组投入和产出的系数（$uj$ 和 $vi$）用于所有其他对比服务单位时，没有一个服务单位将超过 100% 的效率或超过 1.0 的比率。\n\n#### 3) 约束条件\n\n$$ \\frac{u_{1} O_{1k}+u_{2} O_{2 k}+\\ldots \\ldots+u_{M} O_{M k}}{v_{1} I_{1 k}+v_{2} I_{2 k}+\\ldots \\ldots+v_{N} I_{N k}} \\leq 1.0\\quad,　k=1,2,...,K\\tag{**} $$\n\n式中所有系数值都是正的且非零。\n\n为了用标准线性规划软件求解这个有分数的线性规划，需要进行变形。要注意，目标函数和所有约束条件都是比率而不是线性函数。通过把所评估单位的投入人为地调整为总和 1.0，这样等式（*）的目标函数可以重新表述为： $$ \\max E_{e}=u_{1} O_{1 e}+u_{2} O_{2 e}+\\ldots+u_{M} O_{M e} $$ 满足以下约束条件： $$ v_{1} I_{1 e}+v_{2} I_{2 e}+\\ldots+v_{N} I_{N e}=1 $$ 对于个服务单位，等式（**）的约束条件可类似转化为： $$ u_{1} O_{1 k}+u_{2} O_{2 k}+\\ldots+u_{M} O_{M k}-\\left(v_{1} I_{1 k}+v_{2} I_{2 k}+\\ldots+v_{N} I_{N k}\\right) \\leq 0\\quad,\\quad k=1,2,…,K $$ 式中 $u_j≥0 ,j=1,2,...,M$，$v_i≥0 ,i=1,2,...,N$\n\n　　关于服务单位的样本数量问题是由在分析种比较所挑选的投入和产出变量的数量所决定的。下列关系式把分析中所使用的服务单位数量 $K$ 和所考虑的投入种类数 $N$ 与产出种类数 $M$ 联系出来，它是基于实证发现和 DEA 实践的经验： $$ K\\geq2(N+M) $$\n","tags":["matlab"],"categories":["数学建模"]},{"title":"Paper Reading 4|基于注意力机制特征增强的舰船目标识别","url":"/2022/04/21/文献阅读4/","content":"\n# 概述\n\n本文中作者提出了基于注意力机制的特征增强架构(Feature enhancement architecture based on attention mechanism，FBAM)，在这个架构中，作者改进了两个模块：\n\n- 顶层特征增强模块(Top-level feature enhancement，TLFE)\n  通过融合通道注意力和空间注意力，为舰船识别提供丰富的语义信息和位置信息。\n- 自适应ROI特征增强(Adap-tive ROIfeature enhancement，ROIFE)\n  网络自适应组合多层次的ROI特征信息，增强舰船识别的细粒度级别特征，提高舰船识别的定位能力。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421135113.png)\n\n顶层特征增强模块是一个通道和空间信息双重注意力网络，最顶层的特征层{C5}经过通道注意力和空间注意力之后进行融合为一个新的特征层，以此保证顶层特征层的特征信息更完整的得到保留，将得到的新特征层与后续的特征层再进行融合。自适应ROI特征增强模块为每一个ROI汇集所有特征金字塔层的特征，从特征融合之后的特征金字塔{P2，P3，P4，P5}中的每一层学习生成更好的ROI特征，ROIFE为不同层的ROI特征生成不同的空间权重，将ＲOI 特征加权相融合\n\n# 改进方法\n\n以深度学习目标检测算法作为基础网络，对网络中特征融合部分 FPN 进行改进。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421141539.png)\n\n即骨干网络提取特征后送入FPN进行特征融合，在特征融合的**过程**中，设计增加TLFE模块，在特征融合**之后**，设计增加ROIFE模块，将不同层的特征求和作为这个ROI最终的特征。\n\n改进示意图如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421140317.png)\n\n这两种改进都是针对特征金字塔FPN部分的改进，第一个是**将{C5}通过并联的通道注意力和空间注意力模块**，赋予其更多的语义和空间信息；第二个是在特征融合后，对于任意一个ROI预测，**提取出该ROI在{P2,P3,P4,P5}上的所有对应的特征，然后利用网络本身学习权重参数，将不同层的特征求和**作为这个ROI最终的特征。\n\n## 顶层特征增强模块\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421142432.png)\n\n其中上半部分是空间注意力机制，通过一个卷积Conv+一个激活函数Sigmoid；下半部分先经过全局平均池化，获取全局感受野，再通过ReLU和Sigmoid激活函数；然后将这两部分原始的C5与通道权重值相乘，得到的两个有关注度的新特征层之后，将空间关注度与通道关注度的特征图相融合构成新的特征层。\n\n> 全局平均池化：编码了全局的统计信息。从空间的角度来看，通道注意力是全局的，而空间注意力是局部的。通道注意力顺着通道维度对C5进行全局平均池化压缩，获取全局感受野，经过Sigmoid非线性处理，将输出结果作为每个通道的权重值。\n\n## 自适应ROI特征增强\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220421143742.png)\n\n4层特征信息进行concat操作 –> 全局最大池化 –> 两次卷积 –> 激活函数\n\n- 全局最大池化：保留通道信息\n- 第一次1×1卷积：缩放通道值\n- 第二次1×1卷积：恢复通道信息值\n\n# 验证实验\n\n## 网络\n\n以**faster R-CNN**作为基础算法，以**resnet50**和**resnet101**这两种网络作为骨干网络\n\n## 数据集\n\n数据集选用HRSC2016数据集。\n\n## 评价指标\n\n精准召回曲线(PRC)和平均精确度(AP)\n\n## 对比试验\n\n实 验 利 用 **FasterＲ-CNN**、**Cascade Ｒ-CNN**、**RetinaNet** 3 种算法，以及**Resnet50** 和**Reanext101** 2 种骨干网络验证FBAM 的有效性。","tags":["CNN","FPN","attention mechanism"],"categories":["文献阅读"]},{"title":"数学建模学习笔记4|题型总结","url":"/2022/04/19/数学建模学习笔记4/","content":"\n# 2016 E题\n\n## 题目\n\n**粮食最低收购价政策问题研究**\n\n粮食，不仅是人们日常生活的必需食品，而且还是维护国家经济发展和政治稳定的战略物资，具有不可替代的特性。由于耕地减少、人口增加、水资源短缺、气候变化等问题日益凸显，加之国际粮食市场的冲击，我国粮食产业面临着潜在的风险。因此，研究我国的粮食保护政策具有十分重要的作用和意义。\n\n一般而言，粮食保护政策体系主要由三大支持政策组成：粮食生产支持政策、粮食价格支持政策和收入支持政策。粮食最低收购价政策就属于粮食价格支持政策范畴。\n\n一般情况下，我国粮食收购价格由市场供需情况决定，国家在充分发挥市场机制作用的基础上实行宏观调控。为保护农民利益、保障粮食市场供应，国家对重点粮食品种，在粮食主产区实行最低收购价格政策，并每年事先公布重点粮食品种的最低收购价。在最低收购价格政策执行期（粮食收获期，一般在2-5个月）内，当市场粮食实际收购价低于国家确定的最低收购价时，国家委托符合一定资质条件的粮食企业，按国家确定的最低收购价格收购农民种植的粮食，以保护粮农的种植积极性。\n\n我国自2005年起开始对粮食主产区实行了最低收购价政策，并连续多年上调最低收购价价格。2016年国家发展与改革委员会公布的小麦（三等）最低收购价格为每50公斤118元，比首次实施小麦最低收购价的2006年提高了66.2%；早籼稻（三等）、中晚籼稻（三等）和粳稻（三等）最低收购价格分别为每50公斤133元、138元和155元，分别比首次实施水稻最低收购价的2005年提高了84.72%、91.67%和106.67%。显而易见，粮食最低收购价政策已经成为了国家保护粮食生产的最为重要的举措之一。\n\n然而，也有学者不认同这项最低收购价政策。他们认为，粮食的实际收购价格（以后称为粮食市场收购价）应该由粮食供需双方通过市场调节来决定。粮食最低收购价政策作为一种粮食种植保护政策，扭曲了粮食市场的供需行为，即该政策的实施很有可能抬高了市场收购价格，导致粮食企业承担了很大的经营风险。\n\n对于粮食最低收购价政策实施效果的评价，学者们也是见解不一。部分地区某些粮食品种种植面积、粮食总产量不增反降，导致部分学者质疑粮食最低收购价政策的效果；但也有学者高度肯定了粮食最低收购价政策，认为如果不实施粮食最低收购价政策，这些地区某些粮食品种的种植面积可能会下降得更快，因而认为粮食最低收购价政策在稳定或增加粮食种植面积方面是有着积极的作用。\n\n一般来讲，粮食的种植面积是决定粮食供给的关键因素，也是保障粮食安全的重要前提。衡量粮食最低收购价政策实施的效果，主要是比较政策实施前后粮食种植面积是否有显著性变化。然而，可能影响粮食种植面积的因素有很多，除了粮食最低收购价政策外，还可能有其他很多的影响因素，如农业劳动力人口、粮食进出口贸易、农民受教育程度、城乡收入差距、家庭负担等。因此，要研究粮食最低收购价政策的实施效果，不能仅仅根据种植面积的变化来评定。\n\n与此同时，也有一些学者就粮食最低收购价制定的合理范围进行了探讨。最低收购价并不是实际的市场收购价格，而是一种心理安慰价，是收购粮食的底价。粮农决定是否种植粮食，取决于很多因素，但最主要的还是看种植粮食所获得的纯收益的大小。粮食最低收购价的公布，使得粮农能清楚地算出这笔经济账。因此粮食最低收购价的高低直接影响着当年的粮食生产。中国是一个“以粮为纲”的国家，存储的粮食一般要能够满足全国人民三年的吃饭和需求。同时国家对于粮食的补贴金额也是有限制的，在保持合理库存的前提下，一般不会超出各地粮食市场价格的10%。因此，过高的粮食最低收购价不仅会提高粮食市场价格从而加重消费者负担，同时也会增加粮食的库存压力和国家财政的支出风险。另一方面，过低的粮食最低收购价会打压粮农种植粮食的积极性，造成粮食种植面积的萎缩，这更不是国家所愿意看到的。\n\n请你们查阅相关资料和数据，结合数据特点，回答下列问题：\n\n1. 影响粮食种植面积的因素比较多，它们之间的关系错综复杂而且可能存在着粮食品种和区域差异。请你们建立影响粮食种植面积的指标体系和关于粮食种植面积的数学模型，讨论、评价指标体系的合理性，研究他们之间的关系，并对得出的相应结果的可信度和可靠性给出检验和分析。\n2. 对粮食最低收购价政策的作用，学者们褒贬不一。请你们建立粮食最低收购价政策执行效果的评价模型。并运用你们所建立的评价模型，结合粮食品种和区域差异，选择几个省份比较研究粮食主产区粮食最低收购价执行的效果。\n3. 粮食市场收购价是粮食企业收购粮食的市场价格，是由粮食供需双方通过市场调节来决定。它与粮食最低收购价一起构成粮食价格体系，是宏观价格调控系统中有一定相对独立性的重要措施。请你们运用数据分析或建立数学模型探讨我国粮食价格所具有的特殊规律性。\n4. 结合前面的研究和国家制定粮食最低收购价政策的初衷，请你们建立粮食最低收购价的合理定价模型，进而对“十二五”期间国家发展与改革委员会公布的粮食最低收购价价格的合理性做出评价，并运用你们所建立的模型对2017年的粮食最低收购价的合理范围进行预测。\n5. 与2000年相比，2015年我国小麦种植面积略有下降。如果国家想让小麦种植面积增加5%，通过调整粮食最低收购价是否能够达到这一目的？请说明理由。\n6. 根据你们的研究结论，请提出调控粮食种植的优化决策和建议。\n\n## 分析\n\n1. 影响粮食种植面积的因素比较多，它们之间的关系错综复杂而且可能存在着粮食品种和区域差异。请你们建立影响粮食种植面积的指标体系和关于粮食种植面积的数学模型，讨论、评价指标体系的合理性，研究他们之间的关系，并对得出的相应结果的可信度和可靠性给出检验和分析。\n\n   - Spearman 相关检验, 主成分回归模型\n\n   - 非参数 Spearman 相关检验法，偏最小二乘回归\n\n   - 结构方程模型，证性因子分析和路径分析\n\n   - Kolmogorov-Smirnov，相关性检验和主成分因子分析，Granger 因果关系检验，Granger 因果关系检验，似然比检验法，单位根检验和协整检验法\n\n   - 多元线性回归模型，相关系数、残差和显著性水平\n2. 对粮食最低收购价政策的作用，学者们褒贬不一。请你们建立粮食最低收购价政策执行效果的评价模型并运用你们所建立的评价模型，结合粮食品种和区域差异，选择几个省份比较研究粮食主产区粮食最低收购价执行的效果。\n\n   - 主成分分析，混合线性模型\n   - 三角模糊数\n   - 最低收购价政策执行效果综合评价指数模型，基于粒子群和投影寻踪算法的权重确定模型\n   - 主成分分析法\n3. 粮食市场收购价是粮食企业收购粮食的市场价格，是由粮食供需双方通过市场调节来决定。它与粮食最低收购价一起构成粮食价格体系，是宏观价格调控系统中有一定相对独立性的重要措施。请你们运用数据分析或建立数学模型探讨我国粮食价格所具有的特殊规律性\n\n   - 市场收购价理论和局部调整模型，包括 供应量模型、企业收购量模型和市场收购价格模型\n   - 基于供需理论构建粮食供需及价格联动模型， ARIMA 模型\n   - “蛛网”模型， ARCH 类模型\n   - 局部均衡模型和正反馈系统\n4. 结合前面的研究和国家制定粮食最低收购价政策的初衷，请你们建立粮食最低收购价的合理定价模型，进而对“十二五”期间国家发展与改革委员会公布的粮食最低收购价价格的合理性做出评价，并运用你们所建立的模型对2017年的粮食最低收购价的合理范围进行预测。\n\n   - 以粮食产量为目标模型，以价格波动、财政支出、库存和种植面积为约束条件建立粮食最低收购价合理定价的线性规划模型\n\n   - 优化模型\n   - GARCH 模型、单变量二阶差分方程模型(DDE)、支持向量机预测模型(SVM模型)以及马尔科夫链的时变权组合预测模型(HM-TWA)\n   - 基于正态分布随机数遗传，多目标合理定价模型，基于有序加权平均(OWA) 算子\n\n## 优秀论文\n","tags":["matlab"],"categories":["数学建模"]},{"title":"数学建模学习笔记3|预测模型","url":"/2022/04/18/数学建模学习笔记3/","content":"\n距离五一数学建模竞赛还有15天。\n\n# 灰色预测\n\n灰色预测是对既含有已知信息又含有不确定信息的系统进行预测，就是对在一定范围内变化的、与时间有关的灰色过程进行预测。灰色预测对原始数据进行生成处理来寻找系统变动的规律，并生成有较强规律性的数据序列，然后建立相应的微分方程模型，从而预测事物未来发展趋势的状况。\n\n# GM(1,1)模型：Grey(Gray)Model\n\nGM(1,1)是使用原始的离散非负数据列，通过一次累加生成削弱随机性的较有规律的新的离散数据列，然后通过建立微分方程模型，得到在离散点处的解经过累减生成的原始数据的近似估计值，从而预测原始数据的后续发展。\n\n## GM(1,1)原理介绍\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418213714.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418213802.png)\n\n## 一阶微分方程\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418214034.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418214258.png)\n\n## 准指数规律的检验\n\n![image-20220418214358823](C:/Users/Pengyk/AppData/Roaming/Typora/typora-user-images/image-20220418214358823.png)\n\n## GM(1,1)模型的评价与检验\n\n![image-20220418214529891](C:/Users/Pengyk/AppData/Roaming/Typora/typora-user-images/image-20220418214529891.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418215756.png)\n\n# 灰色预测的应用场景\n\n1. 数据是以年份度量的非负数据（如果是月份或者季度数据一定要用我们上一讲学过的时间序列模型）；\n2. 数据能经过准指数规律的检验（除了前两期外，后面至少90%的期数的光滑比要低于0.5）；\n3. 数据的期数较短且和其他数据之间的关联性不强（小于等于10，也不能太短了，比如只有3期数据），要是数据期数较长，一般用传统的时间序列模型比较合适。\n\n# 预测题小策略\n\n1. 看到数据后先画时间序列图并简单的分析下趋势（例如：我们上一讲学过的时间序列分解）；\n2. 将数据分为训练组和试验组，尝试使用不同的模型对训练组进行建模，并利用试验组的数据判断哪种模型的预测效果最好（比如我们可以使用SSE这个指标来挑选模型，常见的模型有指数平滑、ARIMA、灰色预测、神经网络等）；\n3. 选择上一步骤中得到的预测误差最小的那个模型，并利用全部数据来重新建模，并对未来的数据进行预测；\n4. 画出预测后的数据和原来数据的时序图，看看预测的未来趋势是否合理。\n\n# 灰色预测例题\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220418220207.png)\n\n# 例题对应代码讲解\n\n1. 画出原始数据的时间序列图，并判断原始数据中是否有负数或期数是否低于4期，如果是的话则报错，否则执行下一步；\n\n2. 对一次累加后的数据进行准指数规律检验，返回两个指标：指标1：光滑比小于0.5的数据占比（一般要大于60%）指标2：除去前两个时期外，光滑比小于0.5的数据占比（一般大于90%）并让用户决定数据是否满足准指数规律，满足则输入1，不满足则输入0\n\n3. 如果上一步用户输入0，则程序停止；如果输入1，则继续下面的步骤。\n\n4. 让用户输入需要预测的后续期数，并判断原始数据的期数：\n\n  - 数据期数为4：分别计算出传统的GM(1,1)模型、新信息GM(1,1)模型和新陈代谢GM(1,1)模型对于未来期数的预测结果，为了保证结果的稳健性，对三个结果求平均值作为预测值。\n\n  - 数据期数为5,6或7：取最后两期为试验组，前面的n-2期为训练组；用训练组的数据分别训练三种GM模型，并将训练出来的模型分别用于预测试验组的两期数据；利用试验组两期的真实数据和预测出来的两期数据，可分别计算出三个模型的SSE；选择SSE最小的模型作为我们建模的模型。\n\n  - 数据期数大于7：取最后三期为试验组，其他的过程和4.2类似。\n\n5. 输出并绘制图形显示预测结果，并进行残差检验和级比偏差检验。\n\n# 灰色预测运行结果\n","tags":["matlab"],"categories":["数学建模"]},{"title":"Yolov5-v6.0学习笔记10|val.py代码详解","url":"/2022/04/18/Yolov5学习笔记10/","content":"# val.py简介\n\nval.py文件主要是在每一轮的训练结束后，验证当面模型的mAP、混淆矩阵等指标。\n\n> - mAP：英文全称为 Mean Average Precision，作为目标检测中的平均精度\n>\n>   AP：(平均精度)是衡量目标检测算法好坏的常用指标，在Faster R-CNN，SSD等算法中作为评估指标。\n>\n>   AP等于recall值取0-1时，precision值的平均值\n>\n> - 混淆矩阵：也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类相比较计算的。\n\n实际上这个脚本最常用的应该是通过train.py调用run函数，而不是通过执行val.py的。所以在这个脚本中，最重要的就是run函数。\n\n# opt参数\n\n```python\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, default=ROOT / 'data/ship.yaml', help='dataset.yaml path')\n    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp4/weights/best.pt', help='model.pt path(s)')\n    parser.add_argument('--batch-size', type=int, default=2, help='batch size')\n    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')\n    parser.add_argument('--task', default='val', help='train, val, test, speed or study')\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\n    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\n    parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')\n    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n    parser.add_argument('--save-json', action='store_true', help='save a COCO-JSON results file')\n    parser.add_argument('--project', default=ROOT / 'runs/val', help='save to project/name')\n    parser.add_argument('--name', default='exp', help='save to project/name')\n    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n    opt = parser.parse_args()\n    opt.data = check_yaml(opt.data)  # check YAML\n    opt.save_json |= opt.data.endswith('coco.yaml')\n    opt.save_txt |= opt.save_hybrid\n    print_args(FILE.stem, opt)\n    return opt\n```\n\nopt参数详解：\n\n- opt参数详解 \n\n- data: 数据集配置文件地址 包含数据集的路径、类别个数、类名、下载地址等信息\n\n- weights: 模型的权重文件地址 weights/yolov5s.pt\n\n- batch_size: 前向传播的批次大小 默认32\n\n- imgsz: 输入网络的图片分辨率 默认640\n\n- conf-thres: object置信度阈值 默认0.25\n\n- iou-thres: 进行NMS时IOU的阈值 默认0.6\n\n- task: 设置测试的类型 有train, val, test, speed or study几种 默认val\n\n- device: 测试的设备\n\n- single-cls: 数据集是否只用一个类别 默认False\n\n- augment: 测试是否使用TTA Test Time Augment 默认False\n\n- verbose: 是否打印出每个类别的mAP 默认False\n\n- 下面三个参数是auto-labelling(有点像RNN中的teaching forcing)\n\n  save-txt: traditional auto-labelling\n\n  save-hybrid: save hybrid autolabels, combining existing labels with new predictions before NMS (existing predictions given confidence=1.0 before NMS. \n\n  save-conf: add confidences to any of the above commands\n\n- save-json: 是否按照coco的json格式保存预测框，并且使用cocoapi做评估（需要同样coco的json格式的标签） 默认False\n\n- project: 测试保存的源文件 默认runs/test \n\n- name: 测试保存的文件地址 默认exp  保存在runs/test/exp下\n\n- exist-ok: 是否存在当前文件 默认False 一般是 no exist-ok 连用  所以一般都要重新创建文件夹\n\n- half: 是否使用半精度推理 默认False\n\n# main()函数\n\n```python\ndef main(opt):\n    global x\n    check_requirements(requirements=ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n\n    if opt.task in ('train', 'val', 'test'):  # run normally\n        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466\n            LOGGER.info(f'WARNING: confidence threshold {opt.conf_thres} >> 0.001 will produce invalid mAP values.')\n        run(**vars(opt))\n\n    else:\n        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]\n        opt.half = True  # FP16 for fastest results\n        if opt.task == 'speed':  # speed benchmarks\n            # python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...\n            opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False\n            for opt.weights in weights:\n                run(**vars(opt), plots=False)\n\n        elif opt.task == 'study':  # speed vs mAP benchmarks\n            # python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n.pt yolov5s.pt...\n            for opt.weights in weights:\n                f = f'study_{Path(opt.data).stem}_{Path(opt.weights).stem}.txt'  # filename to save to\n                x, y = list(range(256, 1536 + 128, 128)), []  # x axis (image sizes), y axis\n                for opt.imgsz in x:  # img-size\n                    LOGGER.info(f'\\nRunning {f} --imgsz {opt.imgsz}...')\n                    r, _, t = run(**vars(opt), plots=False)\n                    y.append(r + t)  # results and times\n                np.savetxt(f, y, fmt='%10.4g')  # save\n            os.system('zip -r study.zip study_*.txt')\n            plot_val_study(x=x)  # plot\n```\n\n在这个模块中，根据opt.task分为三个分支，即[train, val, test]、[speed]、[study]，最主要的分支还是在\n\n```python\nopt.task in ('train', 'val', 'test')\n```\n\n这段代码的意思是如果task in ['train', 'val', 'test']就正常测试 训练集/验证集/测试集。\n一般直接进入第一个分支，执行run()函数。\n\n# run()函数\n\n```python\nif RANK in [-1, 0]:\n  # mAP\n  callbacks.run('on_train_epoch_end', epoch=epoch)\n  ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\n  final_epoch = (epoch + 1 == epochs) or stopper.possible_stop\n  if not noval or final_epoch:  # Calculate mAP\n      results, maps, _ = val.run(data_dict,\n                                 batch_size=batch_size // WORLD_SIZE * 2,\n                                 imgsz=imgsz,\n                                 model=ema.ema,\n                                 single_cls=single_cls,\n                                 dataloader=val_loader,\n                                 save_dir=save_dir,\n                                 plots=False,\n                                 callbacks=callbacks,\n                                 compute_loss=compute_loss)\n```\n\nrun()函数在train.py中执行，用来在每个epoch后验证当前模型。\n\n## 载入参数\n\n```python\ndef run(data,\n        weights=None,  # model.pt path(s)\n        batch_size=32,  # batch size\n        imgsz=640,  # inference size (pixels)\n        conf_thres=0.001,  # confidence threshold\n        iou_thres=0.6,  # NMS IoU threshold\n        task='val',  # train, val, test, speed or study\n        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        workers=8,  # max dataloader workers (per RANK in DDP mode)\n        single_cls=False,  # treat as single-class dataset\n        augment=False,  # augmented inference\n        verbose=False,  # verbose output\n        save_txt=False,  # save results to *.txt\n        save_hybrid=False,  # save label+prediction hybrid results to *.txt\n        save_conf=False,  # save confidences in --save-txt labels\n        save_json=False,  # save a COCO-JSON results file\n        project=ROOT / 'runs/val',  # save to project/name\n        name='exp',  # save to project/name\n        exist_ok=False,  # existing project/name ok, do not increment\n        half=True,  # use FP16 half-precision inference\n        dnn=False,  # use OpenCV DNN for ONNX inference\n        model=None,\n        dataloader=None,\n        save_dir=Path(''),\n        plots=True,\n        callbacks=Callbacks(),\n        compute_loss=None,\n        ):\n```\n\n参数解释：\n\n- data: 数据集配置文件地址–包含数据集的路径、类别个数、类名、下载地址等信息 train.py时传入data_dict\n- weights: 模型的权重文件地址 运行train.py=None 运行test.py=默认weights/yolov5s.pt\n- batch_size: 前向传播的批次大小 运行test.py传入默认32 运行train.py则传入batch_size // WORLD_SIZE * 2\n- imgsz: 输入网络的图片分辨率 运行test.py传入默认640 运行train.py则传入imgsz_test\n- conf_thres: object置信度阈值 默认0.25\n- iou_thres: 进行NMS时IOU的阈值 默认0.6\n- task: 设置测试的类型 有train, val, test, speed or study几种 默认val\n- device: 测试的设备\n- single_cls: 数据集是否只用一个类别 运行test.py传入默认False 运行train.py则传入single_cls\n- augment: 测试是否使用TTA Test Time Augment 默认False\n- verbose: 是否打印出每个类别的mAP 运行test.py传入默认Fasle 运行train.py则传入nc < 50 and final_epoch\n- save_txt: 是否以txt文件的形式保存模型预测框的坐标 默认True\n- save_hybrid: 是否save label+prediction hybrid results to *.txt  默认False\n- save_conf: 是否保存预测每个目标的置信度到预测tx文件中 默认True\n- save_json: 是否按照coco的json格式保存预测框，并且使用cocoapi做评估（需要同样coco的json格式的标签）\n  - 运行test.py传入默认Fasle 运行train.py则传入is_coco and final_epoch(一般也是False)\n\n- project: 测试保存的源文件 默认runs/test\n- name: 测试保存的文件地址 默认exp  保存在runs/test/exp下\n- exist_ok: 是否存在当前文件 默认False 一般是 no exist-ok 连用  所以一般都要重新创建文件夹\n- half: 是否使用半精度推理 FP16 half-precision inference 默认False\n- model: 模型 如果执行test.py就为None 如果执行train.py就会传入ema.ema(ema模型)\n- dataloader: 数据加载器 如果执行test.py就为None 如果执行train.py就会传入testloader\n- save_dir: 文件保存路径 如果执行test.py就为‘’ 如果执行train.py就会传入save_dir(runs/train/expn)\n- plots: 是否可视化 运行test.py传入默认True 运行train.py则传入plots and final_epoch\n- wandb_logger: 网页可视化 类似于tensorboard 运行test.py传入默认None 运行train.py则传入wandb_logger(train)\n- compute_loss: 损失函数 运行test.py传入默认None 运行train.py则传入compute_loss(train)\n- return (Precision, Recall, map@0.5, map@0.5:0.95, box_loss, obj_loss, cls_loss)\n\n## 初始化/加载模型并选择处理器\n\n训练时（train.py）调用：初始化模型参数、训练设备\n验证时（val.py）调用：初始化设备、save_dir文件路径、make dir、加载模型、check imgsz、 加载+check data配置信息\n\n```python\n# Initialize/load model and set device\nglobal stride, ap50\ntraining = model is not None\nif training:  # called by train.py\n    device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model\n\n    half &= device.type != 'cpu'  # half precision only supported on CUDA\n    model.half() if half else model.float()\nelse:  # called directly\n    device = select_device(device, batch_size=batch_size)\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n    stride, pt, jit, onnx, engine = model.stride, model.pt, model.jit, model.onnx, model.engine\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n    half &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16 supported on limited backends with CUDA\n    if pt or jit:\n        model.model.half() if half else model.model.float()\n    elif engine:\n        batch_size = model.batch_size\n    else:\n        half = False\n        batch_size = 1  # export.py models default to batch-size 1\n        device = torch.device('cpu')\n        LOGGER.info(f'Forcing --batch-size 1 square inference shape(1,3,{imgsz},{imgsz}) for non-PyTorch backends')\n\n    # Data\n    data = check_dataset(data)  # check\n```\n\n## 调整模型\n\n```python\n    half &= device.type != 'cpu'  # half precision only supported on CUDA\n    model.half() if half else model.float()\nelse:  # called directly\n    device = select_device(device, batch_size=batch_size)\n```\n\n半精度验证half model + 模型剪枝prune + 模型融合conv+bn\n\n## 模型验证\n\n```python\nmodel.eval()\nis_coco = isinstance(data.get('val'), str) and data['val'].endswith('coco/val2017.txt')  # COCO dataset\nnc = 1 if single_cls else int(data['nc'])  # number of classes\niouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\nniou = iouv.numel()\n```\n\n是否是COCO数据集is_coco + 类别数nc + 计算mAP相关参数 + 初始化日志Logging\n\n## 加载val数据集\n\n训练时（train.py）调用：加载val数据集\n验证时（val.py）调用：不需要加载val数据集 直接从train.py 中传入testloader\n\n```python\n# Dataloader\nif not training:\n    model.warmup(imgsz=(1 if pt else batch_size, 3, imgsz, imgsz), half=half)  # warmup\n    pad = 0.0 if task in ('speed', 'benchmark') else 0.5\n    rect = False if task == 'benchmark' else pt  # square inference for benchmarks\n    task = task if task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n    dataloader = create_dataloader(data[task], imgsz, batch_size, stride, single_cls, pad=pad, rect=rect,\n                                   workers=workers, prefix=colorstr(f'{task}: '))[0]\n```\n\n## 初始化配置\n\n```python\nseen = 0\nconfusion_matrix = ConfusionMatrix(nc=nc)\nnames = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\nclass_map = coco80_to_coco91_class() if is_coco else list(range(1000))\ns = ('%20s' + '%11s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\ndt, p, r, f1, mp, mr, map50, map = [0.0, 0.0, 0.0], 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\nloss = torch.zeros(3, device=device)\njdict, stats, ap, ap_class = [], [], [], []\npbar = tqdm(dataloader, desc=s, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n```\n\n初始化混淆矩阵 + 数据集类名 + 获取coco数据集的类别索引 + 设置tqdm进度条 + 初始化p, r, f1, mp, mr, map50, map指标和时间t0, t1, t2 + 初始化测试集的损失 + 初始化json文件中的字典 统计信息 ap等\n\n# 开始验证\n\n```python\nfor batch_i, (im, targets, paths, shapes) in enumerate(pbar):\n```\n\n## 预处理图片和target\n\n```python\nt1 = time_sync()\nif pt or jit or engine:\n    im = im.to(device, non_blocking=True)\n    targets = targets.to(device)\nim = im.half() if half else im.float()  # uint8 to fp16/32\nim /= 255  # 0 - 255 to 0.0 - 1.0\nnb, _, height, width = im.shape  # batch size, channels, height, width\nt2 = time_sync()\ndt[0] += t2 - t1\n```\n\n## model前向推理\n\n```python\n# Inference\nout, train_out = model(im) if training else model(im, augment=augment, val=True)  # inference, loss outputs\ndt[1] += time_sync() - t2\n```\n\n## 计算验证集损失\n\n```python\n# Loss\nif compute_loss:\n    loss += compute_loss([x.float() for x in train_out], targets)[1]  # box, obj, cls\n```\n\n## 运行NMS\n\n```python\n# NMS\ntargets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\nlb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\nt3 = time_sync()\nout = non_max_suppression(out, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls)\ndt[2] += time_sync() - t3\n```\n\n首先将真实框target的xywh(因为target是在labelimg中做了归一化的)映射到img(test)尺寸； \n\n- save_hybrid: adding the dataset labels to the model predictions before NMS \n\n  意思是在NMS之前将数据集标签targets添加到模型预测中\n  这允许在数据集中自动标记(for autolabelling)其他对象(在pred中混入gt) 并且mAP反映了新的混合标签\n\n- targets: [num_target, img_index+class_index+xywh] = [31, 6]\n- lb: {list: bs} 第一张图片的target[17, 5] 第二张[1, 5] 第三张[7, 5]\n\n## 统计每章图片的真实框、预测框信息\n\n```python\n# Metrics\nfor si, pred in enumerate(out):\n    labels = targets[targets[:, 0] == si, 1:]\n    nl = len(labels)\n    tcls = labels[:, 0].tolist() if nl else []  # target class\n    path, shape = Path(paths[si]), shapes[si][0]\n    seen += 1\n\n    if len(pred) == 0:\n        if nl:\n            stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n        continue\n\n\t# Predictions\n    if single_cls:\n         pred[:, 5] = 0\n     predn = pred.clone()\n     scale_coords(im[si].shape[1:], predn[:, :4], shape, shapes[si][1]) # native-space pred\n```\n\n为每张图片做统计，写入预测信息到txt文件，生成json文件字典，统计tp等\n\n- out: list{bs}  [300, 6] [42, 6] [300, 6] [300, 6]  [:, image_index+class+xywh]\n\n获取第si张图片的gt标签信息，包括class、x、y、w、h，target[:, 0]为标签属于哪张图片的编号\n\n- nl为第si张图片的gt个数\n- path为第si张图片的地址\n\n如果预测为空，则添加空的信息到stats里\n\n- predn 将预测坐标映射到原图img中\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Vision Transform学习笔记2|Self-Attention","url":"/2022/04/13/ViT支线-Self-Attention/","content":"\n# 什么是Attention(注意力机制)\n\n在认知科学中，由于信息处理的瓶颈，人类会选择性地关注所有信息的一部分，同时忽略其他可见的信息。通俗点来说就是，我们在认知事物时，有着明显的主观色彩和测重，比如「我喜欢踢足球，但我更喜欢打篮球」，对于人类显然知道这个人更喜欢打篮球，但对于深度学习或计算机来说，它没办法领会到「更」的含义，因此没有办法知道这个结果。所以我们在训练模型的时候，会大家「更」字的权重，让它在句子中的重要性获得更大的占比。\n\n综上，注意力机制主要有两个方面：**决定需要关注输入的哪部分**；**分配有限的信息处理资源给重要的部分**。\n\n# 什么是Self-Attention\n\n在知道了attention在机器学习中的含义之后（下文都称之为注意力机制）。人为设计的注意力机制，是非常主观的，而且没有一个准则来评定，这个权重设置为多少才好。所以，如何让模型自己对变量的权重进行**自赋值**成了一个问题，这个**权重自赋值**的过程也就是self-attention。\n\n# Self-Attention原理\n\n$$\nAttention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n$$\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220413150524.png)\n\n## Softmax操作\n\n抛开Q,K,V三个矩阵不谈，self-attention最原始的形态是$Softmax(XX^T)X$\n\n这个公式表示什么意思呢？\n\n==Q1：$XX^T$代表什么？==\n\n一个矩阵乘以它自己的转置，会得到什么结果，有什么意义？\n\n我们知道，矩阵可以看作由一些向量组成，一个矩阵乘以它自己转置的运算，其实可以看成这些向量分别与其他向量计算内积。（此时脑海里想起矩阵乘法的口诀，第一行乘以第一列、第一行乘以第二列......嗯哼，矩阵转置以后第一行不就是第一列吗？这是在计算**第一个行向量与自己**的内积，第一行乘以第二列是计算**第一个行向量与第二个行向量的内积**第一行乘以第三列是计算**第一个行向量与第三个行向量的内积**.....）\n\n回想我们文章开头提出的问题，向量的内积，其几何意义是什么？\n\n**A1：表征两个向量的夹角，表征一个向量在另一个向量上的投影**\n\n实例：我们假设$X=[x^T_1;x^T_2;x^T_3]$，其中X为一个二维矩阵，$X^T_i$为一个行向量，下图中模拟运算了$XX^T$：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220413153154.png)\n\n首先，行向量$X^T_i$分别与自己和其他两个向量做内积，得到了一个新的向量。\n\n==Q2：新的向量有什么意义？表征什么？==\n\n**A2：投影的值大，说明两个向量相关度高。**\n\n更进一步，这个向量是词向量，是词在高维空间的数值映射。词向量之间相关度高表示什么？是不是**在一定程度上**（不是完全）表示，在关注词A的时候，应当给予词B更多的关注？\n\n==Q3：$XX^T$的意义是什么？==\n\n**A3：矩阵**$XX^T$**是一个方阵，我们以行向量的角度理解，里面保存了每个向量与自己和其他向量进行内积运算的结果。**\n\n==Q4：Softmax操作的意义是什么？==\n\n回到Softmax的公式：$Softmax(z_i)=\\frac{e^{z_i}}{\\sum^C_{c=1}e^{z_c}}$\n\n**A4：归一化。**\n\n也就是说通过Softmax操作后，这些数字的和为1了。\n\n==Q5：那么Attention机制的核心是什么呢？==\n\n**A5：加权求和。**\n\n## Q,K,V矩阵\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220413155750.png)\n\n在很多文章中提到的Q,K,V矩阵、查询矩阵之类的字眼，本质上都是**X矩阵的线性变换**，其来源是X与某个矩阵的乘积。\n\n==Q6：为什么不直接使用X而要对其进行线性变换呢？==\n\n**A6：当然是为了提升模型的拟合能力，矩阵W都是可以训练的，起到一个缓冲的作用。**\n\n## $\\sqrt{d_k}$的意义\n\n假设Q,K里的元素全为0，方差为1，那么$A^T=Q^TK$中元素的均值为0，方差为d。当d变得很大时，A中的元素的方差也会变得很大，如果A中的元素方差很大，那么Softmax(A)的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。\n\n**总结**：Softmax(A)的分布会和d有关。因此A中每一个元素除以$\\sqrt{d_k}$后，方差又变为1。这使得Softmax(A)的分布陡峭程度与d解耦，从而使得训练过程中梯度保持稳定。\n\n**对self-attention来说，它跟每一个input vector都做attention，所以没有考虑到input sequence的顺序**。\n\n# Self-Attention的优点\n\n与RNN相比，RNN的一个最大的问题是：前面的变量在经过多次RNN计算后，已经失去了原有的特征。越到后面，最前面的变量占比就越小，这是一个很反人类的设计。而self-attention在每次计算中都能保证每个输入变量a a*a*的初始占比是一样的，这样才能保证经过self-attention layer计算后他的注意力系数是可信的。\n\n总结下来，它的优点是：\n\n- 需要学习的参数量更少\n- 可以并行计算\n- 能够保证每个变量初始化占比是一样的\n\n# Multi-head Self-Attention\n\n这里继续讲解multi-head self-attention，所谓head也就是指一个a a*a*衍生出几个q , k , v 。上述所讲解的self-attention是基于single-head的。以2 head为例：\n\n首先，$a^i$先生成$q^1$，$ k^1$，$ v^1$。然后，接下来就和single-head不一样了，$q^i$生成 $q^{i,1}$,$q^{i,2}$，生成的方式有两种：\n\n1.  $q^i$乘上一个$W^{q,1}$得到 $q^{i,1}$，乘上 $W^{q,2}$得到$q^{i,2}$这个和single-head的生成是差不多的；\n2. $q^i$**直接从通道维，平均拆分成两个，得到$ q^{i,1}$,$q^{i,2}$；\n\n这两种方式，在最后结果上都差不多。至于为啥，后面会讲一下原因。\n\n那么这里的图解使用第1个方式，先得到$q^{i,1}$ $k^{i,1}$ $v^{i,1}$。对 $a^j$做同样的操作得到 $q^{j,1}$**, $k^{j,1}$,$ v^{j,1}$。这边需要注意的一点，$q^{i,1}$是要和$k^{j,1}$做矩阵乘法，而非$k^{j,2}$，一一对应。后面计算就和single-head一样了，最后得到$b^{i,1}$。\n\n# Position Encoding","tags":["python","计算机视觉","Vision Transform","Self-Attention"],"categories":["计算机视觉"]},{"title":"数学建模综述","url":"/2022/04/12/数学建模综述/","content":"\n# 数学建模基本题型\n\n## 预测类\n\n### 概述\n\n指的是通过分析已有的数据或者现象，找出其内在发展规律，然后对未来情形做出预测的过程。\n根据已知条件和求解目的，往往将预测类问题分为：\n\n- 小样本内部预测\n- 大样本内部预测\n- 小样本未来预测\n- 大样本随机因素或周期特征的未来预测\n- 大样本的未来预测\n\n### 例题——2021年第十一届MathorCup高校数学建模挑战赛B题\n\n> **B题 三维团簇的能量预测**\n> 团簇，也称超细小簇，属纳米材料的尺度概念。团簇是由几个乃至上\n> 千个原子、分子或离子通过物理或化学结合力组成的相对稳定的微观或亚\n> 微观聚集体，其物理和化学性质随所含的原子数目而变化。\n>\n> 团簇是材料尺度纳米材料的一个概念。团簇的空间尺度是几埃至几百埃的范围，用无机分子来描述显得太小，用小块固体描述又显得太大，许多性质既不同于单个原子分子，又不同于固体和液体，也不能用两者性质的简单线性外延或内插得到。因此，人们把团簇看成是介于原子、分子与宏观固体物质之间的物质结构的新层次。团簇科学是凝聚态物理领域中非常重要的研究方向。\n>\n> 团簇可以分为金属团簇和非金属团簇，由于金属团簇具有良好的催化性能，因此备受关注。但由于团簇的势能面过于复杂，同时有时候还需要考虑相对论效应等，所以搜索团簇的全局最优结构（即能量最低）显得尤为困难。其中，传统的理论计算方法需要数值迭代求解薛定谔方程，并且随原子数增加，高精度的理论计算时间呈现指数增长，非常耗时。因此，目前需要对这种方法加以改进，例如：考虑全局优化算法，结合机器学习等方法，训练团簇结构和能量的关系，从而预测新型团簇的全局最优结构，有利于发现新型团簇材料的结构和性能。请建立三维团簇能量预测的数学模型，并使用附件中的坐标和能量数据，解决下列问题。\n>\n> 备注：附件中数据集格式为xyz，第一行是原子数，第二行是能量，后面是原子的三维坐标。可用文本阅读器打开，并用VMD 等软件进行可视化。\n> **问题1**：针对金属团簇，附件给出了1000 个金团簇$Au_{20}$的结构，请你们建立金团簇能量预测的数学模型，并预测金团簇$Au_{20}$ 的全局最优结构，\n> 描述形状；\n> **问题2**：在问题1 的基础上，请你们设计算法，产生金团簇不同结构的异构体，自动搜索和预测金团簇$Au_{32}$的全局最优结构，并描述其几何形\n> 状，分析稳定性；\n> **问题3**：针对非金属团簇，附件给出了3751 个硼团簇$B_{45}$的结构，请你们建立硼团簇能量预测的数学模型，并预测硼团簇B45的全局最优结构，描述形状；\n> **问题4**：在问题3 的基础上，请你们设计算法，产生硼团簇不同结构的异构体，自动搜索和预测硼团簇$B_{40}$的全局最优结构，并描述其几何形状，分析稳定性。\n\n**注：预测问题主要是以某个小问的形式出现，很少有整个赛题所有小问全是预测要求的**\n\n## 评价类\n\n指的是按照一定的标准对事物的发展或者现状进行划分的过程在数学建模中题点可体现在对生态环境，社会建设，方案策略等进行评价。评价类赛题往往没有明确的指标体系和评价标准，往往是需要查阅各类资料进行构建的，因此评价类赛题也没有明确的答案。\n\n**注：解决评价类赛题的关键是指标体系的构建，构建完评价体系后在选择合适的评价方法即可，体系建立应秉承全面，准确，独立的三要素**\n\n## 机理分析类赛题\n\n机理分析是根据对现实对象特性的认识，分析其因果关系，找出反映内部机理的规律。在求解机理分析类问题时首先需要探寻与问题相关的物理，化学，经济等相关的知识，然后通过对已知数据或现象的分析对事物的内在规律做出必要的假设，最后通过构建合适的方程或关系式对其内在规律进行数值表达。\n\n**注：机理分析立足于建立事物内部的规律，相对于其他类型的赛题均有章可循，机理分析类赛题往往需要结合众多关联知识才可以进行求解，如空气动力学，流体力学，热力学等**\n\n## 优化类\n\n指在现有现有条件固定的情况下，如何使目标效果达到最佳。如在一座城市公交车公司拥有的公交车数量是固定的，问如何安排线路能够使盈利达到最高。优化类问题往往需要分析三个关键因素：目标函数，决策变量和约束条件，三者往往缺一不可。\n\n**注：解决优化类赛题必须知道优化的目的，约束的条件和所求解的关键变量，需要有较强的编程能力和赛题分析挖掘能力**\n\n# 数学建模算法\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/模型与算法分类.png)","tags":["数学建模"],"categories":["数学建模"]},{"title":"数学建模学习笔记2|综合评价类模型","url":"/2022/04/11/数学建模学习笔记2/","content":"\n# 简介\n\ntopsis综合评价法即根据有限个评价对象与理想化目标的接近程度进行排序的方法，是在现有的对象中进行相对优劣的评价，是一种逼近于理想解的排序法。\n\n> 基本过程为先将原始数据矩阵统一指标类型（一般正向化处理）得到正向化的矩阵，再对正向化的矩阵进行标准化处理以消除各指标量纲的影响，并找到有限方案中的最优方案和最劣方案，然后分别计算各评价对象与最优方案和最劣方案间的距离，获得各评价对象与最优方案的相对接近程度，以此作为评价优劣的依据。该方法对数据分布及样本含量没有严格限制，数据计算简单易行。\n\n**适用于：**决策层中指标的数据是已知的，利用这些数据使得评价的更加准确。\n\n# 算法原理\n\n## 统一指标类型\n\n将所有的指标转化为极大型称为**指标正向化**(最常用)\n\n## 第一步：将原始矩阵正向化\n\n常见的四种指标：\n\n|      指标名称      |     指标特点     |           例子           |\n| :----------------: | :--------------: | :----------------------: |\n| 极大型(效益型)指标 |   越大(多)越好   | 成绩、GDP增速、企业利润  |\n| 极小型(成本型)指标 |   越小(少)越好   |  费用、坏品率、污染程度  |\n|     中间型指标     | 越接近某个值越好 |    水质量评估时的PH值    |\n|     区间型指标     | 落在某个区间最好 | 体温、水中植物性营养物量 |\n\n*注：将原始矩阵正向化，就是要将所有的指标类型统一转化为极大型指标*\n**转换的函数形式可以不唯一**\n\n### 各种类型指标的转换\n\n- 极小型指标→极大型指标\n\n  - 公式：$max-x$\n\n  - **补充**：如果所有元素均为正数，那么也可以使用1/x\n\n- 中间型指标→极大型指标\n\n  - 中间型指标：指标值既不要太大也不要太小，取某特定值最好(如水质量评估PH值)。\n\n  - 公式：$M=max{|X_i-X_{best}|}$,$\\widetilde{X_i}=1-\\frac{|X_i-X_{best}|}{M}$，其中{X~i~}是一组中间型指标序列，且最佳的数值为X~best~。\n\n- 区间型指标→极大型指标\n\n  - 区间型指标：指标值落在某个区间内最好，例如人的体温在36°～37°这个区间比较好。\n\n  - 公式：$M=max[{a-min({x_i}),max(x_i)-b}]$， $\\widetilde{X_i}=\\begin{cases}{ 1-\\frac{a-x_i}{M},x_i<a} \\\\ 1 ,     a\\le x_i \\le b\\\\ 1-\\frac{x_i-b}{M}, x_i > b \\end{cases}$\n\n    其中$x_i$是一组区间型指标序列，且最佳的区间为[a,b]\n\n## 第二步：正向化矩阵标准化\n\n**标准化的目的是消除不同指标量纲的影响**\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220412222627.png)\n\n## 第三步：计算得分并归一化\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220412222728.png)\n\n# 例题\n\n> 题目：评价下表中20条河流的水质情况。\n> 注：含氧量越高越好；PH值越接近7越好；细菌总数越少越好；植物性营养物量介于10‐20之间最佳，超过20或低于10均不好。\n> ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220412222931.png)\n\n# 实现代码\n\n## 第一步：把数据复制到工作区，并将矩阵命名为X\n\n（1）在工作区右键，点击新建（Ctrl+N)，输入变量名称为X\n（2）在Excel中复制数据，再回到Excel中右键，点击粘贴Excel数据（Ctrl+Shift+V）\n（3）关掉这个窗口，点击X变量，右键另存为，保存为mat文件\n注意：代码和数据要放在同一个目录下，且Matlab的当前文件夹也要是这个目录。\n\n==导入数据代码：==\n\n```matlab\nload XX.mat\n```\n\n## 第二步：判断是否需要正向化\n\n```matlab\n[n,m] = size(X); %将表格数据转换为矩阵\n```\n\n==正向化代码：==\n\n- 极小型→极大型\n\n  ```matlab\n  function [posit_x] = Min2Max(x)\n      posit_x = max(x) - x;\n       %posit_x = 1 ./ x;    %如果x全部都大于0，也可以这样正向化\n  end\n  ```\n\n- 中间型→极大型\n\n  ```MATLAB\n  function [posit_x] = Mid2Max(x,best)\n      M = max(abs(x-best));\n      posit_x = 1 - abs(x-best) / M;\n  end\n  ```\n\n- 区间型→极大型\n\n  ```MATLAB\n  function [posit_x] = Inter2Max(x,a,b)\n      r_x = size(x,1);  % row of x \n      M = max([a-min(x),max(x)-b]);\n      posit_x = zeros(r_x,1);   %zeros函数用法: zeros(3)  zeros(3,1)  ones(3)\n      % 初始化posit_x全为0  初始化的目的是节省处理时间\n      for i = 1: r_x\n          if x(i) < a\n             posit_x(i) = 1-(a-x(i))/M;\n          elseif x(i) > b\n             posit_x(i) = 1-(x(i)-b)/M;\n          else\n             posit_x(i) = 1;\n          end\n      end\n  end\n  ```\n\n## 第三步：对正向化后的矩阵进行标准化\n\n```matlab\nZ = X ./ repmat(sum(X.*X) .^ 0.5, n, 1);\ndisp('标准化矩阵 Z = ')\ndisp(Z)\n```\n\n## 第四步：计算与最大值的距离和最小值的距离，并算出得分\n\n```MATLAB\nD_P = sum([(Z - repmat(max(Z),n,1)) .^ 2 ],2) .^ 0.5;   % D+ 与最大值的距离向量\nD_N = sum([(Z - repmat(min(Z),n,1)) .^ 2 ],2) .^ 0.5;   % D- 与最小值的距离向量\nS = D_N ./ (D_P+D_N);    % 未归一化的得分\ndisp('最后的得分为：')\nstand_S = S / sum(S)\n[sorted_S,index] = sort(stand_S ,'descend')\n```\n\n## 补充——幻方矩阵\n\nA = magic(5)  % 幻方矩阵\nM = magic(n)返回由1到n^2的整数构成并且总行数和总列数相等的n×n矩阵。阶次n必须为大于或等于3的标量。\nsort(A)若A是向量不管是列还是行向量，默认都是对A进行升序排列。sort(A)是默认的升序，而sort(A,'descend')是降序排序。\nsort(A)若A是矩阵，默认对A的各列进行升序排列\nsort(A,dim)\ndim=1时等效sort(A)\ndim=2时表示对A中的各行元素升序排列\nA = [2,1,3,8]\nMatlab中给一维向量排序是使用sort函数：sort（A），排序是按升序进行的，其中A为待排序的向量；\n若欲保留排列前的索引，则可用 [sA,index] = sort(A,'descend') ，排序后，sA是排序好的向量，index是向量sA中对A的索引。\nsA  =  8     3     2     1\nindex =  4     3     1     2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["matlab"],"categories":["数学建模"]},{"title":"碎碎念1","url":"/2022/04/09/碎碎念1/","content":"\n所以你读了几篇论文？\n写了几段代码？\n真正思考过什么东西？\n你真的没有在努力。\n\n\n只要我还在一直读书，我就能够一直理解自己的痛苦。\n一直与自己的无知、狭隘、偏见、阴暗见招拆招。\n很多人说：“和自己握手言和”，\n我不要做这样的人，我要拿着石头打磨我这块石头。\n会一直读书，一直痛苦，一直爱着从痛苦荒芜里生出来的喜悦。\n乘兴而来，尽兴而归，在一生中，这是很难得很难得的一件事情。\n\n再后来我知道阅读只是理解世界的一种方式，\n如果你有其他的方式与世界产生沟通，进行理解，\n被世界在风雨雷电中触动，那甚至不读书也行，\n不是必须要走的路，也不是那条路比另一条路高级，\n你需要做的甚至也不是阅读，是与世界产生对话，理解自己所在的时代。\n\n你知道你最大的问题是什么么？\n误认为自己有无限的时间和无限的可能，又不知道自己需要努力的方向在哪里。","tags":["务虚笔记"],"categories":["随笔"]},{"title":"mAP值解析","url":"/2022/04/06/Yolov5支线学习之mAP值/","content":"\n# 目标检测算法评价指标——mAP值\n\n> mAP值即为平均精度，是衡量目标检测算法优劣的常用指标。\n> AP（平均精度）是衡量目标检测算法好坏的常用指标，在Faster R-CNN，SSD等算法中作为评估指标。\n> AP等于recall值取0-1时，precision值的平均值。\n\n## Precision & Recall(查准率和查全率)\n\n### 概念\n\n **Precision**：衡量你的模型预测准确度。即预测的数目中正确的百分比。\n\n> 例：你预测100个图片是苹果，其中80个真的是苹果，那么你的Precision为0.8\n\n**recall**：召回表示预测正确的目标数量。\n\n> 例：总共有100张苹果图片，你成功找到其中50张，那么你的recall为0.5\n\n### 定义\n\n以二分类结果为例：\n\n对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为**真正例(true positive)**、**假正例(false positive)**、**真反例(true negative)**、**假反例(false negative)**四种情形，令**TP**、**FP**、**TN**、**FN**分别表示其对应的样例数，则显然有**TP+FP+TN+FN=样例总数**。分类结果的“混淆矩阵”(confusion matrix)如表所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220409165719.png)\n\n查准率P和查全率R分别定义为：\n\n$ P = \\frac{TP}{TP+FP}$        $R = \\frac{TP}{TP+FN}$\n\n一般来说查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。\n\n**平衡点(BER):查准率=查全率时的取值,用来比较模型好坏.** \n\n## 交并比(IoU)\n\nIoU是预测框与ground truth的交集和并集的比值。\n\n为了计算precision和recall，与所有机器学习问题一样，我们必须鉴别出**True Positives**（真正例）、**False Positives**（假正例）、**True Negatives**（真负例）和 **False Negatives**（假负例）。\n\n假设边界框对应的IoU大于某个阈值（一般来说，比较常用的IoU阈值是0.5），我们就可以说这个预测的边界框是对的，或者说可以被划分为TP中。反之如果IoU小于阈值，那么这个预测的边界框就是错的，或者说是一个FP。如果对于图像中某个物体来说，我们的模型没有预测出对应的边界框，那么这种情况就可以被记为一次FN。\n\n- **True Positive (TP)**: IOU>=阈值的检测框\n- **False Positive (FP)**: IOU<阈值的检测框\n- **False Negative (FN)**: 未被检测到的GT\n- **True Negative (TN)**: 忽略不计\n\n对于每一个图片，ground truth数据会给出该图片中各个类别的实际物体数量。我们可以计算每个Positive预测框与ground truth的IoU值，并取最大的IoU值，认为该预测框检测到了那个IoU最大的ground truth。然后根据IoU阈值，我们可以计算出一张图片中各个类别的正确检测值（True Positives, TP）数量以及错误检测值数量（False Positives, FP）。\n\n既然我们已经得到了正确的预测值数量（True Positives），也很容易计算出漏检的物体数（False Negatives, FN）。\n\n## AP值\n\n### 定义\n\n**PR曲线下面积的近似，是一个0~1之间的数值，也可用来衡量模型的performance**。\n\n- PR曲线比较直观，但由于曲线的上下震荡，不方便比较不同模型的PR曲线\n- AP是一个数字，模型的AP大，则模型更好，方便比较不同模型\n\n### 计算方法\n\n计算AP值，一般有两种方法：\n\n1. 11点插值法：\n\n   选取当Recall >= 0, 0.1, 0.2, ..., 1共11个点时的Precision最大值，AP是这11个Precision的平均值，此时只由11个点去近似PR曲线下面积。\n   $$\n   AP = \\frac{1}{11}\\sum_{r\\in(0,0,,1…1)}\\rho_{interp(r)}\n   $$\n\n   $$\n   \\rho_{interp(r)} = max_{\\widetilde{r}:\\widetilde{r}\\geq{r}}\\rho(\\widetilde{r})\n   $$\n\n   \n\n2. 所有点插值法：\n\n   针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值：\n   $$\n   \\sum_{r=0}^{1}(r_{n+1}-r_n)\\rho_{interp}(r_{n+1})\n   $$\n\n   $$\n   \\rho_{interp(r)} = max_{\\widetilde{r}:\\widetilde{r}\\geq{r}}\\rho(\\widetilde{r})\n   $$\n\n   由于此方法用了所有点去近似PR曲线下面积，计算的AP比11点插值法更准确。\n\n## mAP(mean Average Precision, 即各类别AP的平均值)\n\n对于各个类别，分别按照上述方式计算AP，取所有类别的AP平均值就是mAP。这就是在目标检测问题中mAP的计算方法。可能有时会发生些许变化，如COCO数据集采用的计算方式更严格，其计算了不同IoU阈值和物体大小下的AP.\n\n**在评测时，COCO评估了在不同的交并比(IoU)[0.5:0.05:0.95]共10个IoU下的AP，并且在最后以这些阈值下的AP平均作为结果，记为mAP@[.5, .95]。**\n\n而在Pascal VOC中，检测结果只评测了IOU在0.5这个阈值下的AP值。因此相比VOC而言，COCO数据集的评测会更加全面：不仅评估到物体检测模型的分类能力，同时也能体现出检测模型的定位能力。因此在IoU较大如0.8时，预测框必须和真实的框具有很大的重叠比才能被视为正确。","categories":["计算机视觉"]},{"title":"Deep_Learning学习笔记2|优化模型","url":"/2022/04/05/Deep_Learning学习笔记2/","content":"# Deep_Learning学习笔记2\n\n## 极简手写数字识别模型\n\n### 基础模型：神经网络\n\n- 套用房价预测的模型\n- 输入：由28*28改为784/每个像素值\n- 输出：1，预测的数据值\n\n### 以类的方式组建网络\n\n- 初始化函数：定义每层的函数\n- Forward函数：层之间的串联方式\n\n```python\n# 定义mnist数据识别网络结构，同房价预测网络\nclass MNIST(fluid.dygraph.Layer):\n    def __init__(self, name_scope):\n        super(MNIST, self).__init__(name_scope)\n        name_scope = self.full_name()\n        # 定义一层全连接层，输出维度是1,激活函数为None，即不使用激活函数\n        self.fc = Linear(input_dim=784, output_dim=1, act=None)\n\n    # 定义网络结构的前向计算过程\n    def forward(self, inputs):\n        outputs = self.fc(inputs)\n        return outputs\n```\n\n> input_dim设置为784,即输入为784\n>\n> output_dim为1，即网络层数为1\n>\n> act为None，即不使用激活函数\n>\n> 在init()中申明网络结构，在forward()函数中把这些结构串联，\n\n### 训练过程\n\n- 代码几乎与房价预测任务一致\n- 包含四个部分：\n  - 生成模型实例，设为“训练”状态\n  - 配置优化器，SGD Optimizer\n  - 两层循环的训练过程\n  - 保存模型参数，便于后续使用\n\n- 仅在向模型灌入数据的代码不同\n  - 先转变成np.array格式\n  - 再转换成框架内置格式 to variable\n\n```python\n# 通过with语句创建一个dygraph运行的context\n# 动态图下的一些操作需要在guard下进行\nwith fluid.dygraph.guard():\n    model = MNIST(\"mnist\")\n    # 启动训练模式\n    model.train()\n    # 加载训练集 batch_size 设为 16\n    train_loader = paddle.batch(paddle.dataset.mnist.train(), batch_size=16)\n    # 定义优化器，使用随机梯度下降SGD优化器，学习率设置为0.001\n    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.001,parameter_list=model.parameters())\n    EPOCH_NUM = 10\n    for epoch_id in range(EPOCH_NUM):\n        for batch_id, data in enumerate(train_loader()):\n            # 准备数据并转化成符合框架要求的格式\n            image_data = np.array([x[0] for x in data]). astype('float32')\n            label_data = np.array([x[1] for x in data]). astype('float32').reshape(-1, 1)\n            # 将格式转为飞桨动态图格式\n            image = fluid.dygraph.to_variable(image_data)\n            label = fluid.dygraph.to_variable(label_data)\n            # 前向计算的过程\n            predict = model(image)\n            # 计算损失\n            loss = fluid.layers.square_error_cost(predict, label)\n            avg_loss = fluid.layers.mean(loss)\n            # 每训练了1000批次的数据，打印下当前Loss的情况\n            if batch_id != 0 and batch_id % 1000 == 0:\n                print(\"epoch_id: {}, batch_id: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n            # 后向传播，更新参数的过程\n            avg_loss.backward()\n            optimizer.minimize(avg_loss)\n            model.clear_gradients()\n# 保存模型\nfluid.save_dygraph(model.state_dict(),'mnist1')\n```\n\n每训练1000批次打印的Loss数据如下:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331155448.png)\n\n可以看出训练的效果并不好\n\n- Loss的值并没有在1以下，甚至有的还超过3\n- 从epoch0到epoch9，Loss值总体上下降趋势并不明显\n\n### 测试效果\n\n```python\n# 测试效果\ndef load_image(img_path):\n    im = Image.open(img_path).convert('L')\n    # print(np.array(im))\n    im = im.resize((28, 28), Image.ANTIALIAS)\n    im = np.array(im).reshape(1, -1).astype(np.float32)\n    im = 2 - im / 127.5\n    return im\n\n# 定义预测过程\nwith fluid.dygraph.guard():\n    model = MNIST(\"mnist\")\n    params_file_path = 'mnist3'\n    img_path = './work/example_0.jpg'\n    # 加载数据模型\n    model_dict, _ = fluid.load_dygraph(\"mnist3\")\n    model.load_dict(model_dict)\n\n    model.eval()\n    tensor_img = load_image(img_path)\n    result = model(fluid.dygraph.to_variable(tensor_img))\n    # 预测输出取整，即为预测的数字\n    print(\"本次预测的数字是:\",result.numpy().astype('int32'))\n```\n\n预测的结果如下图所示:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331161622.png)\n\n显然是不准确的\n\n## 优化版手写数字识别模型\n\n### 网络模型\n\n#### 多层感知机\n\n- 代码如下：\n\n  ```python\n  # 定义多层全连接神经网络\n  class MNIST(paddle.nn.Layer):\n      def __init__(self):\n          super(MNIST, self).__init__()\n          # 定义两层全连接隐含层，输出维度是10，当前设定隐含节点数为10，可根据任务调整\n          self.fc1 = Linear(in_features=784, out_features=10)\n          self.fc2 = Linear(in_features=10, out_features=10)\n          # 定义一层全连接输出层，输出维度是1\n          self.fc3 = Linear(in_features=10, out_features=1)\n      # 定义网络的前向计算，隐含层激活函数为sigmoid，输出层不使用激活函数\n      def forward(self, inputs):\n          # inputs = paddle.reshape(inputs, [inputs.shape[0], 784])\n          outputs1 = self.fc1(inputs)\n          outputs1 = F.sigmoid(outputs1)\n          outputs2 = self.fc2(outputs1)\n          outputs2 = F.sigmoid(outputs2)\n          outputs_final = self.fc3(outputs2)\n          return outputs_final\n  ```\n\n  > - 输入层的尺度为28×28，但批次计算的时候会统一加1个维度（大小为batch size）。\n  > - 中间的两个隐含层为10×10的结构，激活函数使用常见的Sigmoid函数。\n  > - 与房价预测模型一样，模型的输出是回归一个数字，输出层的尺寸设置成1。\n\n- 训练效果\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331200615.png)\n\n#### 卷积神经网络\n\n- 代码如下\n\n  ```python\n  # 多层卷积神经网络实现\n  class MNIST(paddle.nn.Layer):\n      def __init__(self):\n          super(MNIST, self).__init__()\n          # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n          self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n          # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n          self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n          # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n          self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n          # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n          self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n          # 定义一层全连接层，输出维度是1\n          self.fc = Linear(input_dim=980, output_dim=1)\n      # 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出\n      # 卷积层激活函数使用Relu，全连接层不使用激活函数\n      def forward(self, inputs):\n          x = self.conv1(inputs)\n          x = F.relu(x)\n          x = self.max_pool1(x)\n          x = self.conv2(x)\n          x = F.relu(x)\n          x = self.max_pool2(x)\n          x = paddle.reshape(x, [x.shape[0], -1])\n          x = self.fc(x)\n          return x\n  ```\n\n- 训练结果如下\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331201203.png)\n\n**比较经典全连接神经网络和卷积神经网络的损失变化，可以发现卷积神经网络的损失值下降更快，且最终的损失值更小。**\n\n### 损失函数\n\n#### 均方误差\n\n上述卷积神经网络的模型的损失函数用的即为均方误差。\n\n#### 交叉熵——sigmoid()\n\n修改计算损失的函数:\n\n- 从：`loss = paddle.nn.functional.square_error_cost(predict, label)`\n- 到：`loss = paddle.nn.functional.cross_entropy(predict, label)`\n\n训练结果如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331210432.png)\n\n### 优化算法\n\n在深度学习神经网络模型中，通常使用标准的随机梯度下降算法更新参数，学习率代表参数更新幅度的大小，即步长。当学习率最优时，模型的有效容量最大，最终能达到的效果最好。学习率和深度学习任务类型有关，合适的学习率往往需要大量的实验和调参经验。探索学习率最优值时需要注意如下两点：\n\n- **学习率不是越小越好**。学习率越小，损失函数的变化速度越慢，意味着我们需要花费更长的时间进行收敛，如 **图2** 左图所示。\n- **学习率不是越大越好**。只根据总样本集中的一个批次计算梯度，抽样误差会导致计算出的梯度不是全局最优的方向，且存在波动。在接近最优解时，过大的学习率会导致参数在最优解附近震荡，损失难以收敛，如 **图2** 右图所示。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331210808.png)\n\n#### 设置学习率\n\n在训练前，我们往往不清楚一个特定问题设置成怎样的学习率是合理的，因此在训练时可以尝试调小或调大，通过观察Loss下降的情况判断合理的学习率，设置学习率的代码如下所示。\n\n```python\n#设置不同初始学习率\nopt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n# opt = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())\n# opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n```\n\n#### 学习率的主流优化算法\n\n学习率是优化器的一个参数，调整学习率看似是一件非常麻烦的事情，需要不断的调整步长，观察训练时间和Loss的变化。经过研究员的不断的实验，当前已经形成了四种比较成熟的优化算法：SGD、Momentum、AdaGrad和Adam，效果如下图所示。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220331211905.png)\n\n- **SGD**： 随机梯度下降算法，每次训练少量数据，抽样偏差导致的参数收敛过程中震荡。\n- **Momentum**： 引入物理“动量”的概念，累积速度，减少震荡，使参数更新的方向更稳定。\n\n> 每个批次的数据含有抽样误差，导致梯度更新的方向波动较大。如果我们引入物理动量的概念，给梯度下降的过程加入一定的“惯性”累积，就可以减少更新路径上的震荡，即每次更新的梯度由“历史多次梯度的累积方向”和“当次梯度”加权相加得到。历史多次梯度的累积方向往往是从全局视角更正确的方向，这与“惯性”的物理概念很像，也是为何其起名为“Momentum”的原因。类似不同品牌和材质的篮球有一定的重量差别，街头篮球队中的投手（擅长中远距离投篮）喜欢稍重篮球的比例较高。一个很重要的原因是，重的篮球惯性大，更不容易受到手势的小幅变形或风吹的影响。\n\n- **AdaGrad**： 根据不同参数距离最优解的远近，动态调整学习率。学习率逐渐下降，依据各参数变化大小调整学习率。\n\n> 通过调整学习率的实验可以发现：当某个参数的现值距离最优解较远时（表现为梯度的绝对值较大），我们期望参数更新的步长大一些，以便更快收敛到最优解。当某个参数的现值距离最优解较近时（表现为梯度的绝对值较小），我们期望参数的更新步长小一些，以便更精细的逼近最优解。类似于打高尔夫球，专业运动员第一杆开球时，通常会大力打一个远球，让球尽量落在洞口附近。当第二杆面对离洞口较近的球时，他会更轻柔而细致的推杆，避免将球打飞。与此类似，参数更新的步长应该随着优化过程逐渐减少，减少的程度与当前梯度的大小有关。根据这个思想编写的优化算法称为“AdaGrad”，Ada是Adaptive的缩写，表示“适应环境而变化”的意思。RMSProp是在AdaGrad基础上的改进，学习率随着梯度变化而适应，解决AdaGrad学习率急剧下降的问题。\n\n- **Adam**： 由于动量和自适应学习率两个优化思路是正交的，因此可以将两个思路结合起来，这就是当前广泛应用的算法。\n\n#### 利用不同的优化算法训练模型\n\n```python\n#四种优化算法的设置方案，可以逐一尝试效果\n    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n    # opt = paddle.optimizer.Momentum(learning_rate=0.01, momentum=0.9, parameters=model.parameters())\n    # opt = paddle.optimizer.Adagrad(learning_rate=0.01, parameters=model.parameters())\n    # opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n```\n\n### 超参数\n\n在深度学习中，超参数有很多，比如学习率α、使用momentum或Adam优化算法的参数（β1，β2，ε）、层数layers、不同层隐藏单元数hidden units、学习率衰退、mini=batch的大小等。\n\n","tags":["python","CNN","deep_learning"],"categories":["深度学习"]},{"title":"Paper Reading 3|EfficientNet","url":"/2022/04/04/文献阅读3/","content":"\n# 文献阅读3|EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\n\n## Intrudoction\n\n在论文中，作者介绍了**放大卷积神经网络**是一种常见的提高模型准确率的方法。但是在传统的方法中，通常只是在某单一维度上进行放大（**宽度width**，**深度depth**，**图片分辨率resolution**），宽度就是网络中的过滤器的数量，因为增加了过滤器的数量，该层的输出的通道数就相应变大了，深度可以理解为整个网络结构的长度，即网络中layer的数量。那么为什么在这几个维度上进行放大可以提高准确率？因为增加了图片的分辨率或则增加了网络的宽度，网络就能够捕获到更过细节的特征，而增加网络的深度能够捕获到更丰富和更复杂的特征。\n虽然也可以任意的放大两个或三个维度，但是因为维度变多，设计空间也随之变大，因此随意的放大多个维度需要耗费较大的人力来调整，并且也通常会一个次优的精度和效率。因此作者通过研究实验提出了一种新的缩放方法——复合缩放方法(compound scaling method)。\n\n在一些手工设计网络中(如AlexNET、VGG、ResNet等)，我们常常会有这样的疑问：为什么输入图像分辨率要固定为224，为什么卷积的个数要设置为这个值？为什么网络的深度设为这么深？这些问题你要问设计作者的话，估计回复就四个字——工程经验。\n这篇论文使用NAS(Neural Architecture Search)技术来搜索网络的图像输入分辨率r，网络的深度depth，以及channel的宽度width三个参数的合理化配置。\n\t\tEfficientNetB0到B7与其他网络的对比如下图所示:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402161317.png)\n\n为什么在这几个维度上进行放大可以提高准确率呢？因为增加了图片的分辨率或则增加了网络的宽度，网络就能够捕获到更过细节的特征，而增加网络的深度能够捕获到更丰富和更复杂的特征。\n\n虽然也可以任意的放大两个或三个维度，但是因为维度变多，设计空间也随之变大，因此随意的放大多个维度需要耗费较大的人力来调整，并且也通常会有一个次优的精度和效率。因此作者通过研究实验提出了一种新的缩放方法——**复合缩放方法(compound scaling method)**。\n下图所展示的便是放大神经网络的几种方法：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402162025.png)\n\n在之前的论文中，有的会通过增加网络的**width**即增加卷积核的个数(增加特征矩阵的**channels**)来提升网络的性能(上图b)，有的会通过增加网络的**深度**即**使用更多的层结构**来提升网络的性能(上图c)，有的会通过增加输入网络的分辨率来提升网络的性能(上图d)。而在本篇论文中会同时增加网络的**width**、网络的**深度**以及输入网络的**分辨率**来提升网络的性能(上图e)\n\n但是因为网络结构的缩放并不会改变具体某一层的卷积操作，所以一个良好的基线网络是必须的，作者在论文中也提出了一种新的基线网络结构——**EfficientNet**。\n\n## compound scaling method\n\n### 论文思想\n\n- **Depth（d）**：缩放网络深度是许多卷积网络中最常用的方法。*更深的卷积网络能够捕获到更丰富和复杂的特征，但是更深的网络由于存在**梯度消失**的问题而难以训练。*尽管有一些方法可以解决梯度消失（例如 跨层连接skip connections和批量归一化 batch normalization），但是对于非常深的网络所获得的准确率的增益会减弱。例如ResNet-1000和ResNet-101有着相近的准确率尽管depth相差很大。*下图的中间的曲线图表示用不同的深度系数d缩放模型的准确率曲线，并且表明了对于非常深的网络，准确率的增益会减弱。*\n\n  > The intuition is that deeper ConvNet can capture richer and more complex features, and generalize well on new tasks. However, deeper networks are also more difficult to train due to the vanishing gradient problem\n\n- **Width（w）**：缩放网络宽度对于小规模的网络也是很常用的一种方式。*更宽的网络更能够捕捉到更多细节的特征，也更容易训练。*很宽但很浅的网络结构很难捕捉到更高层次的特征。*下图中左边的曲线图则是作者的不同宽度系数实验结果曲线，当w不断增大的时候，准确率很快就饱和了。*\n\n  > wider networks tend to be able to capture more fine-grained features and are easier to train. However, extremely wide but shallow networks tend to have difficulties in capturing higher level features.\n\n- **Resolution（r）**：使用更高分辨率的图像，网络能够捕获到更细粒度的特模式。增加输入网络的图像分辨率能够潜在得获得更高细粒度的特征模板，但对于非常高的输入分辨率，*准确率的增益也会减小，并且大分辨率图像会增加计算量*。*下图中右边的曲线图则是作者的不同分辨率系数实验结果曲线，对于非常高分辨率的图像，准确率的增益会减弱。（r=1.0表示224x224，r=2.5表示560x560）。*\n\n  > With higher resolution input images, ConvNets can potentially capture more fine-grained patterns. but the accuracy gain diminishes for very high resolutions.\n\n下图展示了在基准**EfficientNetB-0**上分别增加**width**、**depth**以及**resolution**后得到的统计结果。通过下图可以看出大概在Accuracy达到80%时就趋于饱和了。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402203414.png)\n\n通过以上实验得出**结论1：对网络深度、宽度和分辨率中的任一维度进行缩放都可以提高精度，但是当模型非常大时，这种放大的增益都会减弱**。\n\n接着作者又做了一个实验，采用不同的d , r 组合，然后不断改变网络的width就得到了如下图所示的4条曲线，通过分析可以发现在相同的FLOPs下，同时增加d和r的效果最好。\n\n### 复合缩放**Compound Scaling**\n\n> 作者通过实验发现缩放的各个维度并不是独立的。直观上来讲，对于分辨率更高的图像，我们应该增加网络深度，因为需要更大的感受野来帮助捕获更多像素点的类似特征。为了证明这种猜测，作者做了一下相关实验：比较宽度缩放在不同深度和分辨率之下对准确率的影响。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402204530.png)\n\n*通过上图的结果我们可以看到d=2.0，r=1.3时宽度缩放在相同flops下有着更高的准确率。*\n得到**结论2：为了达到更好的准确率和效率，在缩放时平衡网络所有维度至关重要。**\n\n为了方便后续理解，我们先看下论文中通过**NAS**(**Neural Architecture Search**)技术搜索得到的EfficientNetB0的结构，如下图所示，整个网络框架由一系列Stage，$\\widehat{Fi}$表示对应stage的运算操作，$\\widehat{Li}$表示在该stage中重复$\\widehat{Fi}$的次数：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402210952.png)\n\n作者在论文中对整个网络的运算进行抽象：$N(d,\\omega,r) = ^{\\bigodot}_{i=1…s}F_i^{L_i}(X_{<H_iW_iC_i>})$\n\n其中：\n\n- $ ^{\\bigodot}_{i=1…s}$表示连乘运算\n- $F_i$表示一个运算操作(如上图中的**operator**)，那么$F_i^{L_i}$表示在Stage_i中$F_i$运算被重复执行$L_i$次。\n- X表示输入Stage_i的特征矩阵(**input tensor**)\n- $<H_i,W_i,C_i>$表示X的高度，宽度以及**Channels**(**shape**)。\n\n为了探究d,r,w这三个因子对最终准确率的影响，作者将d,r,w加到公式中，我们可以得到抽象化后的优化问题(在指定资源限制下)，其中s.t.代表限制条件：\n\n> Our target is to maximize the model accuracy for any given resource constraints, which can be formulated as an optimization problem:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403140239.png)\n\n其中：\n\n- d用来缩放深度$\\widehat{Li}$\n- r用来缩放分辨率即影响$\\widehat{Hi}$和$\\widehat{Wi}$\n- $\\omega$用来缩放特征矩阵的channel即$\\widehat{Ci}$\n- target_memory为memory限制\n- target_flops为FlOPs限制\n\n然后，作者又提出了一种新的复合缩放方法使用了一个复合系数**ϕ** ，通过这个系数按照以下原则来统一的缩放**网络深度**、**宽度**和**分辨率**：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220402204755.png)\n\n这里：\n\n- FLOPs(理论计算量)与depth的关系是：当depth翻倍，FLOPs也翻倍。\n- FLOPs与width的关系是：当width翻倍(即channel翻倍)，FLOPs会翻4倍，因为卷积层的FLOPs约等于$feature_\\omega \\times feature_h \\times feature_c \\times kernel_\\omega \\times kernel_h \\times kernel_{number}$(假设输入输出特征矩阵的高宽不变)，当width翻倍，输入特征矩阵的channels($feature_c$)和输出特征矩阵的channel或卷积核的个数($kernel_{number}$)都会翻倍，所以FLOPs会翻4倍。\n- FLOPs与resolution的关系是：当resolution翻倍，FLOPs也会翻4倍，和上面类似因为特征矩阵的宽度$feature_\\omega$和特征矩阵的高度$feature_h$都会翻倍。\n\n所以总的FLOPs倍率可以近似用($({\\alpha \\cdot \\beta^2 \\cdot \\gamma^2})^\\phi$)来表示，当限制${\\alpha \\cdot \\beta^2 \\cdot \\gamma^2} \\approx 2$，对于任意一个$\\phi$而言FLOPs想当增加了$2^\\phi$倍\n\n接下来作者在基准网络EfficientNetB-0上使用NAS来搜索$\\alpha,\\beta,\\gamma$这三个参数。\n\n1. 首先固定$\\phi = 1$，并基于上面给出的公式(2)和(3)进行搜索，作者发现对于EfficientNetB-0最佳参数为\n2. 接着固定$\\alpha=1.2,\\beta=1.1,\\gamma=1.15$，在EfficientNetB-0的基础上使用不同的$\\phi$分别得到EfficientNetB-1至EfficientNetB-7。\n\n需要注意的是，对于不同的基准网络搜索出的α , β , γ 也不一定相同。还需要注意的是，在原论文中，作者也说了，如果直接在大模型上去搜索α , β , γ 可能获得更好的结果，但是在较大的模型中搜索成本太大，所以这篇文章就在比较小的EfficientNetB-0模型上进行搜索的。\n\n> Notably, it is possible to achieve even better performance by searching for α, β, γ directly around a large model, but the search cost becomes prohibitively more expensive on larger models. Our method solves this issue by only doing search once on the small baseline network (step 1), and then use the same scaling coefficients for all other models (step 2).\n\n## 网络详细结构\n\n下表为EfficientNet-B0的网络框架（B1-B7就是在B0的基础上修改**Resolution**，**Channels**以及**Layers**），可以看出网络总共分成了9个**Stage**，第一个Stage就是一个卷积核大小为3x3步距为2的普通卷积层（包含BN和激活函数**Swish**），**Stage2～Stage8**都是在重复堆叠**MBConv**结构（最后一列的**Layers**表示该**Stage**重复**MBConv**结构多少次），而**Stage9**由一个普通的1x1的卷积层（包含BN和激活函数**Swish**）一个平均池化层和一个全连接层组成。表格中每个**MBConv**后会跟一个数字1或6，这里的1或6就是倍率因子n即**MBConv**中第一个1x1的卷积层会将输入特征矩阵的**channels**扩充为n倍，其中k3x3或k5x5表示**MBConv**中**Depthwise Conv**所采用的卷积核大小。**Channels**表示通过该**Stage**后输出特征矩阵的Channels。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403144813.png)\n\n### MBConv\n\nMBConv其实就是MobileNetV3网络中的InvertedResidualBlock，但也有些许区别。一个是采用的激活函数不一样(EfficientNet的MBConv中使用的都是Swish激活函数)，另一个是在每个MBConv中都加入了SE(Squeeze-and-Excitation)模块。\n\n以下结构图为B站UP主[霹雳吧啦Wz](https://space.bilibili.com/18161609)绘制的MBConv结构。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403145314.png)\n\n如图所示，**MBConv**结构主要由一个1x1的普通卷积（升维作用，包含**BN**和**Swish**），一个**kxk**的**Depthwise Conv**卷积（包含**BN**和**Swish**）**k**的具体值可看**EfficientNet-B0**的网络框架主要有3x3和5x5两种情况，一个SE模块，一个1x1的普通卷积（降维作用，包含BN），一个**Droupout**层构成。搭建过程中还需要注意几点：\n\n- 第一个升维的**1x1**卷积层，它的卷积核个数是输入特征矩阵**channel**的n倍， $n \\in \\left\\{1, 6\\right\\}$(n∈{1,6})。\n- 当n = 1时，不要第一个升维的1x1卷积层，即**Stage2**中的**MBConv**结构都没有第一个升维的1x1卷积层（这和**MobileNetV3**网络类似）。\n- 关于**shortcut**连接，仅当输入**MBConv**结构的特征矩阵与输出的特征矩阵**shape**相同时才存在（代码中可通过$stride==1 and inputc_channels==output_channels$条件来判断）。\n- SE模块如下所示，由==一个全局平均池化==，==两个全连接层==组成。第一个全连接层的节点个数是输入该MBConv特征矩阵channels的$\\frac{1}{4}$ 且使用Swish激活函数。第二个全连接层的节点个数等于Depthwise Conv层输出的特征矩阵channels，且使用Sigmoid激活函数。\n- Dropout层的dropout_rate在tensorflow的keras源码中对应的是drop_connect_rate后面会细讲（注意，在源码实现中只有使用shortcut的时候才有Dropout层）。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403153958.png)\n\n### EfficientNet(B0-B7)参数\n\n|     Model      | input_size | width_coefficient | depth_coefficient | drop_connect_rate | dropout_rate |\n| :------------: | :--------: | :---------------: | :---------------: | :---------------: | :----------: |\n| EfficientNetB0 |  224x224   |        1.0        |        1.0        |        0.2        |     0.2      |\n| EfficientNetB1 |  240×240   |        1.0        |        1.1        |        0.2        |     0.2      |\n| EfficientNetB2 |  260x260   |        1.1        |        1.2        |        0.2        |     0.3      |\n| EfficientNetB3 |  300x300   |        1.2        |        1.4        |        0.2        |     0.3      |\n| EfficientNetB4 |  380x380   |        1.4        |        1.8        |        0.2        |     0.4      |\n| EfficientNetB5 |  456x456   |        1.6        |        2.2        |        0.2        |     0.4      |\n| EfficientNetB6 |  528x528   |        1.8        |        2.6        |        0.2        |     0.5      |\n| EfficientNetB7 |  600x600   |        2.0        |        3.1        |        0.2        |     0.5      |\n\n- **input_size**代表训练网络时输入网络的图像大小\n- **width_coefficient**代表**channel**维度上的倍率因子，比如在 **EfficientNetB0**中**Stage1**的3x3卷积层所使用的卷积核个数是32，那么在B6中就是32 × 1.8 = 57.6 32 \\times 1.8=57.632×1.8=57.6接着取整到离它最近的8的整数倍即56，其它**Stage**同理。\n- **depth_coefficient**代表depth维度上的倍率因子（仅针对**Stage2**到**Stage8**），比如在**EfficientNetB0**中**Stage7**的 $\\widehat L_i=4$，那么在B6中就是4 × 2.6 = 10.4 4 \\times 2.6=10.44×2.6=10.4接着向上取整即11。\n- **drop_connect_rate**是在**MBConv**结构中**dropout**层使用的**drop_rate**，在官方keras模块的实现中**MBConv**结构的**drop_rate**是从0递增到**drop_connect_rate**的（具体实现可以看下官方源码，注意，在源码实现中只有使用**shortcut**的时候才有**Dropout**层）。还需要注意的是，这里的**Dropout**层是**Stochastic Depth**，即会随机丢掉整个**block**的主分支（只剩捷径分支，相当于直接跳过了这个**block**）也可以理解为减少了网络的深度。\n- **dropout_rate**是最后一个全连接层前的**dropout**层（在**stage9**的Pooling与FC之间）的**dropout_rate**。\n\n最后是原论文中关于EfficientNet与当时主流网络的性能参数对比:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220403155412.png)\n","tags":["计算机视觉","EfficientNet"],"categories":["文献阅读"]},{"title":"文件夹指定软件打开","url":"/2022/04/03/文件夹指定软件打开/","content":"\n# 鼠标右键属性相关\n\n## 右键文件夹指定软件打开\n\nGitHub上下载的代码通常需要对应的编译器打开项目文件，比如我们通常要指定用pycharm或webcharm打开相关文件夹。\n\n教程如下：\n\n1. win+R，输入regedit打开注册表编辑器。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408145825.png)\n\n2.在路径HKEY_CURRENT_USER\\SOFTWARE\\Classes\\Directory\\shell下新建项，命名为Open Folder as XXX Project。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408150134.png)\n\n3. 在此文件夹右击新建字符串值Icon,属性为 软件的本地安装路径\\xxx.exe\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408150400.png)\n\n4. 在Open Folder as XXX Project文件夹下新建项Command,其值修改为 “软件的本地安装路径\\xxx.exe” “%1”\n\n   ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408150617.png)\n\n5. 设置完成，选项即可出现在您右击文件夹之后！\n\n## 删除不需要的鼠标右键属性\n\n1. 打开注册表\n\n   win+R -> input “regedit”\n\n2. 在地址栏输入“HKEY_CLASSES_ROOT\\Directory\\shell”，并回车进入\n3. 进入ContextMenuHandlers项后，可以根据个人的需求，把鼠标右键菜单中不需要项目删掉，或者保留new项，其他的全部删掉即可删除鼠标右键菜单选项。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220408151748.png)","categories":["使用技巧"]},{"title":"远处的拉莫","url":"/2022/04/03/远处的拉莫/","content":"\n**“远处的拉莫在看着你，那是你的神。**\n\n**你存在的每一秒，被痛苦占据的每一秒，他都在看着你。**\n\n**有时候你可以感觉到他，但一生只有那么几个瞬间。”**\n\n# 2022.4.3\nP54\n重要的是，我知道痛苦其他的样貌，它们像是白色的羽毛，像是水面上的烟火，像是雪山的幽灵，它们是一切不可诉说的、静默在永恒里的、被掩埋着的枯萎、灰败和消亡。\n\nP62\n很久之前，我就告诉自己不能被任何所击垮，只要有一瞬间的崩塌，便会迅速瓦解。\n\nP68\n人就是这样，要有比他们自身更糟的东西在上方控制着他们，才能不处于濒死的状态。\n\nP69\n有一种叫作塌陷的感受，几乎每次入睡时都会溢出来，最开始是胸腔，然后是腹部、膝盖，向上抵达脊椎，向下抵达生殖器，最后是四肢的末端，全部塌陷，然后进入睡眠。\n\nP74\n那些聪明人，从古至今追求着智慧的人，他们令文明得到进化，逐利使文明扩张，扩张代表着侵蚀、封锁、屠杀，然而仍有奔向智慧的人，一切糟糕的结果由他们而起，他们进化着文明的同时，让更野蛮的力量得以无限扩张。这从来都不是双刃剑，一直都是通向此刻的必然。\n\nP79\n上一代人总是会不遗余力地压制下一代，这与进化的意志相反。在我吃这橘子的那个夜晚，我的朋友说。他的女朋友坐在一侧，腿放在他手腕上。\n你被压制什么了？我说\n我被不剥夺了很多，也对抗不了，他们扣押了我所有的版权。就像现在，我把这些称作邪恶，但可能二十年后，我也会这么干。我剥夺年轻人，压制他们，利用他们，可能只是因为他们拥有的东西令我心烦。他揉着那个女人的脚，我能看出这中间有种色情的意味，情侣喜欢在公开场合以不起眼的方式调情，这种色情使他们有乐趣。如果没有旁观者，充斥在这里的只剩下乏味。\n我能理解你，就你所说的这种邪恶，人们会在不同的年龄以不同的方式发作出来。我说。\n童年时是什么？他说。\n杀戮。\n杀戮？他的女朋友抽回了脚。\n我总觉得，虽然所有阶段都会产生杀戮，但杀戮始于童年，你身边更为强大的个体告诉你杀戮是可怕的。某个儿童敲死一片蚂蚁，这被认为是不好的，但这个不好，只是因为你屈服于周围的强大，毕竟那段日子，你没有选择任何事物的权利。\n所以呢？朋友说。\n所以杀戮被掩埋住，在一些年代以别的方式发泄出来。像你所说的，在一些年代你被剥夺了，在另一些年代以直接的杀戮呈现。\n哈哈，那青年呢？女人问。\n侵占。\n我没觉得自己在侵占什么啊。朋友说。\n让自己覆盖更多的事物，侵占所有可以看得到的。我仔细想想，我觉得这个民族的自负跟这个有关系。这个民族，还停留在青年人的阶段，也就是一个侵占的时期，必然会认为自己无所不能。\n那中年呢？\n我还不知道，但我观察到，中年已经开始向毁灭过渡了，不计任何后果地令世界丑陋下去。\n你这样看待周遭，因此活得糟糕透顶。朋友说。\n我无论怎么看待，这都是注定的。你能想象十几年之后的样子吗？我们还能坐在这里，你递给我两个橘子，你虚伪地跟我说起这漫长的友谊，你讲起我们的过去那看起来好玩的事情。但到了某些情况下，即便是很脆弱的情况，我认为所有人也会毫不犹豫地获得那个强大的本能。\n十几年后，我们已经结婚很多年，有了两个孩子，会告诫他们不能变成你这样。女人笑着说。\n他们的狗过来咬着我的拖鞋。\n\nP83\n愚笨是因为安逸。\n\nP84\n“我们无法触碰，亦不可调和”\n\nP85\n我们，与什么事物调和过呢？我抬起自己的手，看着那条渐变如山脊的伤痕，上面沾满了尘土，我与自身的伤口都无法调和。\n\nP92\n“其实听别人的故事，不会让你感受到什么。”\n\n# 2022.4.9\nP100\n实际上，喝酒这件事，不需要破产或者家破人亡，哪怕摔伤了膝盖，或者一根手指不小心被划伤，都可以喝酒。\n\nP136\n“爱情有一个衰变期，如果之前没有变化的话，便会走向终结。”\n\nP147\n我喜欢庸俗的女人，以前还没有发现，现在我很确定了。比如归结到容貌、性格，或者其他乱七八糟的，根本不是。我只是喜欢庸俗的女人。她们考虑事情的角度差不多，有时候她们很聪明，但不会超过算清五毛钱的账。我很鄙视自己这一点，但不能控制。一开始我总以为是什么特别神秘的缘由，最后结果都是，我发现我们的生活就是坐在那，她可以做一晚上毛线球，我就在一旁刷手机，从下午到凌晨，之后我会打开窗户，如果有啤酒我也会开一瓶，站在窗前就好像发现了什么可悲的事情一样。其实一直如此，可能我三岁时就已经这样了，喜欢庸俗的女人。我们互相讲着社交网络上看来的笑话，就跟是自己身上发生的一样，再开怀大笑。有时我能笑得哭出来，但是没办法，我好像只能做这些事。比如她洗澡时会放三五年前的流行音乐，我听了也会很伤感，眼前浮现一个涂着星空眼影的过气女歌手，她一开口台下的人就开始哭，我听了也想哭，但其实我没什么好哭的。等她洗完澡走出来，我看着她，目光里都是，天啊这是世上最漂亮的女人了。就是这样的。起码今天就是这样的。”\n\n# 2022.4.10\nP173\n我所珍藏的东西，总是在触碰的时候就轻易瓦解成粉尘，这便是一种可以称为陷阱的东西。\n\nP184\n我越是去经历和感受些什么，她就会越来越远离我。\n\nP187\n也就是说，那些十几年前所期待的——虽然我并不知道在期待什么——都没有发生，构成我生活的每一部分都原封不动地矗立在这里。","tags":["阅读","文摘"],"categories":["解体与救赎"]},{"title":"Yolov5学习笔记9|YOLOv5-Face|改进原理解读与论文复现","url":"/2022/03/30/Yolov5学习笔记9/","content":"\n# Yolov5改进|YOLOv5-Face|改进原理解读与论文复现\n\n## YOLOv5Face的设计目标和主要贡献\n\n### 设计目标\n\nYOLOv5Face针对人脸检测的对YOLOv5进行了再设计和修改，考虑到大人脸、小人脸、Landmark监督等不同的复杂性和应用。YOLOv5Face的目标是为不同的应用程序提供一个模型组合，从非常复杂的应用程序到非常简单的应用程序，以在嵌入式或移动设备上获得性能和速度的最佳权衡。\n\n### 主要贡献\n\n1. 重新设计了YOLOV5来作为一个人脸检测器，并称之为YOLOv5Face。对网络进行了关键的修改，以提高平均平均精度(mAP)和速度方面的性能；\n2. 设计了一系列不同规模的模型，从大型模型到中型模型，再到超小模型，以满足不同应用中的需要。除了在YOLOv5中使用的Backbone外，还实现了一个基于ShuffleNetV2的Backbone，它为移动设备提供了最先进的性能和快速的速度；\n3. 在WiderFace数据集上评估了YOLOv5Face模型。在VGA分辨率的图像上，几乎所有的模型都达到了SOTA性能和速度。这也证明了前面的结论，不需要重新设计一个人脸检测器，因为YOLO5就可以完成它。\n\n## YOLOv5-Face的结构\n\n###  YOLOv5-Face模型架构\n\n![YOLOv5-Face架构图](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329211411.png)\n\n> YOLOv5Face是以YOLOv5作为Baseline来进行改进和再设计以适应人脸检测。这里主要是检测小脸和大脸的修改。\n\nYOLO5人脸检测器的网络架构如图1所示。它由Backbone、Neck和Head组成，描述了整体的网络体系结构。在YOLOv5中，使用了CSPNet Backbone。在Neck中使用了SPP和PAN来融合这些特征。在Head中也都使用了回归和分类。\n\n#### CBS Block\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329211622.png)\n\n在上图中重新定义了一个CBS Block，它由Conv、BN和SiLU激活函数组成。但其实架构和Yolov5的一样。\n\n对应的代码如下：\n\n```python\nclass Conv(nn.Module):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super(Conv, self).__init__()\n        # 卷积层\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        # BN层\n        self.bn = nn.BatchNorm2d(c2)\n        # SiLU激活层\n        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n    def fuseforward(self, x):\n        return self.act(self.conv(x))\n```\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329212205.png)\n\n\n\n在上图中显示了Head的输出标签，其中包括边界框(bbox)、置信度(conf)、分类(cls)和5-Point Landmarks。这些Landmarks是对YOLOv5的改进点，使其成为一个具有Landmarks输出的人脸检测器。如果没有Landmarks，最后一个向量的长度应该是6而不是16。\n\n**请注意，P3中的输出尺寸80×80×16，P4中的40×40×16，P5中的20×20×16，可选P6中的10×10×16为每个Anchor。实际的尺寸应该乘以Anchor的数量。**\n\n#### Stem Block\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220519.png)\n\n上图为stem，它同于取代Yolov5中原来的Focus层(实际上在yolov5-v5.0之后就没有Focus层了)。在Yolov5中引入Stem模块用于人脸检测时Yolov5-Face创新之一。\n\n```python\nclass StemBlock(nn.Module):\n    def __init__(self, c1, c2, k=3, s=2, p=None, g=1, act=True):\n        super(StemBlock, self).__init__()\n        # 3×3卷积\n        self.stem_1 = Conv(c1, c2, k, s, p, g, act)\n        # 1×1卷积\n        self.stem_2a = Conv(c2, c2 // 2, 1, 1, 0)\n        # 3×3卷积\n        self.stem_2b = Conv(c2 // 2, c2, 3, 2, 1)\n        # 最大池化层\n        self.stem_2p = nn.MaxPool2d(kernel_size=2,stride=2,ceil_mode=True)\n        # 1×1卷积\n        self.stem_3 = Conv(c2 * 2, c2, 1, 1, 0)\n\n    def forward(self, x):\n        stem_1_out  = self.stem_1(x)\n        stem_2a_out = self.stem_2a(stem_1_out)\n        stem_2b_out = self.stem_2b(stem_2a_out)\n        stem_2p_out = self.stem_2p(stem_1_out)\n        out = self.stem_3(torch.cat((stem_2b_out,stem_2p_out),1))\n        return out\n```\n\n用Stem模块替代网络中原有的Focus模块，**提高了网络的泛化能力，降低了计算复杂度，同时性能也没有下降**。\n\n```python\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, StemBlock, [64, 3, 2]],  # 0-P1/2\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],      # 2-P3/8\n   [-1, 9, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],      # 4-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],     # 6-P5/32\n   [-1, 1, SPP, [1024, [3,5,7]]],\n   [-1, 3, C3, [1024, False]],      # 8\n  ]\n```\n\nStem模块的图示中虽然都是用的CBS，但是看代码可以看出来第2个和第4个CBS是1×1卷积，第1个和第3个CBS是3×3，stride=2的卷积。配合yaml文件可以看到stem以后图像大小由640×640变成了160×160。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220426.png)\n\n在上图中，显示了一个CSP Block(C3)。CSP Block的设计灵感来自于DenseNet。但是，不是在一些CNN层之后添加完整的输入和输出，输入被分成 2 部分。其中一半通过一个CBS Block，即一些Bottleneck Blocks，另一半是经过Conv层进行计算：\n\n```python\nclass C3(nn.Module):\n    # CSP Bottleneck with 3 convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super(C3, self).__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n\n    def forward(self, x):\n        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n```\n\n#### Bottleneck Block\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220241.png)\n\n上图即为C3模块中的Bottleneck层。\n\n```python\nclass Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super(Bottleneck, self).__init__()\n        c_ = int(c2 * e)  # hidden channels\n        #第1个CBS模块\n        self.cv1 = Conv(c1, c_, 1, 1)\n        #第2个CBS模块\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        #元素add操作\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n```\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220329220924.png)\n\n上图为SPP Block、。YOLOv5Face在这个Block中把YOLOv5中的13×13,9×9,5×5的kernel size被修改为7×7,5×5,3×3，这个改进更适用于人脸检测并提高了人脸检测的精度。\n\n```python\nclass SPP(nn.Module):\n    # 这里主要是讲YOLOv5中的kernel=(5,7,13)修改为(3, 5, 7)\n    def __init__(self, c1, c2, k=(3, 5, 7)):\n        super(SPP, self).__init__()\n        c_ = c1 // 2  # hidden channels\n        # 对应第1个CBS Block\n        self.conv1 = Conv(c1, c_, 1, 1)\n        # 对应第2个 cat后的 CBS Block\n        self.conv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n        # ModuleList=[3×3 MaxPool2d,5×5 MaxPool2d,7×7 MaxPool2d]\n        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return self.conv2(torch.cat([x] + [m(x) for m in self.m], 1))\n```\n\n同时，YOLOv5Face添加一个stride=64的P6输出块，P6可以提高对大人脸的检测性能。（之前的人脸检测模型大多关注提高小人脸的检测性能，这里作者关注了大人脸的检测效果，提高大人脸的检测性能来提升模型整体的检测性能）。P6的特征图大小为10x10。\n\n> 同时，YOLOv5Face添加一个stride=64的P6输出块，P6可以提高对大人脸的检测性能。（之前的人脸检测模型大多关注提高小人脸的检测性能，这里作者关注了大人脸的检测效果，提高大人脸的检测性能来提升模型整体的检测性能）。P6的特征图大小为10x10。\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n###  输入改进\n\nYOLOv5Face作者发现一些目标检测的数据增广方法并不适合用在人脸检测中，包括上下翻转和Mosaic数据增广。**删除上下翻转可以提高模型性能**。**对小人脸进行Mosaic数据增广反而会降低模型性能**，但是**对中尺度和大尺度人脸进行Mosaic可以提高性能**。**随机裁剪有助于提高性能**。\n\n> 这里主要还是COCO数据集和WiderFace数据集尺度有差异，WiderFace数据集小尺度数据相对较多。\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n### Landmark回归\n\nLandmark是人脸的重要特征。它们可以用于人脸比对、人脸识别、面部表情分析、年龄分析等任务。传统Landmark由68个点组成。它们被简化为5点时，这5点Landmark就被广泛应用于面部识别。人脸标识的质量直接影响人脸对齐和人脸识别的质量。\n\n一般的物体检测器不包括Landmark。可以直接将其添加为回归Head。因此，作者将它添加到YOLO5Face中。Landmark输出将用于对齐人脸图像，然后将其发送到人脸识别网络。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330085638.png)\n\n用于Landmark回归的一般损失函数为L2、L1或smooth-L1。MTCNN使用的就是L2损失函数。然而，作者发现这些损失函数对小的误差并不敏感。为了克服这个问题，提出了Wing loss:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330085731.png)\n\nw: 正数w将非线性部分的范围限制在[-w,w]之间；\n\n$\\varepsilon $: 约束非线性区域的曲率，并且$C=\\omega-\\omega ln(1+\\frac{x}{\\varepsilon)}$是一个常数，可以平滑的连接分段的线性和非线性部分。$\\varepsilon$的取值是一个很小的数值，因为它会使网络训练变得不稳定，并且会因为很小的误差导致梯度爆炸问题。\n\n> 实际上，的Wing loss函数的非线性部分只是简单地采用ln(x)在[$\\frac{\\varepsilon}{\\omega},1+\\frac{\\varepsilon}{\\omega}$]之间的曲线，并沿X轴和Y轴将其缩放比例为w。另外，沿y轴应用平移以使wing(0)=0，并在损失函数上施加连续性。\n\n#### landmark的获取：\n\n```python\n#landmarks\nlks = t[:,6:14]\nlks_mask = torch.where(lks < 0, torch.full_like(lks, 0.), torch.full_like(lks, 1.0))\n#应该是关键点的坐标除以anch的宽高才对，便于模型学习。使用gwh会导致不同关键点的编码不同，没有统一的参考标准\nlks[:, [0, 1]] = (lks[:, [0, 1]] - gij)\nlks[:, [2, 3]] = (lks[:, [2, 3]] - gij)\nlks[:, [4, 5]] = (lks[:, [4, 5]] - gij)\nlks[:, [6, 7]] = (lks[:, [6, 7]] - gij)\n```\n\nWing Loss的计算如下：\n\n```python\nclass WingLoss(nn.Module):\n    def __init__(self, w=10, e=2):\n        super(WingLoss, self).__init__()\n        # https://arxiv.org/pdf/1711.06753v4.pdf   Figure 5\n        self.w = w\n        self.e = e\n        self.C = self.w - self.w * np.log(1 + self.w / self.e)\n \n    def forward(self, x, t, sigma=1):  #这里的x，t分别对应之后的pret，truel\n        weight = torch.ones_like(t) #返回一个大小为1的张量，大小与t相同\n        weight[torch.where(t==-1)] = 0\n        diff = weight * (x - t)\n        abs_diff = diff.abs()\n        flag = (abs_diff.data < self.w).float()\n        y = flag * self.w * torch.log(1 + abs_diff / self.e) + (1 - flag) * (abs_diff - self.C) #全是0，1\n        return y.sum()\n \nclass LandmarksLoss(nn.Module):\n    # BCEwithLogitLoss() with reduced missing label effects.\n    def __init__(self, alpha=1.0):\n        super(LandmarksLoss, self).__init__()\n        self.loss_fcn = WingLoss()#nn.SmoothL1Loss(reduction='sum')\n        self.alpha = alpha\n \n    def forward(self, pred, truel, mask): #预测的，真实的 600（原来为62*10）(推测是去掉了那些没有标注的值)\n        loss = self.loss_fcn(pred*mask, truel*mask)  #一个值（tensor）\n        return loss / (torch.sum(mask) + 10e-14)\n```\n\n#### **分析比较L1，L2和Smooth L1损失函数**\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092332.png)\n\n其中s是人脸关键点的ground-truth,函数f(x)就等价于：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092354.png)\n\n损失函数对x的导数分别为:\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092423.png)\n\nL2损失函数，**当x增大时L2 loss对x的导数也增大，这就导致训练初期，预测值与ground-truth差异过大时，损失函数对预测值的梯度十分大，导致训练不稳定**。\n\nL1 loss的导数为常数，在训练后期，预测值与ground-truth差异很小时， 损失对预测值的导数的绝对值仍然为1，此时学习率(learning rate)如果不变，**损失函数将在稳定值附近波动，难以继续收敛达到更高精度**。\n\nsmooth L1损失函数，**在x较小时，对x的梯度也会变小，而在x很大时，对x的梯度的绝对值达到上限 1，也不会太大以至于破坏网络参数。smooth L1完美地避开了L1和L2损失的缺陷**。\n\n此外，根据fast rcnn的说法，\"… L1 loss that is less sensitive to outliers than the L2 loss used in R-CNN and SPPnet.\" 也就是**smooth L1让loss对于离群点更加鲁棒，即相比于L2损失函数，其对离群点、异常值（outlier）不敏感，梯度变化相对更小，训练时不容易跑飞**。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092522.png)\n\n上图描绘了这些损失函数的曲线图。需要注意的是，Smoolth L1损失是Huber损失的一种特殊情况，L2损失函数在人脸关键点检测中被广泛应用，然而，L2损失对异常值很敏感。\n\n#### 为什么是Wing Loss？\n\n上一部分中分析的所有损失函数在出现较大误差时表现良好。这说明神经网络的训练应更多地关注具有小或中误差的样本。为了实现此目标，提出了一种新的损失函数，即基于CNN的面部Landmark定位的Wing Loss。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220330092732.png)\n\n> 当NME在0.04的时候，测试数据比例已经接近1了，所以在0.04到0.05这一段，也就是所谓的large errros段，并没有分布更多的数据，说明各损失函数在large errors段都表现很好。\n>\n> 模型表现不一致的地方就在于small errors和medium errors段，例如，在NME为0.02的地方画一根竖线，相差甚远的。因此作者提出训练过程中应该更多关注samll or medium range errros样本。\n\n可以使用ln x来增强小误差的影响，它的梯度是$\\frac{1}{x}$,对于接近0的值就会越大,optimal step size为$x^2$，这样gradient就由small errors“主导”，step size由large errors“主导”。这样可以恢复不同大小误差之间的平衡。\n\n但是，为了防止在可能的错误方向上进行较大的更新步骤，重要的是不要过度补偿较小的定位错误的影响。这可以通过选择具有正偏移量的对数函数来实现。\n\n但是这种类型的损失函数适用于处理相对较小的定位误差。在wild人脸关键点检测中，可能会处理极端姿势，这些姿势最初的定位误差可能非常大，在这种情况下，损失函数应促进从这些大错误中快速恢复。这表明损失函数的行为应更像L1或L2。由于L2对异常值敏感，因此选择了L1。\n\n所以，对于小误差，它应该表现为具有偏移量的对数函数，而对于大误差，则应表现为L1。因此复合损失函数Wing Loss就诞生了。\n\n#### Yolov5-Face的后处理NMS\n\n其实本质上没有改变，这里仅仅给出对比的代码。\n\nYolov5的NMS代码如下：\n\n```python\ndef non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, labels=()):\n    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n    Returns:\n         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n    \"\"\"\n \n    nc = prediction.shape[2] -5  # number of classes\n```\n\nYolov5-Face的NMS代码如下：\n\n```\ndef non_max_suppression_face(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, labels=()):\n    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n    Returns:\n         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n    \"\"\"\n    # 不同之处\n    nc = prediction.shape[2] - 15  # number of classes\n```\n\n## 论文复现\n\n- 源码地址: https://github.com/deepcam-cn/yolov5-face\n\n- widerface数据集: https://drive.google.com/file/d/1tU_IjyOwGQfGNUvZGwWWM4SwxKp2PUQ8/view?usp=sharing\n\n> 下载后，解压缩位置放到yolov5-face-master项目里data文件夹下的widerface文件夹下。\n\n- 运行train2yolo.py和val2yolo.py\n\n> 把数据集转成yolo的训练格式，\n\n- 运行train.py\n\n## OpenCV-C++部署\n\n### 参数配置\n\n该部分主要是输入输出尺寸、Anchor以及Strides设置等。\n\n代码如下:\n\n```python\n const float anchors[3][6] = { {4,5,  8,10,  13,16}, \n                               {23,29,  43,55,  73,105},\n                               {146,217,  231,300,  335,433} };\n const float stride[3] = { 8.0, 16.0, 32.0 };\n const int inpWidth = 640;\n const int inpHeight = 640;\n float confThreshold;\n float nmsThreshold;\n float objThreshold;\n```\n\n### 模型加载以及Sigmoid的定义\n\n该部分主要设置ONNX模型的加载。\n\n```python\nYOLO::YOLO(Net_config config)\n{\n cout << \"Net use \" << config.netname << endl;\n this->confThreshold = config.confThreshold;\n this->nmsThreshold = config.nmsThreshold;\n this->objThreshold = config.objThreshold;\n strcpy_s(this->netname, config.netname.c_str());\n\n string modelFile = this->netname;\n modelFile += \"-face.onnx\";\n this->net = readNet(modelFile);\n}\n\nvoid YOLO::sigmoid(Mat* out, int length)\n{\n float* pdata = (float*)(out->data);\n int i = 0; \n for (i = 0; i < length; i++)\n {\n  pdata[i] = 1.0 / (1 + expf(-pdata[i]));\n }\n}\n```\n\n### 后处理部分\n\n这里对坐标的处理和Yolov5保持一致，但是由于多出来的Landmark，所以也多出了这一部分的处理:\n\n```python\n  if (box_score > this->objThreshold)\n     {\n      // 该部分与yolov5的保持一致\n      float face_score = sigmoid_x(pdata[15]);\n      float cx = (sigmoid_x(pdata[0]) * 2.f - 0.5f + j) * this->stride[n];  ///cx\n      float cy = (sigmoid_x(pdata[1]) * 2.f - 0.5f + i) * this->stride[n];   ///cy\n      float w = powf(sigmoid_x(pdata[2]) * 2.f, 2.f) * anchor_w;   ///w\n      float h = powf(sigmoid_x(pdata[3]) * 2.f, 2.f) * anchor_h;  ///h\n\n      int left = (cx - 0.5*w)*ratiow;\n      int top = (cy - 0.5*h)*ratioh;   \n\n      confidences.push_back(face_score);\n      boxes.push_back(Rect(left, top, (int)(w*ratiow), (int)(h*ratioh)));\n      // landmark的处理\n      vector<int> landmark(10);\n      for (k = 5; k < 15; k+=2)\n      {\n       const int ind = k - 5;\n       landmark[ind] = (int)(pdata[k] * anchor_w + j * this->stride[n])*ratiow;\n       landmark[ind + 1] = (int)(pdata[k + 1] * anchor_h + i * this->stride[n])*ratioh;\n      }\n      landmarks.push_back(landmark);\n     }  \n```\n\n## 参考文献\n\n[1].https://github.com/hpc203/yolov5-face-landmarks-opencv-v2\n[2].https://github.com/deepcam-cn/yolov5-face\n[3].YOLO5Face: Why Reinventing a Face Detector\n[4].https://zhuanlan.zhihu.com/p/375966269\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Deep_Learning学习笔记1","url":"/2022/03/25/Deep_Learning学习笔记1/","content":"\n#  Deep_Learning学习笔记——深度神经网络(DNN)实现手写数字识别\n\n深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。\n\n## 从感知机到神经网络\n\n感知机接收多个输入信号，输出一个信号。如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-8309413cef2521d53a8e0f8b82bc0e0c_r.jpg)\n\n输出和输入之间学习到一个线性关系，得到中间输出结果：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-91698dfe8a2cbf280d728fcda98dc6fb_r.jpg)\n$$\nx_i代表人们选择的输入信号，w_i为感知机的内部参数，称为权重，上图中的○通常称为“神经元”或“节点”。\n$$\n感知机的多个输入都有各自的权重，权重越大，对应信号的重要性就越高。\n\n接着是一个神经元激活函数：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-8d892ca795f7ef652b1b63a0f2335052_r.jpg)\n\n当输出1时，称此神经元被激活，其中w是体现输入信号重要性的参数，而偏置b是调整神经元被激活的容易程度的参数。有时将w，b统称为权重。\n\n这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。而神经网络则在感知机的模型上做了**扩展**，总结下主要有三点：\n\n1. **加入了隐藏层**：隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。\n2. **输出层的神经元也可以不止一个输出，可以有多个输出**，，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。\n3. （3）对激活函数做扩展，感知机的激活函数是sign(z) ,虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：\n\n$$\nf(z)=\\frac{1}{1+e^{-z}}\n$$\n\n还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。\n\n## DNN基本结构\n\n神经网络是基于感知机的扩展，而DNN可以理解为有很多隐藏层的神经网络。多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机(Multi-Layer perceptron,MLP)。\n\n从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/v2-1afa0c7d95bea01c038d82deca9d683b_720w.jpg)\n\n层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系![](F:\\Blog\\picture\\equation1-1647673870094.svg) 加上一个激活函数 ![](F:\\Blog\\picture\\equation-1647661061238-1647673900849.svg)。\n\n**首先看线性关系系数w的定义。**以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性关系定义为![](F:\\Blog\\picture\\equation-1647673316596-1647673958389.svg).上标3代表线性系数w所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。你也许会问，为什么不是![](F:\\Blog\\picture\\equation-1647673316631-1647674001886.svg)呢？这主要是为了便于模型用于矩阵表示运算，如果是![](F:\\Blog\\picture\\equation-1647673316631-1647674022987.svg)而每次进行矩阵运算是![](F:\\Blog\\picture\\equation-1647673316732-1647674053223.svg)，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置，即直接为![](F:\\Blog\\picture\\equation-1647673316774-1647674086090.svg)。第i−1层的第k个神经元到第l层的第j个神经元的线性系数定义为![](F:\\Blog\\picture\\equation-1647673316822-1647674145381.svg)。注意，输入层是没有w参数的。\n\n**再看偏倚b的定义。**还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为$b^2_3$.其中，上标2代表所在的层数，下标3代表偏倚所在的神经元的索引。同样的道理，第三层的第一个神经元的偏倚应该表示为$a^3_1$ .输出层是没有偏倚参数的。\n\n## DNN反向传播算法\n\n在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。\n\n### 损失函数\n\n在神经网络中，衡量网络预测结果$y=F(x)$与真实值$y$之间差别的指标称为**损失函数**。损失函数值越小，表示神经网络的预测结果越接近真实值。神经网络进行分类和回归任务时会使用不同的损失函数，下面列出一些常用的分类损失和回归损失。\n\n### 分类损失函数\n\n1. Logistic损失:\n\n$$\nloss(\\widehat{y},y) = \\prod_{i=1}^{N}\\widehat{y}_i^{y_i}·{(1-\\widehat{y}_i)}^{1-y_i}\n$$\n\n2. 负对数似然损失\n\n3. 交叉熵损失\n\n### 回归损失函数\n\n1. 均方误差，也称L2损失\n2. 平均绝对误差，也称L1损失\n3. 均方对数差损失\n4. HUber损失\n5. Log-Cosh损失函数\n\n## DNN反向传播算法过程\n\n由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种，为了简化描述，这里我们以最基本的批量梯度下降法为例来描述反向传播算法。实际上在业界使用最多的是mini-Batch的梯度下降法。区别仅仅在于迭代时训练样本的选择。\n\n输入：总层数**L**，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长**a**，最大迭代次数**max**与停止迭代阈值$\\varepsilon$，输入的m个训练样本\n\n输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量。","tags":["deeplearning","DNN"],"categories":["深度学习"]},{"title":"Paper Reading 2|YoloF：You Only Look One-level Feature","url":"/2022/03/25/文献阅读2/","content":"\n# 文献阅读2-YoloF：You Only Look One-level Feature\n\n> 本文是旷视科技&中科院孙剑团队在单阶段目标检测方面一次突破性的创新，它针对单阶段目标检测中的FPN(特征金字塔)进行了深入的分析并得出：FPN最重要的成分是分而治之的处理思路缓解了优化难问题。针对FPN的多尺度特征、分而治之思想分别提出了Dilated编码器提升特征感受野，Uniform Matching进行不同尺度目标框的匹配；结合所提两种方案得到了本文的YOLOF，在COCO数据集上，所提方案取得了与RetinaNet相当的性能且推理速度快2.5倍；所提方法取得了与YOLOv4相当的性能且推理速度快13%。\n\n## Abstract\n\n本文对单阶段目标检测中的FPN进行了重思考并指出**FPN的成功之处在于它对目标检测优化问题的分而治之解决思路而非多尺度特征融合**。从优化的角度出发，作者引入了另一种方式替换复杂的特征金字塔来解决该优化问题：从而可以**仅仅采用一级特征进行检测**。基于所提简单而有效的解决方案，作者提出了YOLOF(You Only Look One-level Feature)。\n\nYOLOF有两个关键性模块：Dilated Encoder与Uniform Matching，它们对最终的检测带来了显著的性能提升。COCO基准数据集的实验表明了所提YOLOF的有效性，YOLOF取得与RetinaNet-FPN同等的性能，同时快2.5倍；无需transformer层，YOLOF仅需一级特征即可取得与DETR相当的性能，同时训练时间少7倍。以608×608大小的图像作为输入，YOLOF取得了44.3mAP的指标且推理速度为60fps@2080Ti，它比YOLOv4快13%。\n\n本文的贡献主要包含以下几点：\n\n- FPN的关键在于针对稠密目标检测优化问题的“分而治之”解决思路，而非多尺度特征融合；\n- 提出了一种简单而有效的无FPN的基线模型YOLOF，它包含两个关键成分(Dilated Encoder与Uniform Matching)以减轻与FPN的性能差异；\n- COCO数据集上的实验证明了所提方法每个成分的重要性，相比RetinaNet，DETR以及YOLOv4，所提方法取得相当的性能同时具有更快的推理速度。\n\n## Introduction\n\n本文主要针对单阶段检测器中的FPN的两个重要因素进行了研究，作者以RetinaNet为基线，通过解耦**多尺度特征融合**、**分而治之**进行实验设计。作者将FPN视作多输入多输出编码器(MiMo，见下图)，它对骨干网络的多尺度特征进行编码并为后接的解码器提供多尺度特征表达。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325110700.png)\n\n为进行更好的对比分析，作者设计了MiMo、SiMo、MiSo、SiSo等四种类型的解码器，见上图。令人惊艳的是：**SiMo编码器仅仅采用C5特征且不进行特征融合即可取得与MiMo编码器相当的性能**，且性能差异小于1mAP。相反，**MiSo编码器的性能则出现了显著下降**。这个现象意味着：\n\n- C5包含了充分的用于检测不同尺度目标的上下文信息，这促使SiMo编码器可以取得与MiMo相当的结果；\n- 多尺度特征融合带来的收益要远小于分而治之带来的收益，因此多尺度特征融合可能并非FPN最重要的影响因素；相反，分而治之将不同尺度的目标检测进行拆分处理，缓解了优化问题。\n\n##  Cost Analysis of MiMo Encoders\n\n如前所述FPN的成功在于它对于优化问题的解决思路，而非多尺度特征融合。为说明这一点，作者对FPN(即MiMo)进行了简单的分析。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325110918.png)\n\n以RetinaNet-ResNet50为基线方案，作者将检测任务的流水线分解为三个关键部分：骨干网络、Encoder以及Decoder。下图给出了不同部分的Flops对比，可以看到：\n\n- 相比SiMoEncoder，MiMoEncoder带来显著的内存负载问题(134G vs 6G)；\n- 基于MiMoEncoder的检测器推理速度明显要慢于SiSoEncoder检测器(13FPS vs 34FPS)；\n- 这个推理速度的变慢主要是因为高分辨率特征部分的目标检测导致，即C3特征部分。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648177868000.png)\n\n基于上述分析，作者期望寻找另一种解决优化问题的方案，且保持检测器检测、精确、快速。\n\n##  Method\n\n受上述目标驱动以及新发现：C5特征包含足够的信息进行大量目标检测，作者尝试用简单的SiSoEncoder替换复杂的MiCoEncoder。但是，这种简单的替换会带来显著性的性能下降(35.9mAP vs 23.7mAP)，见上图。对于这种情况 ，作者进行了仔细分析得出SiSoEncoder性能下降的两个重要原因：\n\n- The range of scales matching to the C5 feature's receptive field is limited\n- The imbalance problem on positive anchors\n\n接下来，作者将针对这两个问题进行讨论并提出对应的解决方案。\n\n### Limited Scale Range\n\n识别不同尺寸的目标是目标检测的一个根本挑战。一种常见的方案是采用多级特征。在MiMo与SiMoEncoder检测器中，作者构建了不同感受野的多级特征(C3-C7)并在匹配尺度上进行目标检测。然而，单级特征破坏了上述游戏规则，在SiSoEncoder中仅有一个输出特征。\n\n以下图(a)为例，C5特征感受野仅仅覆盖有限的尺度范围，当目标尺度与感受野尺度不匹配时就导致了检测性能的下降。为使得SiSoEncoder可以检测所有目标，作者需要寻找一种方案生成具有可变感受野的输出特征，以补偿多级特征的缺失。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648177983742.png)\n\n在C5特征的基础上，作者采用堆叠扩张卷积方式提升其感受野。尽管其覆盖的尺度范围可以在一定程度上扩大，但它仍无法覆盖所有的目标尺度。以上图(b)为例，相比图(a)，它的感受野尺度朝着更大尺度进行了整体的偏移。然后，作者对原始尺度范围与扩大后尺度范围通过相加方式进行组合，因此得到了覆盖范围更广的输出特征，见上图(c)。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648178037832.png)\n\n上图给出了采用本文所提SiSoEncoder结构示意图，作者称之为Dilated Encoder。它包含两个主要成分：Prejector与Residual Block。投影层采用1×1卷积，然后采用3×3卷积提取上下文语义信息(作用类似FPN)；然后堆叠四个不同扩张因子的残差模块以生成多感受野的输出特征(覆盖所有的目标尺度)。\n\n### Imbalance Problem on Positive Anchors\n\n正锚点的定义对于目标检测中的优化问题尤其重要。在基于锚点的检测方案中，正锚点的定义策略主要受锚点与真实box之间的IoU决定。在RetinaNet中，如果IoU大于0.5则锚点设为正。作者称之为*Max-IoU matching*。\n\n在MiMoEncoder中，锚点在多级特征上以稠密方式进行预定义，同时按照尺度生成特征级的正锚点。在分而治之的机制下，Max-IoU匹配使得每个尺度下的真实Box可以生成充分数量的正锚点。然而，**当作者采用SiSoEncoder时，锚点的数量会大量的减少(比如从100K减少到5K)**，导致了稀疏锚点。稀疏锚点进一步导致了采用Max-IoU匹配时的不匹配问题。以下图为例，大的目标框包含更多的正锚点，这就导致了正锚点的不平衡问题，进而导致了检测器更多关注于大目标而忽视了小目标。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1648178166896.png)\n\n为解决上述正锚点不平衡问题，作者提出了Uniform Matching策略：**对于每个目标框采用k近邻锚点作为正锚点，这就确保了所有的目标框能够以相同数量的正锚点进行均匀匹配**。正锚点的平衡确保了所有的目标框都参与了训练且贡献相等。在实现方面，参考了Max-IoU匹配，作者对`Uniform matching`中的IoU阈值进行设置以忽略大IoU负锚点和小IoU正锚点。\n\n### YOLOF\n\n基于上述解决方案呢，作者提出了一种快速而直接的单级特征检测框架YOLOF，它由骨干网络、Encoder以及Decoder构成，整体结构如下图所示。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325111843.png)\n\n- BackBone。在所有模型中，作者简单的采用了ResNet与ResNeXt作为骨干网络，所有模型在ImageNet上与训练，输出C5特征该通道数为2048，下采样倍率为32；\n- Encoder。在这部分，作者参考FPN添加了两个投影层，将通道数降到512，然后堆叠四个不同扩张因子的残差模块；\n- Decoder。在这部分，作者采用了RetinaNet的主要设计思路，它包含两个并行的任务相关的Head分别用于分类和回归。作者仅仅添加两个微小改动：(1) 参考DETR中的FFN设计让两个Head的卷积数量不同，回归Head包含4个卷积而分类Head则仅包含两个卷积；(2) 作者参考AutoAssign在回归Head上对每个锚点添加了一个隐式目标预测。\n- Other Detail。正如前面所提到的YOLOF中的预定义锚点是稀疏的，这会导致目标框与锚点之间的匹配质量下降。作者在图像上添加了一个随机移动操作以缓解该问题，同时作者发现这种移动对于最终的分类是有帮助的。\n\n##  Experiments\n\n为说明所提方案的有效性，作者在MS COC数据集上与RetinaNet、DETR、YOLOv4进行了对比。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325113201.png)\n\n上表给出了所提方法与RetineNet在COCO数据集上的性能对比。从中可以看到：\n\n- YOLOF取得了与改进版RetinaNet+相当的性能，同时减少了57%的计算量，推理速度快了2.5倍；\n- 当采用相同骨干网络时，由于仅仅采用C5特征，YOLOF在小目标检测方面要比RetinaNet+弱一些(低3.1)；但在大目标检测方面更优(高3.3)；\n- 当YOLOF采用ResNeXt作为骨干网络时，它可以取得与RetinaNet在小目标检测方面相当的性能且推理速度同样相当。\n- 经由多尺度测试辅助，所提方法取得了47.1mAP的指标，且在小目标方面取得了极具竞争力的性能31.8mAP。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325113018.png)\n\n上图给出了所提方法与DETR的性能对比。从中可以看到：\n\n- YOLOF取得了与DETR相匹配的的性能；\n- 相比DETR，YOLOF可以从更深的网络中收益更多，比如ResNet50时低0.4，在ResNet10时多了0.2；\n- 在小目标检测方面，YOLOF要优于DETR；在大目标检测方面，YOLOF要弱于DETR。\n- 在收敛方面，YOLOF要比DETR快7倍，这使得YOLOF更适合于作为单级特征检测器的基线。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325113409.png)\n\n最后，作者再来看一下所提方法与YOLOv4的性能对比(注：这里采用了与YOLOv4类似的数据增强方法，并采用了三阶段训练方案，同时对骨干网络的最后阶段进行了调整)。从上表作者可以看到：\n\n- YOLOF-DC5取得了比YOLOv4快13%的推理速度，且性能高0.8mAP；\n- YOLOF-DC5在小目标检测方面弱于YOLOv4，而在大目标检测方面显著优于YOLOv4；\n- 这也就意味着：单级检测器具有极大的潜力获得SOTA速度-精度均衡性能。\n\n\n\n\n\n参考文献: [You Only Look One-level Feature](<https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_You_Only_Look_One-Level_Feature_CVPR_2021_paper.pdf>) (点击链接下载PDF)\n\n文献源码:https://github.com/megvii-model/YOLOF","tags":["yolov5","FPN"],"categories":["文献阅读"]},{"title":"FPN网络结构及源码分析","url":"/2022/03/24/Yolov5支线学习之FPN网络结构+源码分析/","content":"\n# FPN网络结构及源码分析\n\n> FPN即Feature Pyramid Networks，特征金字塔。\n\n特征金字塔是<u>*多尺度(muiti-scale)目标检测*</u>领域中的重要组成部分，但是由于此方法对计算和内存的需求，在FPN之前的深度学习任务都刻意回避了这类模型。在文献阅读2中，作者利用深度神经网络固有的多尺度、多层级的金字塔结构，使用一种 **自上而下的侧边连接** 在所有尺度上构建出高级语义特征图，构造了特征金字塔的经典结构。\n\n具体做法是:**把低分辨率、高语义信息的高层特征和高分辨率、低语义信息的低层特征自上而下进行融合，使得所有尺度下的特征都有丰富的语义信息。**\n\n其结构如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325142705.png)\n\n在FPN网络结构中，还有很多图像算法。\n\n## Featurized image pyramid\n\n> 生成不同尺寸的图片，每张图片生成不同的特征，分别进行预测，最后统计所有尺寸的预测结果。\n\n一种比较笨的多尺度方法，对输入图像设置不同的缩放比例实现多尺度。这样可以解决多尺度，但是相当于训练了多个模型（假设要求输入大小固定），即便允许输入大小不固定，但是也增加了存储不同尺度图像的内存空间。\n\n## Single feature map\n\n相当于早期的CNN模型，通过卷积层不断学习图像的高级语义特征。\n\n> 使用神经网络某一层输出的feature map进行预测，一般是网络最后一层feature map（例如Fast R-CNN、Faster R-CNN等）；然而靠近网络输入层的feature map包含粗略的位置信息，导致预测的目标狂bbox不准确，靠近最后网络最后一层的feature map会忽略小物体信息。\n\n## Pyramidal feature hierarchy\n\n> 使用不同层次的金字塔层feature map进行预测。SSD就是采用这种多尺度特征融合方法，从网络不同层抽取不同尺寸的特征做预测，没有增加额外的计算量。但是SSD没有使用足够底层的特征，SSD使用最底层的特征是VGG的conv4_3。\n\nSSD较早尝试了使用CNN金字塔形的层级特征，重用了前向过程计算出的多尺度特征图，因此这种形式是不消耗额外的资源的。但是SSD为了避免使用low-level的特征，放弃了浅层的特征图信息，直接从conv4_3开始建立金字塔，并且加入了一些新的层，但是这些低层级、高分辨率的特征图信息对检测小目标是非常重要的。\n\n## Feature Pyramid Network\n\n> 对最底层的特征进行向上采样，并与该底层特征进行融合，得到高分辨率、强语义的特征（即加强了特征的提取）。\n\nFPN为了能够自然地利用CNN层级特征的金字塔形式，同时生成在所有尺度上都具有强语义信息的特征金字塔，便以此为目的设计了top-down结构和lateral connection。这种金字塔结构以此融合具有高分辨率的浅层feature和具有丰富语义信息的深层feature。这样就实现了从单尺度的单张输入图像，快速构建在所有尺度上都具有强语义信息的特征金字塔，同时不产生明显的代价。\n\n上述四种的FPN的网络结构如下图所示：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325142959.png)\n\n## FPN特征金字塔\n\nFPN官方的backbone是ResNet。CNN的前馈计算就是自下而上的路径，特征图经过卷积核计算，通常是越变越小的，也有一些特征层的输出大小和输入大小一样。\n\n特征金字塔网络包括自底向上、自顶向下和横向连接。\n\nresnet特征提取（feature map）：自底向上\n\n最后一层feature map上采样：自顶向下\n\n特征融合：横向连接\n\n横向连接的两层特征在空间尺寸上要相同，主要是为了利用底层的定位细节信息（由于底部的feature map包含更多的定位细节，而顶部的feature map包含更多的目标特征信息）。","tags":["python","FPN"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记8——v6.0源码剖析——Head部分","url":"/2022/03/23/Yolov5学习笔记8/","content":"\n# Yolov5学习笔记8——v6.0源码剖析——Head部分\n\n## Yolov5s网络结构总览\n\n> 要了解head，就不能将其与前两部分割裂开。head中的主体部分就是三个Detect检测器，即利用基于网格的anchor在不同尺度的特征图上进行目标检测的过程。由下面的网络结构图可以很清楚的看出：当输入为640*640时，三个尺度上的特征图分别为：80x80、40x40、20x20。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325191212.png)\n\n## Detect解析\n\n### Detect源码\n\n```python\nclass Detect(nn.Module):\n    stride = None  # strides computed during build\n    onnx_dynamic = False  # ONNX export parameter\n\n    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) // 2  # number of anchors\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\n        self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n        self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n\n    def forward(self, x):\n        z = []  # inference output\n        for i in range(self.nl):\n            x[i] = self.m[i](x[i])  # conv\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n            if not self.training:  # inference\n                if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n                y = x[i].sigmoid()\n                if self.inplace:\n                    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                    y = torch.cat((xy, wh, y[..., 4:]), -1)\n                z.append(y.view(bs, -1, self.no))\n\n        return x if self.training else (torch.cat(z, 1), x)\n\n    def _make_grid(self, nx=20, ny=20, i=0):\n        d = self.anchors[i].device\n        if check_version(torch.__version__, '1.10.0'):  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility\n            yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)], indexing='ij')\n        else:\n            yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)])\n        grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n        anchor_grid = (self.anchors[i].clone() * self.stride[i]) \\\n            .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n        return grid, anchor_grid\n```\n\n### initial部分\n\n```python\ndef __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n    super().__init__()\n    self.nc = nc  # number of classes\n    self.no = nc + 5  # number of outputs per anchor\n    self.nl = len(anchors)  # number of detection layers\n    self.na = len(anchors[0]) // 2  # number of anchors\n    self.grid = [torch.zeros(1)] * self.nl  # init grid\n    self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n    self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n    self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n    self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n```\n\ninitial部分定义了Detect过程中的重要参数\n1. **nc:**类别数目\n2. **no:**每个anchor的输出，包含类别数nc+置信度1+xywh4，故nc+5\n3. **nl:**检测器的个数。以上图为例，我们有3个不同尺度上的检测器：[[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]，故检测器个数为3。\n4. **na:**每个检测器中anchor的数量，个数为3。由于anchor是w h连续排列的，所以需要被2整除。\n5. **grid:**检测器Detect的初始网格\n6. **anchor_grid:**anchor的初始网格\n7. **m：**每个检测器的最终输出，即检测器中anchor的输出no×anchor的个数nl。打印出来很好理解（60是因为我的数据集nc为15，coco是80）：\n\n### forward\n\n```python\ndef forward(self, x):\n    z = []  # inference output\n    for i in range(self.nl):\n        x[i] = self.m[i](x[i])  # conv\n        bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n        x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n        if not self.training:  # inference\n            if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n            y = x[i].sigmoid()\n            if self.inplace:\n                y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n            else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                y = torch.cat((xy, wh, y[..., 4:]), -1)\n            z.append(y.view(bs, -1, self.no))\n\n    return x if self.training else (torch.cat(z, 1), x)\n```\n\n在forward操作中，网络接收3个不同尺度的特征图，分别为：128×80×80、256×40×40、512×20×20\n\n```python\n    for i in range(self.nl):\n        x[i] = self.m[i](x[i])  # conv\n        bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n        x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).co\n```\n\n网络的循环次数为3，也就是依次在这3个特征图上进行网格化预测，利用卷积操作得到通道数为no×nl的特征输出。拿128x80x80举例，在nc=15的情况下经过卷积得到60x80x80的特征图，这个特征图就是后续用于格点检测的特征图。\n\n```python\n        if not self.training:  # inference\n            if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n```\n\n```python\ndef _make_grid(self, nx=20, ny=20, i=0):\n    d = self.anchors[i].device\n    if check_version(torch.__version__, '1.10.0'):  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)], indexing='ij')\n    else:\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)])\n    grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n    anchor_grid = (self.anchors[i].clone() * self.stride[i]) \\\n        .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n    return grid, anchor_grid\n```\n\n随后就是基于经过检测器卷积后的特征图划分网格，网格的尺寸是与输入尺寸相同的，如20x20的特征图会变成20x20的网格，那么一个网格对应到原图中就是32x32像素；40x40的一个网格就会对应到原图的16x16像素，以此类推。\n\n```python\ny = x[i].sigmoid()\nif self.inplace:\ny[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\ny[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\nelse:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\nxy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\nwh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\ny = torch.cat((xy, wh, y[..., 4:]), -1)\nz.append(y.view(bs, -1, self.no))\n```\n\n这部分代码是预测偏移的主体部分。\n\n`y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy`\n\n这一句是对x和y进行预测。x、y在输入网络前都是已经归一好的(0,1)，乘以2再减去0.5就是(-0.5,1.5)，也就是让x、y的预测能够跨网格进行。后边的`self.grid[i]) * self.stride[i]`就是将相对位置转为网格中的绝对位置。\n\n`y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh`\n\n这句是对宽和高进行预测的。\n\n`z.append(y.view(bs, -1, self.no))`\n\n最后再将结果填入z。","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记7——v6.0源码剖析——Neck部分","url":"/2022/03/22/Yolov5学习笔记7/","content":"\n\n\n\n# Yolov5学习笔记7——v6.0源码剖析——Neck部分\n\n> 在网络结构配置文件yolov5s.yaml中，并未将neck和head区分开来，而是直接以head命名，这也是方便在model/yolo.py中的加载。Yolov5学习笔记7只讨论head中的neck部分。\n\n## neck结构概览及参数\n\n### neck部分结构图\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220325095329.png)\n\n### neck部分参数配置源码\n\n```python\n# YOLOv5 v6.0 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n  ]\n```\n\n可以看到，Neck部分的组件相较于Backbone较为单一，基本上就由CBS、Upsample、Concat和不带shortcut的CSP（C3)。\n\n### FPN和PAN\n\nNeck的网络结构设计也是沿用了**FPN+PAN的结构**。FPN就是使用一种 自顶向下的侧边连接在所有尺度上构建出高级语义特征图，构造了特征金字塔的经典结构；PAN的结构也不稀奇，FPN中间经过多层的网络后，底层的目标信息已经非常模糊了，因此PAN又加入了自底向上的路线，弥补并加强了定位信息。\n\n## Neck部分各模块\n\n### CBS模块\n\n在Backbone中，为了进一步提取图像中的信息，CBS在改变特征图通道的同时，也会控制卷积模块中的步长s下采样来改变特征图的尺寸。Neck中左侧采用FPN自顶向下设计的过程中，是特征图上采样的过程，因此这个时候再下采样就不合时宜了，所以在FPN中s=1；而到了右侧PAN再次自下而上提取位置信息时，就需要使用CBS继续下采样抽取高层次的语义信息，这也是CBS前后参数差异的原因。\n\n### nn.Upsample\n\n```python\n[-1, 1, nn.Upsample, [None, 2, 'nearest']],\n```\n\n使用的是Pytroch内置的上采样模块，需要指定上采样的倍数和方式。\n\n这里我们不指定size，上采样倍数为2，上采样方式为nearest，也就是最近填充。\n\n### Concat\n\n```python\n[[-1, 6], 1, Concat, [1]],  # cat backbone P4\n```\n\nConcat即拼接，接对象通过from传入，拼接的维度由args参数指定，此处即按照维度1(channel)拼接，其他维度不变。\n\n### CSP/C3\n\nBackbone需要更深层次的网络获取更多的信息，可以说backbone已经完成了主要特征信息的提取，所以在Neck阶段我们并不需要再一味地加深网络，采取不带残差的C3模块可能会更合适一些。","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记6——v6.0源码剖析——Backbone部分3","url":"/2022/03/21/Yolov5学习笔记6/","content":"\n\n# Yolov5学习笔记6——v6.0源码剖析——Backbone部分3\n\n## Backbone概览及参数\n\n### 源码如下\n\n```Python\n# Parameters\nnc: 80  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\nanchors:\n  - [10,13, 16,30, 33,23]  # P3/8\n  - [30,61, 62,45, 59,119]  # P4/16\n  - [116,90, 156,198, 373,326]  # P5/32\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n```\n\n> yolov5s的backbone部分如上，其网络结构使用yaml文件配置，通过./models/yolo.py解析文件加了一个输入构成的网络模块。与v3和v4所使用的config设置的网络不同，yaml文件中的网络组件不需要进行叠加，只需要在配置文件中设置number即可。\n\n#### Parameters\n\n```python\n# Parameters\nnc: 80  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\n```\n\n- **nc: 80**\n  代表数据集中的类别数目，例如MNIST中含有0-9共10个类.\n\n- **depth_multiple: 0.33**\n  用来控制模型的深度，仅在number≠1时启用。 如第一个C3层（c3具体是什么后续介绍）的参数设置为[-1, 3, C3, [128]]，其中number=3，表示在v5s中含有1个C3（3*0.33）；同理，v5l中的C3个数就是3（v5l的depth_multiple参数为1）。\n\n- **width_multiple: 0.50**\n  用来控制模型的宽度，主要作用于args中的ch_out。如第一个Conv层，ch_out=64，那么在v5s实际运算过程中，会将卷积过程中的卷积核设为64x0.5，所以会输出32通道的特征图。\n\n#### backbone\n\n```python\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n```\n\n- **from**：-n表示从前n层获得的输入，如-1表示从前一层获得输入\n- **number**：表示网络模块的数目，如[-1, 2, C3, [128] ]表示含有3个C3模块\n- **model**：表示网络模块的名称，具体细节可以在/models/common.py中查看，如Conv、C3、SPPF都是已经在common中定义好的模块。\n- **args**：表示向不同模块内传递的参数，即[ch_out, kernel, stride, padding, groups]，这里连ch_in都省去了，因为输入都是上层的输出（初始ch_in为3）。为了修改过于麻烦，这里输入的获取是从./models/yolo.py的`def parse_model(md, ch)`函数中解析得到的。\n\n#### 示例\n\n```python\n[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n```\n\n- input为：3×640×640\n\n- [ch_out，kernel，stride，padding]=[64, 6, 2, 2]\n\n  > 故新的通道数为64×0.5=32\n  >\n  > 根据特征图计算公式：Feature_new=(Feature_old-kernel+2xpadding)/stride+1可得：\n  >\n  > 新的特征图尺寸为：Feature_new=(640-6+2x2)/2+1=320\n\n```python\n[-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n```\n\n- input为：32×320×320\n\n- [ch_out, kernel, stride]=[128, 3, 2]\n\n  > 同理可得：新的通道数为64，新的特征图尺寸为160\n\n## Backbone组成\n\nv6.0版本的Backbone去除了Focus模块（便于模型导出部署），Backbone主要由CBL、BottleneckCSP/C3以及SPP/SPPF等组成，具体如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324165913.png)\n\n### CBS模块\n\n> CBS模块实际上就是Conv+BatchNorm+SiLU。\n\n- CBS模块框架图\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324170357.png)\n\n- CBS源码\n\n  ```python\n  class Conv(nn.Module):\n      # Standard convolution\n      def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n          super().__init__()\n          self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n          self.bn = nn.BatchNorm2d(c2)\n          self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n  \n      def forward(self, x):\n          return self.act(self.bn(self.conv(x)))\n  \n      def forward_fuse(self, x):\n          return self.act(self.conv(x))\n  ```\n\n这里配合CBS模块的源码，分析Conv()函数里的一些参数，作为pytorch中卷积操作的复习。\n\n从源码可以看出，Conv()包含7个参数，这些参数也是二维卷积Conv2d()中的重要参数。ch_in, ch_out, kernel, stride这4个参数前文已经提到过，是用来计算特征图尺寸的。主要分析后三个参数。\n\n#### padding\n\n> 从目前主流卷积操作来看，大多数的研究者不会通过kernel来改变特征图的尺寸，如googlenet中3x3的kernel设定了padding=1，所以当kernel≠1时需要对输入特征图进行填充。当指定p值时按照p值进行填充，当p值为默认时则通过autopad函数进行填充：\n\n```python\ndef autopad(k, p=None):  # kernel, padding\n    # Pad to 'same'\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n    return p\n```\n\n这里作者考虑到对不同的卷积操作使用不同大小的卷积核时padding也需要做出改变，所以这里在为p赋值时会首先检查k是否为int，如果k为列表则对列表中的每个元素整除。\n\n#### groups\n\n表示分组卷积，示意图如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324171353.png)\n\n> groups – 从输入通道到输出的阻塞连接数\n>\n> - groups=1 时，所有输入都卷积到所有输出。\n> - groups=2 时，该操作等效于并排具有两个凸层，每个凸层看到一半的输入通道，并产生一半的输出通道，随后两者都串联起来。\n> - groups= in_channels 时，每个输入通道都用自己的一组滤波器进行卷积，其大小为：⌊（out_channels）/（in_channels）⌋。\n\n#### act参数\n\n决定是否对特征图进行激活操作，SILU表示使用Sigmoid进行激活。\n\n*关于激活函数的内容在Yolov5学习笔记3中有提及*\n\n#### 补充：dilation参数\n\n在Conv2d()中有一个重要的参数——空洞卷积dilation\n\n```python\ndilation: _size_2_t = 1,\n```\n\n通俗解释就是控制kernel点（卷积核点）间距的参数，通过改变卷积核间距实现特征图及特征信息的保留，在语义分割任务中空洞卷积比较有效。\n\n### CSP/C3\n\n**注**：CSP即backbone中的C3，因为在backbone中C3存在shortcut，而在neck中C3不使用shortcut，所以backbone中的C3层使用CSP1_x表示，neck中的C3使用CSP2_x表示。\n\n#### CSP结构\n\n- 源码：\n\n  ```python\n  class C3(nn.Module):\n      # CSP Bottleneck with 3 convolutions\n      def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n          super().__init__()\n          c_ = int(c2 * e)  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = Conv(c1, c_, 1, 1)\n          self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n          self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n          # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])\n  \n      def forward(self, x):\n          return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n  ```\n\n> 从源码中可以看出：输入特征图一条分支先经过.cv1，再经过.m，得到子特征图1；另一分支经过.cv2后得到子特征图2。最后将子特征图1和子特征图2拼接后输入.cv3得到C3层的输出，如下图所示。\n>\n> **这里的CV就是前面的Conv2d+BN+SiLU**\n\n- 结构图\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324190458.png)\n\n> .m操作是用nn.Sequential将多个Bottleneck(也就是上图中的Res_u)串接到网络中，\n\n#### Bottleneck\n\n在Resnet出现之前，人们的普遍为网络越深获取信息也越多，模型泛化效果越好。然而随后大量的研究表明，网络深度到达一定的程度后，模型的准确率反而大大降低。这并不是过拟合造成的，而是由于反向传播过程中的梯度爆炸和梯度消失。也就是说，**网络越深，模型越难优化，而不是学习不到更多的特征。**\n\n为了能让深层次的网络模型达到更好的训练效果，残差网络中提出的残差映射替换了以往的基础映射。对于输入x，期望输出H(x)，网络利用恒等映射将x作为初始结果，将原来的映射关系变成F(x)+x。与其让多层卷积去近似估计H(x) ，不如近似估计H(x)-x，即近似估计残差F(x)。因此，ResNet相当于将学习目标改变为目标值H(x)和x的差值，后面的训练目标就是要将残差结果逼近于0。\n\n残差函数有什么好处呢？\n\n> - **梯度弥散方面。**加入ResNet中的shortcut结构之后，在反传时，每两个block之间不仅传递了梯度，还加上了求导之前的梯度，这相当于把每一个block中向前传递的梯度人为加大了，也就会减小梯度弥散的可能性。\n> - **特征冗余方面。**正向卷积时，对每一层做卷积其实只提取了图像的一部分信息，这样一来，越到深层，原始图像信息的丢失越严重，而仅仅是对原始图像中的一小部分特征做提取。这显然会发生类似欠拟合的现象。加入shortcut结构，相当于在每个block中又加入了上一层图像的全部信息，一定程度上保留了更多的原始信息。\n\n**在resnet中，人们可以使用带有shortcut的残差模块搭建几百层甚至上千层的网络，而浅层的残差模块被命名为Basicblock（18、34），深层网络所使用的的残差模块，就被命名为了Bottleneck（50+）。**\n\nBottleneck与Basicblock最大的区别是卷积核的组成。 Basicblock由两个3x3的卷积层组成，Bottleneck由两个1x1卷积层夹一个3x3卷积层组成：其中1x1卷积层降维后再恢复维数，让3x3卷积在计算过程中的参数量更少、速度更快。\n第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。\n**Bottleneck减少了参数量，优化了计算，保持了原有的精度。**\n\n- Bottleneck的源码如下：\n\n```python\nclass Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n```\n\n- 结构图如下:\n\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324191951.png)\n\n可以看到，CSP中的Bottleneck同resnet模块中的类似，先是1x1的卷积层（CBS)，然后再是3x3的卷积层，最后通过shortcut与初始输入相加。但是这里与resnet的不通点在于：CSP将输入维度减半运算后并未再使用1x1卷积核进行升维，而是将原始输入x也降了维，采取concat的方法进行张量的拼接，得到与原始输入相同维度的输出。其实这里能区分一点就够了：**resnet中的shortcut通过add实现，是特征图对应位置相加而通道数不变；而CSP中的shortcut通过concat实现，是通道数的增加。二者虽然都是信息融合的主要方式，但是对张量的具体操作又不相同.**\n\n### SSPF模块\n\n- 源码\n\n```python\nclass SPPF(nn.Module):\n    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher\n    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n\n    def forward(self, x):\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning\n            y1 = self.m(x)\n            y2 = self.m(y1)\n            return self.cv2(torch.cat([x, y1, y2, self.m(y2)], 1))\n```\n\n- 结构图\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324194944.png)\n\nSSPF模块将经过<u>CBS的x</u>、<u>一次池化后的y1</u>、<u>两次池化后的y2和3次池化后的self.m(y2)</u>先进行拼接，然后再<u>CBS提取特征</u>。 仔细观察不难发现，虽然SSPF对特征图进行了多次池化，但是**特征图尺寸并未发生变化**，通道数更不会变化，所以后续的4个输出能够在channel维度进行融合。这一模块的主要作用是**对高层特征进行提取并融合，在融合的过程中作者多次运用最大池化，尽可能多的去提取高层次的语义特征。**\n\n## Yolov5s的Backbone总览\n\n运行yolo.py，结合上述的分析，对输出的结果应该很容易理解了。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324195512.png)","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"基于DNN神经网络的手写数字识别","url":"/2022/03/21/Depp_Learning实战训练1/","content":"\n# 基于DNN神经网络的手写数字识别\n\n**导入相关的库**\n\n```python\nimport numpy as np\nimport paddle as paddle\nimport paddle.fluid as fluid\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\npaddle.enable_static()\n```\n\n**读取数据**\n\n```python\n# 使用多层感知器训练（DNN）模型，用于预测手写数字图片。\n\nBUF_SIZE=512\nBATCH_SIZE=128\n\n#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\ntrain_reader = paddle.batch(\n    paddle.reader.shuffle(paddle.dataset.mnist.train(),\n                          buf_size=BUF_SIZE),\n    batch_size=BATCH_SIZE)\n\n#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\ntest_reader = paddle.batch(\n    paddle.reader.shuffle(paddle.dataset.mnist.test(),\n                          buf_size=BUF_SIZE),\n    batch_size=BATCH_SIZE)\n\n#用于打印，查看mnist数据\ntrain_data=paddle.dataset.mnist.train();\nsampledata=next(train_data())\n# print(sampledata)\n```\n\n定义一个多层感知器**\n\n```python\ndef multilayer_perceptron(input):\n    # 第一个全连接层，激活函数为ReLU\n    hidden1 = fluid.layers.fc(input=input, size=100, act='relu')\n    # 第二个全连接层，激活函数为ReLU\n    hidden2 = fluid.layers.fc(input=hidden1, size=100, act='relu')\n    # 以softmax为激活函数的全连接输出层，输出层的大小必须为数字的个数10\n    prediction = fluid.layers.fc(input=hidden2, size=10, act='softmax')\n    return prediction\n```\n\n**定义图像和标签数据**\n\n```python\n# 输入的原始图像数据，大小为1*28*28\nimage = fluid.layers.data(name='image', shape=[1, 28, 28], dtype='float32')#单通道，28*28像素值\n# 标签，名称为label,对应输入图片的类别标签\nlabel = fluid.layers.data(name='label', shape=[1], dtype='int64')          #图片标签\n```\n\n**获取分类器**\n\n```python\npredict = multilayer_perceptron(image)\n```\n\n**损失函数**\n\n```python\n# 使用交叉熵损失函数,描述真实样本标签和预测概率之间的差值\ncost = fluid.layers.cross_entropy(input=predict, label=label)\n# 使用类交叉熵函数计算predict和label之间的损失函数\navg_cost = fluid.layers.mean(cost)\n# 计算分类准确率\nacc = fluid.layers.accuracy(input=predict, label=label)\n```\n\n**测试**\n\n```python\n# 获取测试程序\ntest_program = fluid.default_main_program().clone(for_test=True)\n#使用Adam算法进行优化, learning_rate 是学习率(它的大小与网络的训练收敛速度有关系)\noptimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.001)\nopts = optimizer.minimize(avg_cost)\n# （1）创建训练的Executor\n# 首先定义运算场所 fluid.CPUPlace()和 fluid.CUDAPlace(0)分别表示运算场所为CPU和GPU\n# Executor:接收传入的program，通过run()方法运行program。\n# 定义使用CPU还是GPU，使用CPU时use_cuda = False,使用GPU时use_cuda = True\nuse_cuda = False\nplace = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\nexe = fluid.Executor(place)\nexe.run(fluid.default_startup_program())\n#（2）告知网络传入的数据分为两部分，第一部分是image值，第二部分是label值\n# DataFeeder负责将数据提供器（train_reader,test_reader）返回的数据转成一种特殊的数据结构，使其可以输入到Executor中。\nfeeder = fluid.DataFeeder(place=place, feed_list=[image, label])\n\nall_train_iter=0\nall_train_iters=[]\nall_train_costs=[]\nall_train_accs=[]\n```\n\n可视化与模型训练\n\n```python\ndef draw_train_process(title,iters,costs,accs,label_cost,lable_acc):\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"iter\", fontsize=20)\n    plt.ylabel(\"cost/acc\", fontsize=20)\n    plt.plot(iters, costs,color='red',label=label_cost)\n    plt.plot(iters, accs,color='green',label=lable_acc)\n    plt.legend()\n    plt.grid()\n    plt.show()\n\nEPOCH_NUM = 2\nmodel_save_dir = \"CH4_File/model\"\nfor pass_id in range(EPOCH_NUM):\n    # 进行训练\n    for batch_id, data in enumerate(train_reader()):  # 遍历train_reader\n        train_cost, train_acc = exe.run(program=fluid.default_main_program(),  # 运行主程序\n                                        feed=feeder.feed(data),  # 给模型喂入数据\n                                        fetch_list=[avg_cost, acc])  # fetch 误差、准确率\n\n        all_train_iter = all_train_iter + BATCH_SIZE\n        all_train_iters.append(all_train_iter)\n\n        all_train_costs.append(train_cost[0])\n        all_train_accs.append(train_acc[0])\n\n        # 每200个batch打印一次信息  误差、准确率\n        if batch_id % 200 == 0:\n            print('Pass:%d, Batch:%d, Cost:%0.5f, Accuracy:%0.5f' %\n                  (pass_id, batch_id, train_cost[0], train_acc[0]))\n\n    # 进行测试\n    test_accs = []\n    test_costs = []\n    # 每训练一轮 进行一次测试\n    for batch_id, data in enumerate(test_reader()):  # 遍历test_reader\n        test_cost, test_acc = exe.run(program=test_program,  # 执行训练程序\n                                      feed=feeder.feed(data),  # 喂入数据\n                                      fetch_list=[avg_cost, acc])  # fetch 误差、准确率\n        test_accs.append(test_acc[0])  # 每个batch的准确率\n        test_costs.append(test_cost[0])  # 每个batch的误差\n\n    # 求测试结果的平均值\n    test_cost = (sum(test_costs) / len(test_costs))  # 每轮的平均误差\n    test_acc = (sum(test_accs) / len(test_accs))  # 每轮的平均准确率\n    print('Test:%d, Cost:%0.5f, Accuracy:%0.5f' % (pass_id, test_cost, test_acc))\n\n    # 保存模型\n    # 如果保存路径不存在就创建\nif not os.path.exists(model_save_dir):\n    os.makedirs(model_save_dir)\nprint('save models to %s' % (model_save_dir))\nfluid.io.save_inference_model(model_save_dir,   # 保存推理model的路径\n                              ['image'],        # 推理（inference）需要 feed 的数据\n                              [predict],        # 保存推理（inference）结果的 Variables\n                              exe)              # executor 保存 inference model\n\nprint('训练模型保存完成！')\ndraw_train_process(\"training\", all_train_iters, all_train_costs, all_train_accs, \"trainning cost\", \"trainning acc\")\n\ndef load_image(file):\n    im = Image.open(file).convert('L')                          # 将RGB转化为灰度图像，L代表灰度图像，像素值在0~255之间\n    im = im.resize((28, 28), Image.ANTIALIAS)                   # resize image with high-quality 图像大小为28*28\n    im = np.array(im).reshape(1, 1, 28, 28).astype(np.float32)  # 返回新形状的数组,把它变成一个 numpy 数组以匹配数据馈送格式。\n    # print(im)\n    im = im / 255.0 * 2.0 - 1.0                                 # 归一化到【-1~1】之间\n    return im\n\ninfer_path='CH4_File/data/infer_3.png'\nimg = Image.open(infer_path)\nplt.imshow(img)   #根据数组绘制图像\nplt.show()        #显示图像\n```\n\n**预测**\n\n```python\n# 加载数据并开始预测\nwith fluid.scope_guard(inference_scope):\n    #获取训练好的模型\n    #从指定目录中加载 推理model(inference model)\n    [inference_program,                                             # 推理Program\n     feed_target_names,                                             # 是一个str列表，它包含需要在推理 Program 中提供数据的变量的名称。\n     fetch_targets] = fluid.io.load_inference_model(model_save_dir, # fetch_targets：是一个 Variable 列表，从中我们可以得到推断结果。model_save_dir：模型保存的路径\n                                                    infer_exe)      # infer_exe: 运行 inference model的 executor\n    img = load_image(infer_path)\n\n    results = infer_exe.run(program=inference_program,              # 运行推测程序\n                   feed={feed_target_names[0]: img},                # 喂入要预测的img\n                   fetch_list=fetch_targets)                        # 得到推测结果,\n    # 获取概率最大的label\n    lab = np.argsort(results)                                       # argsort函数返回的是result数组值从小到大的索引值\n    #print(lab)\n    print(\"该图片的预测结果的label为: %d\" % lab[0][0][-1])             # -1代表读取数组中倒数第一列\n```\n\n**结果**\n\n训练过程可视化：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/训练曲线.png)\n\n识别如下图片：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/识别的图片.png)\n\n预测的结果为：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319162335.png)\n\n","tags":["deeplearning","DNN","手写数字识别"],"categories":["深度学习"]},{"title":"Yolov5学习笔记5——v5.0源码剖析——Backbone部分2","url":"/2022/03/20/Yolov5学习笔记5/","content":"\n# Yolov5学习笔记5——v5.0源码剖析——Backbone部分2\n\n用**netron**得到yolov5s的框架结构图如下，可以非常直观的得到关于backbone部分的网络结构图。\n\n**注**：所使用的代码版本为 2020年11月24日发布的Yolov5-master。\n\n## YOLOv5s结构图如下所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/yolov5s_2.png)\n\n## Backbone结构图\n\n- backbone的意义是：在不同图像细粒度上聚合并形成图像特征的卷积神经网络；\n\n- backbone所需的主要模块在common.py里面可以找到。\n\n从整体结构中我们抽出backbone部分学习：\n\nbackbone的结构图如下：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220323212340.png)\n\nNetron打开yolov5s.pt导出的BackBone部分的框架图如下：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220323212829.png)\n\nbackbone对应的代码为：\n\n```python\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, BottleneckCSP, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 9, BottleneckCSP, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, BottleneckCSP, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\n   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n  ]\n```\n\n从上述代码中我们可以看到backbone由如下组成：\n\nBACKBONE =FOCUS(1个)+CONV (1个)+BCSP(3个)+CONV (1个)+BCSP(9个)+CONV (1个)+SPP(1个)+BCSP(1个)\n\n- 注：这里的BCSP相当于CSP\n\n## Backbone源码解析\n\n### Focus模块\n\nFocus()函数在common.py中，对应的源码如下:\n\n```python\nclass Focus(nn.Module):\n    # Focus wh information into c-space\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super(Focus, self).__init__()\n        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\n\n    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n```\n\n根据前述代码我们知道，输入的图片尺寸为640×640×3，而Focus()函数的功能为：*将640 × 640 × 3的图像输入Focus结构，采用切片操作，先变成320 × 320 × 12的特征图，再经过3 × 3的卷积操作，输出通道32，最终变成320 × 320 × 32的特征图*\n\nFocus模块的结构图如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324110433.png)\n\n### CBL模块(CBH)\n\n该部分对应的源码如下：\n\n```python\nclass Conv(nn.Module):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super(Conv, self).__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.Hardswish() if act else nn.Identity()\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n    def fuseforward(self, x):\n        return self.act(self.conv(x))\n```\n\nCONV是一个标准的卷积模块。\n\n从源码第5 6 7行可以看出，激活函数变成了Hardwish()，实际上这里不应该叫CBL模块了，应该是CBH模块。\n\n- conv：来自于代码的torch.nn.Conv2d,是一个卷积操作\n- bn：来自于代码的torch.nn.BatchNorm2d：归一化处理，使batch里面的feature map 满足均值为1，方差为0 的正太分布\n- Hardswish：激活函数\n  故：CBL=CONV+BN+Hardswish\n\n### BottleneckCSP模块\n\n3个BCSP相当于是几个标准的Bottleneck的堆叠+几个标准卷积层。\n\n- BottleneckCSP的网络结构如下：\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324115258.png)\n\n- 残差组件Resunit的网络结构如下：\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324143720.png)\n\n- BottleneckCSP的源码如下：\n\n  ```python\n  class BottleneckCSP(nn.Module):\n      # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n      def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n          super(BottleneckCSP, self).__init__()\n          c_ = int(c2 * e)  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n          self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n          self.cv4 = Conv(2 * c_, c2, 1, 1)\n          self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n          self.act = nn.LeakyReLU(0.1, inplace=True)\n          self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n  \n      def forward(self, x):\n          y1 = self.cv3(self.m(self.cv1(x)))\n          y2 = self.cv2(x)\n          return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n  ```\n\n**注**：nn.sequential将所有的块链接在一起。self.bn = nn.BatchNorm2d(2 * c_)就是concat 块，cv2,cv3对应于图中的concat;\n\n- Resunit的源码如下：\n\n  ```python\n  class Bottleneck(nn.Module):\n      # Standard bottleneck\n      def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n          super(Bottleneck, self).__init__()\n          c_ = int(c2 * e)  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = Conv(c_, c2, 3, 1, g=g)\n          self.add = shortcut and c1 == c2\n  \n      def forward(self, x):\n          return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n  ```\n\n**注**：cv1、cv2对应于图中的CBL模块，add不变。\n\n### SPP某块\n\n> - SPP模块，就是常说的*空间金字塔池化模块*，分别采用5/9/13的最大池化，在进行concat融合，提高感受野。\n>\n> - SPP的输入时512×512×20，经过1×1的卷积层后输出256×20×20，然后经过并列的三个Maxpool进行下采样，将结果与其初始特征相加，输出1024×20×20，最后用512的卷积核将其恢复到512×20×20.\n\n- SPP模块的结构图如下：\n  ![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220324143720.png)\n\n- SPP模块对应的代码如下：\n\n  ```python\n  class SPP(nn.Module):\n      # Spatial pyramid pooling layer used in YOLOv3-SPP\n      def __init__(self, c1, c2, k=(5, 9, 13)):\n          super(SPP, self).__init__()\n          c_ = c1 // 2  # hidden channels\n          self.cv1 = Conv(c1, c_, 1, 1)\n          self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n          self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n  \n      def forward(self, x):\n          x = self.cv1(x)\n          return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n  ```\n\n  \n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Paper Reading 1|YOLO-Z","url":"/2022/03/20/文献阅读1/","content":"\n# 文献阅读1-YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles\n\n本文的中文题目为：YOLO-Z:基于改进YOLOv5的自动驾驶车辆小目标检测算法。\n\n## 概述\n\n本研究探索了如何对YOLOv5进行修改，以提高其在检测较小目标时的性能，并在自动赛车中进行了特殊应用。为了实现这一点，作者研究了替换模型的某些结构会如何影响性能和推理时间。在这一过程中在不同的尺度上提出一系列的模型YOLO-Z，并得到高达6.9%的改善，相比原YOLOv5推理时间检测更小的目标时的成本就增加3ms。\n\n## 数据集(Dataset)\n\n本文所采用的数据集为基于自动驾驶赛车视角的带注释的锥体数据集。该数据集包括数字增强图像和具有挑战性天气条件的情况。数据集共有4类圆锥体(黄色、蓝色、橙色和大橙色)，接近4000张图像。\n\n数据集以65:15:20的比例分为训练、验证和测试。\n\n## 架构改进\n\nYOLOv5使用yaml文件来指导解析器如何构建模型。为了实现新的YOLOv5模型结构，作者为原有的YOLOv5的每个构建模块或层提供参数，并在必要时指导解析器如何构建它。换句话说，作者利用YOLOv5提供的基础和实验网络块，同时在需要模拟所需结构的地方实现额外的块。\n\n### Backbone部分\n\n模型的Backbone是用于获取输入图像并从中提取特征映射的组件。这是任何目标检测器的关键步骤，因为它是负责从输入图像提取上下文信息以及将该信息提取为模式的主要结构。\n\n作者尝试用2个Backbone替换YOLOv5中现有的Backbone。\n\n- ResNet是一种流行的结构，它引入残差连接来减少在深层神经网络中收益递减的影响。\n- DenseNet使用类似的连接，在网络中尽可能多地保存信息。实现这些结构需要将它们分解为基本块，并确保各层适当的通信。这包括确保正确的特征图尺寸，这有时需要为模型的宽度和深度略微修改缩放因子。\n\n**改进部分如下**\n\n- 使用ResNet50\n- 按比例缩小DenseNet\n- YOLOv5利用了Backbone和Neck之间的空间金字塔池化(SPP)层，改进的代码中没有用到。\n\n**实验结果**：对于小目标 DenseNet性能更好，增加的推理时间也相对较少，ResNet性能较差。\n\n### Neck部分\n\nNeck部分的作用是将Backbone提取的信息反馈到Head之前尽可能多地聚合这些信息。该结构通过防止小目标信息丢失，在传递小目标信息方面发挥了重要作用。它通过再次提高特征图的分辨率来做到这一点，这样来自Backbone的不同层的特征就可以被聚合，以提升整体的检测性能。\n\n**改进部分**：\n\n当前的PAN-Net替换为bi-FPN。\n\n虽然都保留了类似的特征，但复杂性不同，因此实现所需的层数和连接数也不同。\n\n### 其他修改\n\n作者提到为了提高小目标检测性能，YOLOv5的改进除了输入图像的大小之外，还可以**修改模型的深度和宽度**(废话)，以改变处理的主要方向；**Neck和Head的层连接方式**也可以手动改变，以便专注于检测特定的特征图。\n\n作者探索了涉及高分辨率特征映射的重定向连接的效果，以便将它们直接反馈到Neck和Head，做了如下修改：\n\n- **扩大Neck以适应额外的特征图**来实现\n- 通过**替换最低分辨率的特征图以适应新的特征图**来实现\n\n用这两种方式在Neck中整合。\n\n## 实验结果\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220321112728.png)\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220321113416.png)\n\n\n\n### Backbone改进的影响\n\nDenseNet在推理时间(约3ms)相对较低的固定增加时，始终显示出显著的改进。作者的结论是：一般来说，DenseNet是一个更适合于小尺度目标检测。在较小比例的模型中，这可能是因为没有足够深的网络来获得ResNet骨干网的好处，而DenseNet在保存特征地图的细节方面做得很好。\n\n### Neck结构改进的影响\n\n使用FPN只在S尺度上优于双FPN。\n\n### Feature maps\n\n作者的实验表明重定向向颈部和头部提供的特征映射具有最显著的影响。在头部包含了更高分辨率的地图后，小对象最终占据了更多的像素，因此具有更大的影响力，而不是在脊柱的卷积阶段“丢失”。\n\n但对于超大规模的(即yolov5x)，在这种情况下，改进并不显著，保持较低分辨率的特征图实际上似乎对性能有害。\n\n### Influence of the number of anchors(锚点数)\n\n作者的实验表明：让YOLO根据所提供的数据集生成锚被证明在性能上是有效的，而且不会影响推理时间。在S模型下，3个锚点的表现优于5个锚点的表现，而在M模型下，差距减小。另一方面，L模型和X模型在5个锚点显示出更好的性能。\n\n**结论**：更复杂或更深入的模型可能确实受益于额外的锚点，或者换句话说，可能更有能力利用额外锚点提供的细节。\n\n### 其他因素\n\n- 更大的学习率被证明可以更好地利用模型\n- 较宽的模型(较高的宽度乘法器)对较小的尺度显示出积极的影响，而对较深的尺度(图6中既深又宽)则相反。\n- 类型的改变对推理速度有明显的负面影响，阻碍了它们的使用。\n\n### 改进模型\n\n作者通过使用上述修改方法的各种组合进行了其他测试，以寻找进一步偏离原始但同时又能进一步提高性能的模型——即YOLO-z，结论如下：\n\n- Neck的**FPN**结构往往优于双**FPN**\n- X量表，它似乎从这些变化中获益较少，即使使用不同的颈部结构，也不会像其他量表那样带来显著的改善。\n- 对于所有对象，在50% IoU的绝对mAP上，YOLO-Z模型的性能平均提高了2.7，而对于所有尺度上相同IoU的小对象，性能的绝对提高了5.9。这是以平均增加2.6ms的推理时间为代价的。\n\n## 讨论与结论\n\n在对YOLOv5进行调整以更好地检测更小的目标的方法的实验中，本文能够识别体系结构修改，这种修改在性能上比原始的检测器有明显改善，而且成本相对较低，因为新的模型保持了实时推理速度。\n\n所应用的技术，即自动赛车技术，可以从这样的改进中获益良多。在这项工作中，不仅显著提高了Baseline模型的性能，而且还确定了一些特定的技术，这些技术可以应用于任何其他应用程序，包括检测小或远的物体。\n\n最终结果是YOLO-Z系列的模型优于的YOLOv5类，同时保留一个推理时间等实时应用程序兼容的自动化赛车(见表2和图7)。特别是较小的目标是本研究的重点(图7中，中间)，而对于中等大小的目标(下图)，性能是稳定的。\n\n# 参考资料\n\n1. [YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles](<https://arxiv.org/pdf/2112.11798.pdf>)\n2. [深度学习论文及PyTorch实现](<https://blog.csdn.net/shanglianlm/article/details/122301921>)\n\n","tags":["yolov5","目标检测"],"categories":["文献阅读"]},{"title":"Yolov5学习笔记4——源码剖析——Head部分","url":"/2022/03/19/Yolov5学习笔记4/","content":"\n# Yolov5学习笔记4——源码剖析——Head部分\n\nDetect类对应yolov5的检查头(head)部分\n\nDetect类在yolo.py程序中的33行。\n\n## class Detect()代码分析\n\n```python\nclass Detect(nn.Module):\n    stride = None  # strides computed during build\n    onnx_dynamic = False  # ONNX export parameter\n\n    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.no = nc + 5  # number of outputs per anchor\n        self.nl = len(anchors)  # number of detection layers\n        self.na = len(anchors[0]) // 2  # number of anchors\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\n        self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n        self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n\n    def forward(self, x):\n        z = []  # inference output\n        for i in range(self.nl):\n            x[i] = self.m[i](x[i])  # conv\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n            if not self.training:  # inference\n                if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n                y = x[i].sigmoid()\n                if self.inplace:\n                    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                    y = torch.cat((xy, wh, y[..., 4:]), -1)\n                z.append(y.view(bs, -1, self.no))\n\n        return x if self.training else (torch.cat(z, 1), x)\n```\n\n### 初始化函数init()\n\n首先分析这个类的初始化函数：\n\n```python\ndef __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # 检测头\n    super().__init__()\n    self.nc = nc  # 类别数\n    self.no = nc + 5  # 输出的锚点数量\n    self.nl = len(anchors)  # 检测的层数\n    self.na = len(anchors[0]) // 2  # 锚点的数量\n    self.grid = [torch.zeros(1)] * self.nl  # 初始化网格\n    self.anchor_grid = [torch.zeros(1)] * self.nl  # 初始化锚点框\n    self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)=shape(3,3,2)\n    self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # 输出卷积结果\n    self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n```\n\nyolov5的检测头仍为**FPN结构**，所以self.m为3个输出卷积。这三个输出卷积模块的channel变化分别为128$\\longrightarrow$255|256$\\longrightarrow$255|512$\\longrightarrow$255。\nself.no为每个anchor位置的输出channel维度，每个位置都预测80个类（coco）+ 4个位置坐标xywh + 1个confidence score。所以输出channel为85。每个尺度下有3个anchor位置，所以输出85*3=255个channel。检测层数为3，锚点数量为85\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319214026.png)\n\n### forward()函数\n\n接下来看head部分的forward()函数：\n\n```python\ndef forward(self, x):\n    z = []  # inference output\n    for i in range(self.nl):\n        x[i] = self.m[i](x[i])  # conv\n        bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n        x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n        if not self.training:  # inference\n            if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n                self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n            y = x[i].sigmoid()\n            if self.inplace:\n                y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n            else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n                xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n                wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n                y = torch.cat((xy, wh, y[..., 4:]), -1)\n            z.append(y.view(bs, -1, self.no))\n\n    return x if self.training else (torch.cat(z, 1), x)\n```\n\nx是一个列表的形式，分别对应着3个head的输入。它们的shape分别为：\n\n- [bs, 128, 32, 32]\n- [1, 256, 16, 16]\n- [1, 512, 8, 8]\n\n三个输入先后被送入了3个卷积，得到输出结果。\n\n```python\nx[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n```\n\n这里将x进行变换从：\n\nx[0]：(bs,255,32,32) => x(bs,3,32,32,85)\nx[1]：(bs,255,32,32) => x(bs,3,16,16,85)\nx[2]：(bs,255,32,32) => x(bs,3,8,8,85)\n\n### make_grid()函数\n\n```python\ndef _make_grid(self, nx=20, ny=20, i=0):\n    d = self.anchors[i].device\n    if check_version(torch.__version__, '1.10.0'):  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)], indexing='ij')\n    else:\n        yv, xv = torch.meshgrid([torch.arange(ny, device=d), torch.arange(nx, device=d)])\n    grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n    anchor_grid = (self.anchors[i].clone() * self.stride[i]) \\\n        .view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n    return grid, anchor_grid\n```\n\n这里的_make_grid()函数是准备好格点。所有的预测的单位长度都是基于grid层面的而不是原图。注意每一层的grid的尺寸都是不一样的，和每一层输出的尺寸w,h是一样的。\n\n```python\ny = x[i].sigmoid()\nif self.inplace:\n    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\nelse:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n    y = torch.cat((xy, wh, y[..., 4:]), -1)\nz.append(y.view(bs, -1, self.no))\n```\n\n这里是inference的核心代码，对应的是yolov5的bbox回归机制。yolov5的回归机制如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/587c92d2ea4f4be386365fce177112f6.png)\n\n相较于yolov3的回归机制，可以明显的发现box center的x，y的预测被乘以2并减去了0.5，所以这里的值域从yolov3里的(0，1)注意是开区间，变成了(-0.5， 1.5)。从表面理解是yolov5可以跨半个格点预测了，这样可以提高对格点周围的bbox的召回。当然还有一个好处就是也解决了yolov3中因为sigmoid开区间而导致中心无法到达边界处的问题。\n\n同样，在w，h的回归上，yolov5也有了新的变化，同样对比yolov3的源代码：\n\n```python\nx = torch.sigmoid(prediction[..., 0])  # Center x  #B A H W\ny = torch.sigmoid(prediction[..., 1])  # Center y  #B A H W\nw = prediction[..., 2]  # Width                    #B A H W\nh = prediction[..., 3]  # Height                   #B A H W\npred_conf = torch.sigmoid(prediction[..., 4])  # Conf\npred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n```\n\n很明显yolov3对于w，h没有做sigmoid，而在yolov5中对于x，y，w，h都做了sigmoid。其次yolov5的预测缩放比例变成了：(2*w_pred/h_pred) ^2。\n值域从基于anchor宽高的（0，+∞）变成了（0，4）。这可能目的在于使预测的框范围更精准，通过sigmoid约束，让回归的框比例尺寸更为合理。\n\n## class Model()代码分析\n\n接下来分析Model类里面的函数。主要分析它的前向传播过程，这里有两个函数：forward()和forward_once()。\n\n### forward()函数\n\n```python\ndef forward(self, x, augment=False, profile=False, visualize=False):\n    if augment:\n        return self._forward_augment(x)  # augmented inference, None\n    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n\ndef _forward_augment(self, x):\n    img_size = x.shape[-2:]  # height, width\n    s = [1, 0.83, 0.67]  # scales\n    f = [None, 3, None]  # flips (2-ud, 3-lr)\n    y = []  # outputs\n    for si, fi in zip(s, f):\n        xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))\n        yi = self._forward_once(xi)[0]  # forward\n        # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\n        yi = self._descale_pred(yi, fi, si, img_size)\n        y.append(yi)\n    y = self._clip_augmented(y)  # clip augmented tails\n    return torch.cat(y, 1), None  # augmented inference, train\n```\n\nself.forward()函数里面augment可以理解为控制TTA，如果打开会对图片进行**scale**和**flip**。默认是关闭的。\n\nscale_img的源码如下：\n\n#### scale_img()函数\n\n```python\ndef scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)\n    # Scales img(bs,3,y,x) by ratio constrained to gs-multiple\n    if ratio == 1.0:\n        return img\n    else:\n        h, w = img.shape[2:]\n        s = (int(h * ratio), int(w * ratio))  # new size\n        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\n        if not same_shape:  # pad/crop img\n            h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))\n        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean\n```\n\n通过普通的双线性插值实现，根据ratio来控制图片的缩放比例，最后通过pad 0补齐到原图的尺寸。\n\n### forward_once()函数\n\n```python\ndef _forward_once(self, x, profile=False, visualize=False):\n    y, dt = [], []  # outputs\n    for m in self.model:\n        if m.f != -1:  # if not from previous layer\n            x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n        if profile:\n            self._profile_one_layer(m, x, dt)\n        x = m(x)  # run\n        y.append(x if m.i in self.save else None)  # save output\n        if visualize:\n            feature_visualization(x, m.type, m.i, save_dir=visualize)\n    return x\n```\n\nself.foward_once()就是前向执行一次model里的所有module，得到结果。profile参数打开会记录每个模块的平均执行时长和flops用于分析模型的瓶颈，提高模型的执行速度和降低显存占用。\n\n本文分析了yolov5head部分的前向传播和inference的源码。\n\n参考资料：\n\n1.  [yolov5深度剖析+源码debug级讲解系列（三）yolov5 head源码解析](<https://blog.csdn.net/weixin_36714575/article/details/114238645>)\n\n2.  [YOLO全系列更新,YOLO的进化历程](https://www.likecs.com/show-204961615.html)","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"Yolov5学习笔记3——源码剖析——Backbone部分1","url":"/2022/03/18/Yolov5学习笔记3/","content":"\n# Yolov5学习笔记3——源码剖析——Backbone部分1\n\n## yolo.py源码解析\n\n该代码路径为\"**models/yolo.py**\"\n\n```python\ndef __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\n    super().__init__()\n    if isinstance(cfg, dict):\n        self.yaml = cfg  # model dict\n    else:  # is *.yaml\n        import yaml  # for torch hub\n        self.yaml_file = Path(cfg).name\n        with open(cfg, encoding='ascii', errors='ignore') as f:\n            self.yaml = yaml.safe_load(f)  # model dict\n\n    # Define model\n    ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\n    if nc and nc != self.yaml['nc']:\n        LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n        self.yaml['nc'] = nc  # override yaml value\n    if anchors:\n        LOGGER.info(f'Overriding model.yaml anchors with anchors={anchors}')\n        self.yaml['anchors'] = round(anchors)  # override yaml value\n    self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\n    self.names = [str(i) for i in range(self.yaml['nc'])]  # default names\n    self.inplace = self.yaml.get('inplace', True)\n\n    # Build strides, anchors\n    m = self.model[-1]  # Detect()\n    if isinstance(m, Detect):\n        s = 256  # 2x min stride\n        m.inplace = self.inplace\n        m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n        m.anchors /= m.stride.view(-1, 1, 1)\n        check_anchor_order(m)\n        self.stride = m.stride\n        self._initialize_biases()  # only run once\n\n    # Init weights, biases\n    initialize_weights(self)\n    self.info()\n    LOGGER.info('')\n```\n\n首先需要解析yaml配置文件，以yolov5s.yaml进行debug，可以看到解析后是一个dict形式：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319201158.png)\n\n- nc代表类别数量\n- depth_multiple是控制模型深度的参数。\n- width_multiple是一个控制模型宽度的参数。\n- anchors是预置的锚框，FPN每层设置3个，共有3*3=9个。\n- backbone是backbone网络的构建参数，根据这个配置可以加载出backbone网络。\n- head是yolo head网络的构建参数，根据这个配置可以加载出yolo head的网络。（其实可以认为这部分是neck+head）\n\n```python\n# 定义模型\nch = self.yaml['ch'] = self.yaml.get('ch', ch)  # 输入通道\nif nc and nc != self.yaml['nc']:\n    LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n    self.yaml['nc'] = nc  # 覆盖yaml的值\n```\n\n这里判断一下输入的channel和配置文件里的是否一致，不一致则以输入参数为准。\n\n```python\nself.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # 模型, 保存列表\n```\n\n然后进入核心的parse_model()函数。\n\n```python\ndef parse_model(d, ch):  # model_dict, input_channels(3)\n    \n    LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")\n    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n# 这部分很简单，读出配置dict里面的参数，na是判断anchor的数量,no是根据anchor数量推断的输出维度，比如对于coco是255。输出维度=anchor数量*（类别数量+置信度+xywh四个回归坐标）。\n\n    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n        m = eval(m) if isinstance(m, str) else m  # eval strings\n        for j, a in enumerate(args):\n            try:\n                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n            except NameError:\n                pass           \n# 这里开始迭代循环backbone与head的配置。f，n，m，args分别代表着从哪层开始，模块的默认深度，模块的类型和模块的参数。\n\n        n = n_ = max(round(n * gd), 1) if n > 1 else n  # depth gain\n# 网络用n*gd控制模块的深度缩放，比如对于yolo5s来讲，gd为0.33，也就是把默认的深度缩放为原来的1/3。深度在这里指的是类似CSP这种模块的重复迭代次数。而宽度一般我们指的是特征图的channel。一般控制模型的缩放，我们就会控制深度、宽度和resolution（efficientnet的思路）。\n\n        if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv,\n                 BottleneckCSP, C3, C3TR, C3SPP, C3Ghost]:\n            c1, c2 = ch[f], args[0]\n            if c2 != no:  # if not output\n                c2 = make_divisible(c2 * gw, 8)\n\n            args = [c1, c2, *args[1:]]\n            if m in [BottleneckCSP, C3, C3TR, C3Ghost]:\n                args.insert(2, n)  # number of repeats\n                n = 1\n# 对于以上的这几种类型的模块，ch是一个用来保存之前所有的模块输出的channle，ch[-1]代表着上一个模块的输出通道。args[0]是默认的输出通道。\n        elif m is nn.BatchNorm2d:\n            args = [ch[f]]\n        elif m is Concat:\n            c2 = sum(ch[x] for x in f)\n        elif m is Detect:\n            args.append([ch[x] for x in f])\n            if isinstance(args[1], int):  # number of anchors\n                args[1] = [list(range(args[1] * 2))] * len(f)\n        elif m is Contract:\n            c2 = ch[f] * args[0] ** 2\n        elif m is Expand:\n            c2 = ch[f] // args[0] ** 2\n        else:\n            c2 = ch[f]\n\n        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n        t = str(m)[8:-2].replace('__main__.', '')  # module type\n        np = sum(x.numel() for x in m_.parameters())  # number params\n        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n        LOGGER.info(f'{i:>3}{str(f):>18}{n_:>3}{np:10.0f}  {t:<40}{str(args):<30}')  # print\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n        layers.append(m_)\n        if i == 0:\n            ch = []\n        ch.append(c2)\n    return nn.Sequential(*layers), sorted(save)\n```\n\n逐步分析这个函数：\n\n```python\nLOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")\nanchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\nna = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\nno = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n```\n\n这部分很简单，读出配置dict里面的参数，na是判断anchor的数量,no是根据anchor数量推断的输出维度，比如对于coco是255。输出维度=anchor数量*（类别数量+置信度+xywh四个回归坐标）。\n\n```python\nfor i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n    m = eval(m) if isinstance(m, str) else m  # eval strings\n    for j, a in enumerate(args):\n        try:\n            args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n        except NameError:\n            pass\n```\n\n这里开始迭代循环backbone与head的配置。f，n，m，args分别代表着从哪层开始，模块的默认深度，模块的类型和模块的参数。\n\n```python\nn = n_ = max(round(n * gd), 1) if n > 1 else n  # depth gain\n```\n\n网络用n*gd控制模块的深度缩放，比如对于yolo5s来讲，gd为0.33，也就是把默认的深度缩放为原来的1/3。深度在这里指的是类似CSP这种模块的重复迭代次数。而宽度一般我们指的是特征图的channel。一般控制模型的缩放，我们就会控制深度、宽度和resolution（efficientnet的思路）。\n\n```python\nif m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv,\n         BottleneckCSP, C3, C3TR, C3SPP, C3Ghost]:\n    c1, c2 = ch[f], args[0]\n    if c2 != no:  # if not output\n        c2 = make_divisible(c2 * gw, 8)\n```\n\n对于以上的这几种类型的模块，ch是一个用来保存之前所有的模块输出的channle，ch[-1]代表着上一个模块的输出通道。args[0]是默认的输出通道\n\n上述第五行make_divisible()函数的源代码如下：\n\n```python\ndef make_divisible(x, divisor):\n    # Returns nearest x divisible by divisor\n    if isinstance(divisor, torch.Tensor):\n        divisor = int(divisor.max())  # to int\n    return math.ceil(x / divisor) * divisor\n```\n\n这里配合make_divisible()函数，是为了放缩网络模块的宽度（既输出的通道数），比如对于第一个模块“Focus”，默认的输出通道是64，而yolov5s里的放缩系数是0.5，所以通过以上代码变换，最终的输出通道为32。make_divisible()函数保证了输出的通道是8的倍数。\n\n```python\n\t\t\targs = [c1, c2, *args[1:]]\n\t\t\tif m in [BottleneckCSP, C3, C3TR, C3Ghost]:\n    \t\t\targs.insert(2, n)  # number of repeats\n   \t\t\t\tn = 1\n```\n经过以上处理，args里面保存的前两个参数就是module的输入通道数、输出通道数。只有BottleneckCSP和C3这两种module会根据深度参数n被调整该模块的重复迭加次数。\n\n```python\nelif m is nn.BatchNorm2d:\n    args = [ch[f]]\nelif m is Concat:\n    c2 = sum(ch[x] for x in f)\nelif m is Detect:\n    args.append([ch[x] for x in f])\n    if isinstance(args[1], int):  # number of anchors\n        args[1] = [list(range(args[1] * 2))] * len(f)\nelif m is Contract:\n    c2 = ch[f] * args[0] ** 2\nelif m is Expand:\n    c2 = ch[f] // args[0] ** 2\nelse:\n    c2 = ch[f]\n```\n\n以上是其他几种类型的Module。\n如果是nn.BatchNorm2d则通道数保持不变。\n如果是Concat则f是所有需要拼接层的index，则输出通道c2是所有层的和。\n如果是Detect则对应检测头，这部分后面再详细讲。\nContract和Expand目前未在模型中使用。\n\n```python\nm_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n```\n\n这里把args里的参数用于构建了module m，然后模块的循环次数用参数n控制。整体都受到宽度缩放，C3模块受到深度缩放。\n\n```python\nt = str(m)[8:-2].replace('__main__.', '')  # module type\nnp = sum(x.numel() for x in m_.parameters())  # number params\nm_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\nLOGGER.info(f'{i:>3}{str(f):>18}{n_:>3}{np:10.0f}  {t:<40}{str(args):<30}')  # print\n```\n\n这里做了一些输出打印，可以看到每一层module构建的编号、参数量等情况，如下所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220319203847.png)\n\n```python\nsave.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n    layers.append(m_)\n    if i == 0:\n        ch = []\n    ch.append(c2)\nreturn nn.Sequential(*layers), sorted(save)\n```\n\n最后把构建的模块保存到layers里，把该层的输出通道数写入ch列表里。\n待全部循环结束后再构建成模型。至此模型就全部构建完毕了。\n\n再回到yolo.py里刚刚调用parse_model的位置继续学习init()函数的学习。\n\n```python\nm = self.model[-1]  # Detect()\nif isinstance(m, Detect):\n    s = 256  # 2x min stride\n    m.inplace = self.inplace\n    m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n    m.anchors /= m.stride.view(-1, 1, 1)\n    check_anchor_order(m)\n    self.stride = m.stride\n    self._initialize_biases()  # only run once\n\n# Init weights, biases\ninitialize_weights(self)\nself.info()\nLOGGER.info('')\n```\n\n这里通过调用一次forward()函数，输入了一个[1, 3, 256, 256]的tensor(ch=3)，然后得到FPN输出结果的维度。然后求出了下次采样的倍数stride：8，16，32。\n最后把anchor除以以上的数值，将anchor放缩到了3个不同的尺度上。anchor的最终shape是[3,3,2]。\n至此init()函数已经完整的过了一遍。\n\n## 其他Modules中的源码解析\n\n在网络构建的过程中涉及到了多种Modules，这些Modules默认在models文件夹下面的common.py文件里我们下面还过一下这些函数。\n\n### 普通卷积Conv\n\n```python\nclass Conv(nn.Module):\n    # Standard convolution\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n    def forward_fuse(self, x):\n        return self.act(self.conv(x))\n```\n\n普通的卷积，这里调用了autopad()函数计算了same-padding所需要的padding数量。\n默认的激活函数是SiLU()，即Sigmoid激活函数。各种激活函数见下图。\nSiLU函数形式：f(x)=x⋅σ(x)\n导函数形式： f'(x)=f(x)+σ(x)(1−f(x))\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1b4516bab0d8e3664e773d091d59ea32.png)\n\n**yolo5的作者使用了 Leaky ReLU 和 Sigmoid 激活函数。yolo5中中间/隐藏层使用了 Leaky ReLU 激活函数，最后的检测层使用了 Sigmoid 形激活函数。而YOLO V4使用Mish激活函数。**\n\n### BottleNeck结构\n\n```python\nclass Bottleneck(nn.Module):\n    # Standard bottleneck\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n```\n\n可以看出BottleNeck结构默认是先1x1卷积缩小channel为原来的1/2，再通过3x3卷积提取特征。如果输入通道c1和3x3卷积输出通道c2相等，则进行残差输出。shortcut参数控制是否进行残差连接。\n\n### BottleNeckCSP和C3\n\n```python\nclass BottleneckCSP(nn.Module):\n    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n        self.cv4 = Conv(2 * c_, c2, 1, 1)\n        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n        self.act = nn.SiLU()\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n\n    def forward(self, x):\n        y1 = self.cv3(self.m(self.cv1(x)))\n        y2 = self.cv2(x)\n        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n\n\nclass C3(nn.Module):\n    # CSP Bottleneck with 3 convolutions\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])\n\n    def forward(self, x):\n        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n```\n\n在common.py里实现了两种csp结构：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20210228161030675.png)\n\nBottleneckCSP就完全对应着上面的结构。但是作者在yoloV5 4.0的版本中将这部分结构改成了C3。C3的结构如下图：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20210228161503472.png)\n\n残差之后的Conv被去掉了，激活函数从上面的LeakyRelu变为了SiLU。\n\n### SPP\n\n```python\nclass SPP(nn.Module):\n    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729\n    def __init__(self, c1, c2, k=(5, 9, 13)):\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n\n    def forward(self, x):\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning\n            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n```\n\nSPP模块将输入通道减半，然后分别做kernel size为5，9，13的maxpooling，最后将结过拼接，包含原始输入的四组结果合并后通道应该是原来的2倍。\n\n### Focus\n\n```python\nclass Focus(nn.Module):\n    # Focus wh information into c-space\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n        super().__init__()\n        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\n        # self.contract = Contract(gain=2)\n\n    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n        # return self.conv(self.contract(x))\n```\n\n把feature map 切成四等分，然后叠加起来。最后的结果是通道数变为原来的四倍，resolution为原来的1/4（H，W分别减半）。最后通过一个卷积调整通道数为预先设置。\n\n\n\n参考文章：\n\n1. [yolov5深度剖析+源码debug级讲解系列（二）backbone构建](<https://blog.csdn.net/weixin_36714575/article/details/114211796?spm=1001.2014.3001.5501>)\n\n2. [yolo5的改进策略](<https://blog.csdn.net/l641208111/article/details/109286497>)\n\n3. [深入浅出Yolo系列之Yolov5核心基础知识完整讲解]([深入浅出Yolo系列之Yolov5核心基础知识完整讲解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/172121380))","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"实用软件、脚本和工具","url":"/2022/03/17/实用软件、插件和脚本/","content":"\n# 实用软件、脚本和工具。\n\n## Tampermonkey油猴插件\n\n油猴浏览器插件实际上是一个用户脚本管理器，主要依靠各大社区编写的扩展脚本（JavaScript代码）运行在浏览器上，来改变被访问网页的功能，提升我们的网页浏览体验 。\n\n### 插件的安装\n\n**方法一**\n\n①途径——浏览器上打开[谷歌网上应用店](https://chrome.google.com/webstore/search/tampermonkey?hl=zh-CN)搜索 ,然后点击**添加至Chrome**。(谷歌浏览器专用，一般需要科学上网。)\n\n②途径——或者直接进入[Tampermonkey官网](https://www.tampermonkey.net/)**下载**然后添加到浏览器中。\n\n![](F:\\Blog\\picture\\20220317232319.png)\n\n以上，**Tampermonkey Stable**为正式版，**Tampermonkey Beta**为测试版\n\n**方法二**\n\n由**方法一**可得知Tampermonkey是附属于google上的，考虑到文明上网的普及问题，这里我们也可以在其他渠道获取[Tampermonkey的crx文件](https://wmhl.lanzoui.com/ib8glab)，然后解压提取出来。\n\n然后进入浏览器**设置**→**扩展程序**，进入后再打开右上角的**开发者模式**并保持该窗口的开启。之后找到被解压后的`tampermonkey.crx`文件，将其拖动到**扩展程序**界面，释放并同意安装。\n\n![](F:\\Blog\\picture\\20220317233113.png)\n\n### **获取脚本的方式**\n\n我常用[GreasyFork](https://greasyfork.org/zh-CN/scripts/28497-%E7%BD%91%E9%A1%B5%E9%99%90%E5%88%B6%E8%A7%A3%E9%99%A4-%E6%94%B9)，用的人也多，最重要的是支持中文！想用什么脚本吗，或者脚本应用在什么网站都可以直接搜索。\n\n### 我的常用脚本。\n\n|                             脚本                             |                             备注                             |\n| :----------------------------------------------------------: | :----------------------------------------------------------: |\n| [网页限制解除(改)](https://greasyfork.org/zh-CN/scripts/28497-%E7%BD%91%E9%A1%B5%E9%99%90%E5%88%B6%E8%A7%A3%E9%99%A4-%E6%94%B9) | 通杀大部分网站,可以解除禁止复制、剪切、选择文本、右键菜单的限制。 |\n| [Github增强-高速下载](https://greasyfork.org/scripts/412245) | 高速下载 Git Clone/SSH、Release、Raw、Code(ZIP) 等文件、项目列表单文件快捷下载 (☁) |\n| [知网PDF下载助手](https://greasyfork.org/zh-CN/scripts/389343) | 添加知网文献PDF下载按钮，支持搜索列表、详情页，下载论文章节目录，批量下载文献，一键切换CAJ和PDF格式 |\n| [免登录去弹窗](https://greasyfork.org/zh-CN/scripts/428960)  | CSDN/知乎/哔哩哔哩/简书免登录去除弹窗广告+去除所有广告+界面优化 |\n| [蓝奏云网盘增强](https://greasyfork.org/zh-CN/scripts/419224) | 刷新不回根目录、后退返回上一级、右键文件显示菜单、点击直接下载文件、点击空白进入目录、自动显示更多文件、一键复制所有分享链接、自定义分享链接域名、自动打开/复制分享链接、带密码的分享链接自动输密码、拖入文件自动显示上传框、输入密码后回车确认 |\n|  [电子书下载1](https://greasyfork.org/zh-CN/scripts/438563)  |       直接下载全国图书馆参考咨询联盟为PDF，有目录书签        |\n|  [电子书下载2](https://greasyfork.org/zh-CN/scripts/432075)  | 查询全国图书馆参考咨询联盟、读秀、超星、龙岩是否有书互助,自动获取435w无重全文PDF,（全网独家）不需要某度会员高速下载PDF！另（独家）全程免注册、免脚本、手机即可搜,下,看，复制浏览器打开： http://172.247.14.184/ |\n|  [VIP视频网站](https://greasyfork.org/zh-CN/scripts/418804)  | 解锁B站大会员番剧、B站视频解析下载；全网VIP视频免费破解去广告；全网音乐直接下载；油管、Facebook等国外视频解析下载 |\n|                              ……                              |               有需求就进脚本网站找对应脚本即可               |\n\n## 一些其他插件\n\n### ublock origin插件\n\n[ublock origin](https://chrome.zzzmh.cn/info?token=cjpalhdlnbpafiamejdnhcphjbkeiagm)是一款高效的网络请求过滤工具，占用极低的内存和 CPU，占用极低的内存和CPU，和其他常见的过滤工具相比，它能够加载并执行上千条过滤规则。\n\n### IGG谷歌访问助手\n\n[IGG谷歌访问助手](https://chrome.zzzmh.cn/info?token=ncldcbhpeplkfijdhnoepdgdnmjkckij)免费为广大科研及医务工作者、高校学生提供谷歌学术文献、期刊等资料产品的查询与加速访问。\n您可以从一个位置搜索众多学科和资料来源：来自学术著作出版商、专业性社团、预印本、各大学及其他学术组织的经同行评论的文章、论文、图书、摘要和文章。可帮助您在整个学术领域中确定相关性最强的研究。\n\n### Abcd PDF\n\n[Abcd PDF](https://chrome.zzzmh.cn/info?token=mcgnagemenncafhpabimmooimpngdcpn)可以在线将PDF转换为 Word、Excel和PPT。可在线编辑Word和PDF。100%免费，提高您的生产力。\nAbcd PDF 扩展是 100% 免费的多合一 PDF 工具\n\n### Infinity新标签页\n\n[Infinity新标签页](https://chrome.zzzmh.cn/info?token=dbfmnekepjoapopniengjbcpnbljalfg)是一款基于html5的Chrome扩展程序，它重新定义了您的Chrome新标签页。相比Chrome自带的新标签页，您可以通过Infinity自定义添加自己喜爱的网站，我们重绘了上千图标，当然您也可以自定义这些网站的图标。除此之外，您还可以更新新标签页的背景图片，既可以使用您自己的图片，也可以使用自动更换图片。集成了天气，待办事项，笔记等功能，甚至还能显示你的Gmail邮件数量和通知。\n\n### 彩云小译\n\n[彩云小译](https://chrome.zzzmh.cn/info?token=jmpepeebcbihafjjadogphmbgiffiajh)双语对照网页翻译插件，针对浏览器开发的一款网页翻译工具，一键高效获取母语阅读体验。\n\n> 上文中提到的所有插件的某度链接如下：\n>\n> ​\t链接: https://pan.baidu.com/s/17yGQF_krywzgEFEli60zsg?pwd=wz83 \n>\n> ​\t提取码: wz83\n\n## 一些软件\n\n### Everything\n\n\"[Everything](https://www.voidtools.com/zh-cn/)\" 是 Windows 上一款搜索引擎，它能够基于文件名快速定文件和文件夹位置。\n不像 Windows 内置搜索，\"Everything\" 默认显示电脑上每个文件和文件夹 (就如其名 \"Everything\")。\n您在搜索框输入的关键词将会筛选显示的文件和文件夹。\n\n### 流量盘\n\n流量盘是一款高速下载某度云资源的软件。\n\n某度链接：\n\n​\t链接: https://pan.baidu.com/s/1Ukaw2MCfg7uJJ_t32fwOmA \n\n​\t提取码: ivky \n\n\n\n未完待续……","categories":["效率"]},{"title":"YOLOv5学习笔记2","url":"/2022/03/17/Yolov5学习笔记2/","content":"\n\n\n# Yolov5学习笔记2——代码框架\n\n打开Yolov5的代码，可以看到有许多文件夹和很多的子文件。本文主要弄清楚Yolov5的代码框架。\n\n## data文件夹\n\n该文件夹下主要存放项目运行所需要的数据，包括一些超参、默认输入的图片等。\n\n### hyps文件夹\n\n该文件夹下存放的都是训练参数\n\n1. `hyp.Objects365.yaml`——项目在进行Objects365训练的超参数。\n2. `hyp.scratch-high.yaml`——里面的参数为对COCO数据集从零开始进行高增强训练的超参数。\n3. `hyp.scratch-low.yaml`——对COCO数据集从零开始进行低增强训练的超参数。\n4. `hyp.scratch-med.yaml`——对COCO数据集从零开始进行中等增强训练的超参数。\n5. `hyp.VOC.yaml`——对VOC数据集进行训练时的超参数。\n\n一般这些参数都不需要改动，这是作者团队大量训练记录的最好的参数结果。\n\n### images文件夹\n\n该文件夹下存放了代码默认执行detect.py时检测的图片，执行后会在目录中新建一个**runs**文件夹并将检测的结果存放在/runs/detect文件夹下。\n\n### scripts文件夹\n\n脚本文件夹。提供了下载权重文件夹、COCO数据集、COCO128数据集的方法。直接执行该脚本文件就可以直接下载`yolov5x.pt`文件和训练的数据集。\n\n**注意**：如果想要运行脚本文件的话，首先要确定`PyCharm`有没有安装`PowerShell`平台，一般`PyCharm`会提示你安装的。\n\n当然我还是比较推荐直接从`github`官网中下载。而数据集当然是自制。\n\n### 其他.yaml文件\n\n这些文件主要是对一些数据集做一个补充说明。如存放的路径、训练的路径、测试的路径等，以及这些数据集有多少张图片，定义了多少个类别等。当然代码中也写了下载这些数据集的方法。\n\n1. `Argoverse.yaml`——Argo AI提供的Argoverse-HD数据集(环形前置中央摄像头)\n\n2. `coco.yaml`——由微软提供的COCO 2017数据集，网址为：http://cocodataset.org\n3. `coco128.yaml`——由作者所在公司Ultralytics提供的COCO128数据集(来自COCO train2017的前128张图片)，网址为：https://www.kaggle.com/ultralytics/coco128\n4. `GlobalWheat2020.yaml`——由萨斯喀彻温大学提供  全球小麦2020数据集网址为：http://www.global-wheat.com/\n5. `Objects365.yaml`——提供了365个检测对象的数据集，几乎涵盖了生活中的各种常见物体。\n6. `SKU-110K.yaml`——由Trax retail提供的SKU-110K零售项目数据集网址为：https://github.com/eg4000/SKU110K_CVPR19\n7. `VisDrone.yaml`——由天津大学提供的VisDrone2019-DET数据集(无人机拍摄的图片)https://github.com/VisDrone/VisDrone-Dataset\n8. `VOC.yaml`——由牛津大学提供的PASCAL VOC数据集http://host.robots.ox.ac.uk/pascal/VOC\n9. `xView.yaml`——由美国国家地理空间情报局(NGA)提供的DIUx xView 2018挑战赛中的数据集。https://challenge.xviewdataset.org\n\n## model文件夹——模型文件\n\n在本文件夹中主要存放了各种Yolo算法的模型文件。在这些模型文件中定义了如下参数：\n\n|      参数      |         解释         |\n| :------------: | :------------------: |\n|       nc       | 训练和检测的类别数量 |\n| depth_multiple |       网络深度       |\n| width_multiple |       网络宽度       |\n|    anchors     |      锚点框参数      |\n|    backbone    |     骨干网络参数     |\n|      head      |        检测头        |\n\n**下面以yolov5s.yaml为例，对里面的相关参数进行详细解释。**\n\n###  .yaml介绍\n\n1. YAML(YAML Ain`t Markup language)文件， **它不是一个标记语言**。配置文件有xml、properties等，但 **YAML是以数据为中心 **，更适合做配置文件。\n2. YAML的语法和其他高级语言类似，并且可以 **简单表达清单、散列表，标量**等数据形态。\n3. 它使用 **空白符号缩进 **和大量依赖外观的特色，特别适合用来表达或编辑数据结构、各种配置文件、倾印调试内容、文件大纲。 [yaml介绍](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.runoob.com%2Fw3cnote%2Fyaml-intro.html)\n4. 大小写敏感；缩进不允许使用tab，只允许空格；缩进的空格数不重要，只要相同层级的元素左对齐即可；’#'表示注释；使用缩进表示层级关系。\n\n**注意**，在`yaml`文件中空格数其实也是重要的！在建立YAML 对象时，对象键值对使用冒号结构表示 `key: value`， **冒号后面要加一个空格**。\n\n### parameters\n\n```python\n# Parameters\nnc: 80  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\n```\n\n1. nc： **类别数**，你的类别有多少就填写多少。从1开始算起，不是0-14这样算。\n2. depth_multiple：控制 **模型的深度**。\n3. width_multiple：控制 **卷积核的个数**。\n\n> **depth_multiple **是用在 **backbone **中的 **number≠1的情况下， **即在Bottleneck层使用，控制模型的深度，yolov5s中设置为0.33，假设yolov5l中有三个Bottleneck，那yolov5s中就只有一个Bottleneck。\n> 因为一般 **number=1 **表示的是 **功能背景的层 **，比如说下采样Conv、Focus、SPP（空间金字塔池化）。\n> ——————————————————————————————————————\n> **width_multiple **主要是用于设置arguments，例如yolov5s设置为0.5，Focus就变成[32, 3]，Conv就变成[64, 3, 2]。\n> 以此类推，卷积核的个数都变成了设置的一半。\n\nyolov5提供了s、m、l、x四种，所有的`.yaml`文件都设置差不多，只有上面2和3的设置不同，作者团队很厉害，只需要修改这两个参数就可以调整模型的网络结构。\n\n### anchors\n\n```python\nanchors:\n  - [10,13, 16,30, 33,23]  # P3/8\n  - [30,61, 62,45, 59,119]  # P4/16\n  - [116,90, 156,198, 373,326]  # P5/32\n```\n\n首先，anchor box就是从训练集中真实框（ground truth）中统计或聚类得到的几个不同尺寸的框。避免模型在训练的时候盲目的找，有助于模型快速收敛。假设每个网格对应k个anchor，也就是模型在训练的时候，它只是会在每一个网格附近找出这k种形状，不会找其他的。\n\nanchor其实就是对预测的对象范围进行约束，并加入了尺寸先验经验，从而实现多尺度学习的目的。\n\n而对于yolov5l来说，输出为3个尺度的特征图，分别为13×13、26×26、52×52，对应着9个anchor，每个尺度均分3个anchor。\n\n> 最小的13×13的特征图上由于其感受野最大，应该使用大的anchor(116x90)，(156x198)，(373x326)，这几个坐标是针对原始输入的，即416×416的，因此要除以32把尺度缩放到13×3下使用，适合较大的目标检测。中等的26×26特征图上由于其具有中等感受野故应用中等的anchor box (30x61)，(62x45)，(59x119)，适合检测中等大小的目标。较大的33×23特征图上由于其具有较小的感受野故应用最小的anchor box(10x13)，(16x30)，(33x23)，适合检测较小的目标。具体使用就是每个grid cell都有3个anchor box。\n\n根据检测层来相应增加anchors。\n\n### backbone\n\n```python\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 9\n  ]\n```\n\n1. Bottleneck 可以译为“瓶颈层”。\n2. from列参数： **-1**代表是从上一层获得的输入 ， **-2**表示从上两层获得的输入 （head同理）。\n3. number列参数： 1表示只有一个，3表示有三个相同的模块。\n4. SPPF、Conv、Bottleneck、BottleneckCSP的代码可以在 `./models/common.py`中获取到。\n5. [64, 6, 2, 2]解析得到[3, 32, 3] ，输入为3（RGB），输出为32，卷积核k为3；<!--存疑，暂时没有太搞懂args里的参数-->\n6. [128, 3, 2]这是固定的，128表示输出128个卷积核个数。根据[128, 3, 2]解析得到[32, 64, 3, 2] ，32是输入，64是输出（128×0.5=64），3表示3×3的卷积核，2表示步长为2。\n7. 主干网是图片从大到小，深度不断加深。\n8. `args`这里的输入都省去了，因为输入都是上层的输出。为了修改过于麻烦，这里输入的获取是从./models/yolo.py的 `def parse_model(md, ch)`函数中解析得到的。\n\n### head\n\n**head检测头**：一般表示的是经过主干网后输出的特征图，特征图输入head中进行检测，包括类别和位置的检测。\n\n```python\n# YOLOv5 v6.0 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]\n```\n\n运行models文件夹下yolo.py文件，得到的解析图如下所示：\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/20220317165308.png)\n\n与上面的模型似乎不太一样。\n\n后面弄清楚了再来补充。\n\n### common.py\n\n该部分是backbone各个模块参数讲解。\n\n## utils文件夹——工具包\n\n---------------------------------------------------------------------------------------------------------------------\n\n这三个文件夹都用的很少，需要用到时再做了解。\n\n### aws文件夹\n\n里面是一些跟其他语言对接的文件\n\n### flask_rest_api\n\n存放了做后端API的一些例程代码和封装好的函数\n\n### loggers\n\n终端需要打印任务信息的接口函数。\n\n------------------------------------------------------------------------------\n\n### 工具包函数\n\n|      文件名      |            备注            |\n| :--------------: | :------------------------: |\n|  activations.py  |          激活函数          |\n| augmentations.py |        图片增强函数        |\n|  autoanchor.py   |      自动锚点工具函数      |\n|   autobatch.py   |      自动批量处理工具      |\n|  benchmarks.py   |             /              |\n|   callbacks.py   |          回调函数          |\n|   datasets.py    | 用于数据加载和数据集的工具 |\n|   downloads.py   |          下载工具          |\n|    general.py    |        通用工具函数        |\n|     loss.py      |      计算损失函数工具      |\n|    metrics.py    |        模型验证函数        |\n|     plots.py     |         可视化工具         |\n|  torch_utils.py  |     `PyTorch`相关工具      |\n\n\n\n## 主目录下其他.py代码\n\n### detect.py\n\n对图像、视频、路径、流媒体等进行推理检测。  \n\n### export.py\n\n导出`YOLOv5 PyTorch`模型到其他格式。如ONNX、OpenVINO、Core ML以及TensorFlow相关的格式。\n\n### train.py\n\n训练程序，在自定义数据集上训练YOLOv5模型。\n\n\n\n### val.py\n\n在自定义数据集上验证经过训练的YOLOv5模型的准确性 。\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"数学建模学习笔记1","url":"/2022/03/16/数学建模学习1/","content":"\n\n# 数学建模学习笔记1——AHP层次分析法\n\n## 介绍\n\n**作为建模比赛中最基础的模型之一，其主要用于解决评价类问题。(例如：选择哪种方案更好、哪种运动员或者员工表现的更优秀)**\n\nAHP的主要特点是通过建立递阶层次结构，把人类的判断转化到若干因素两两之间重要度的比较上，从而把难于量化的定性判断转化为可操作的重要度的比较上面。在许多情况下，决策者可以直接使用AHP进行决策，极大地提高了决策的有效性、可靠性和可行性，但其本质是一种思维方式，它把复杂问题分解成多个组成因素，又将这些因素按支配关系分别形成递阶层次结构，通过两两比较的方法确定决策阀杆相对重要度的总排序。整个过程体现了人类决策思维的基本特征，即分解、判断、综合。克服了其他方法回避决策者主观判断的缺点。 \n\n## 原理\n\n### 方法一：使用打分法解决评价问题\n\n需要完成权重表格：\n\n|       | 指标权重 | 方案1 | 方案2 | ……   |\n| ----- | -------- | ----- | ----- | ---- |\n| 指标1 |          |       |       |      |\n| 指标2 |          |       |       |      |\n| 指标3 |          |       |       |      |\n| ……    |          |       |       |      |\n\n同颜色的单元格的和为1，它们表示的针对某一因素所占的权重。\n\n#### 例题1：小明想去旅游，初步选择苏杭、北戴河和桂林三地之一作为目标景点。请你确定评价指标、形成评价体系来为小明选择最佳方案。\n\n**分析：**解决评价类问题，首先要想到一下三个问题：\n\n① 我们评价的目标是什么？\n\n② 为了达到这个目标有哪几种可选的方案？\n\n③ 评价的准则或指标是什么？(我们根据什么东西来评价好坏)\n\n一般而言前两个问题的答案是显而易见的，第三个问题的答案需要我们根据题目汇总的*背景材料、常识以及网上搜集到的参考资料*进行结合，从中筛选出最合适的指标。\n\n##### 一致矩阵\n\n$$\na_{ij}=\\frac{i的重要程度}{j的重要程度}\n$$\n\n$$\na_{jk}=\\frac{j的重要程度}{k的重要程度}\n$$\n\n$$\na_{ik}=\\frac{i的重要程度}{k的重要程度}=a_{ij} \\times a_{jk}\n$$\n\n##### 一致性检验\n\n**原理：**检验我们构造的判断矩阵和一致矩阵是否有太大的差别。\n\n##### 一致性检验的步骤\n\n第一步：计算一致性指标CI：\n\n$$\nCI = \\frac{\\lambda_{max}-n}{n-1}\n$$\n第二步：计算查找对应的平均随机一致性指标RI\n\n|  n   | 1    | 2    |  3   |  4   | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |\n| :--: | ---- | ---- | :--: | :--: | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n|  RI  | 0    | 0    | 0.52 | 0.89 | 1.12 | 1.26 | 1.36 | 1.41 | 1.46 | 1.49 | 1.52 | 1.54 | 1.56 | 1.58 | 1.59 |\n\n平均随机一致性RI的表格中n最多是15。\n\n第三步：计算一致性比例CR\n$$\nCR= \\frac{CI}{RI}\n$$\n若CR＜0.1，则可认为判断矩阵的一致性可以接受；否则需要对判断矩阵进行修正。\n\n第四步：计算各层元素度系统目标的合成权重，并进行排序。\n\n##### 层次分析法的一些局限性\n\n1. 评价的决策层不能太多，太多的话n会很大，判断矩阵和一致矩阵的差异也会很大。\n\n2. 如果决策层中指标的数据是已知的，那么我们如何利用这些来使得评价评价准确呢？\n\n   \n\n\n\n","tags":["matlab"],"categories":["数学建模"]},{"title":"IOU相关知识","url":"/2022/03/15/YOLOv5支线学习之IOU相关知识/","content":"\n# IOU相关知识\n\nIoU 作为目标检测算法性能 mAP 计算的一个非常重要的函数。\n\n## 1. 什么是IOU\n\nIoU 的全称为交并比（Intersection over Union），通过这个名称我们大概可以猜到 IoU 的计算方法。IoU 计算的是 “预测的边框” 和 “真实的边框” 的交集和并集的比值。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/IOU计算公式.png)\n\n一般约定，在计算机检测任务中，如果IoU≥0.5，就说检测正确。当然0.5只是约定阈值，你可以将IoU的阈值定的更高。IoU越高，边界框越精确。\n\n**举例如下：**绿色框是准确值，红色框是预测值。\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/1.jpg)\n\n## 2. 代码实现\n\n```python\ndef calculateIoU(candidateBound, groundTruthBound):\n    cx1 = candidateBound[0]\n    cy1 = candidateBound[1]\n    cx2 = candidateBound[2]\n    cy2 = candidateBound[3]\n \n    gx1 = groundTruthBound[0]\n    gy1 = groundTruthBound[1]\n    gx2 = groundTruthBound[2]\n    gy2 = groundTruthBound[3]\n \n    carea = (cx2 - cx1) * (cy2 - cy1) #C的面积\n    garea = (gx2 - gx1) * (gy2 - gy1) #G的面积\n \n    x1 = max(cx1, gx1)\n    y1 = max(cy1, gy1)\n    x2 = min(cx2, gx2)\n    y2 = min(cy2, gy2)\n    w = max(0, abs(x2 - x1))\n    h = max(0, abs(y2 - y1))\n    area = w * h #C∩G的面积\n \n    iou = area / (carea + garea - area)\n \n    return iou\n```\n\n## 3. 原理解析\n\n计算两个图片的交集，首先想到的是考虑两个图片边框的相对位置，然后按照它们的相对位置分情况讨论。\n\n相对位置无非以下几种：\n\n*左上   左下   右上   右下   包含   互不相交*\n\n如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/2.jpg)\n\n但在实际上这样写代码是做不到的。\n\n换个角度思考：两个框交集的计算的实质是两个集合交集的计算，因此我们可以将两个框的交集的计算简化为：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/3.jpg)\n\n通过简化，我们可以清晰地看到，交集计算的关键是交集上下界点（图中蓝点）的计算。\n\n我们假设集合 A 为 [x 1 x_{1}*x*1，x 2 x_{2}*x*2]，集合 B 为 [y 1 y_{1}*y*1，y 2 y_{2}*y*2]。然后我们来求AB交集的上下界限。\n\n交集计算的逻辑\n\n- 交集下界 z 1 z_{1}*z*1：max ( x 1 , y 1 ) \\text{max}(x_{1}, y_{1})max(*x*1,*y*1)\n- 交集上界 z 2 z_{2}*z*2：min ( x 2 , y 2 ) \\text{min}(x_{2}, y_{2})min(*x*2,*y*2)\n- 如果 z 2 − z 1 z_{2}-z_{1}*z*2−*z*1 小于0，则说明集合 A 和集合 B 没有交集。\n\n在YOLOv5的项目代码中，作者使用如下代码计算iou。\n\n代码路径为：/yolov5-master/build/lib.win-amd64-3.9/utils/metrics.py\n\n```python\ndef bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n    box2 = box2.T\n\n    # Get the coordinates of bounding boxes\n    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n    else:  # transform from xywh to xyxy\n        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n\n    # Intersection area\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n\n    # Union Area\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n    union = w1 * h1 + w2 * h2 - inter + eps\n\n    iou = inter / union\n    if CIoU or DIoU or GIoU:\n        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n                with torch.no_grad():\n                    alpha = v / (v - iou + (1 + eps))\n                return iou - (rho2 / c2 + v * alpha)  # CIoU\n            return iou - rho2 / c2  # DIoU\n        c_area = cw * ch + eps  # convex area\n        return iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n    return iou  # IoU\n```\n\n\n\n","categories":["计算机视觉"]},{"title":"YOLOv5学习笔记1","url":"/2022/03/15/Yolov5学习笔记1/","content":"\n# Yolov5学习笔记-预测部分\n\n## 一些使用tips：\n\n### detect.py运行指令——命令行方式\n\n```python\npython detect.py --source 0  # webcam\n                          img.jpg  # image\n                          vid.mp4  # video\n                          path/  # directory\n                          path/*.jpg  # glob  匹配该文件夹下的所有jpg图片\n                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n```\n\n格式如上，“--”后面为传入的参数，通过命令行的方式往执行的.py文件中传入参数。\n\n以detect.py为例，运行detect.py文件肯定是要有默认的输入参数的，在我所使用的YOLOv5代码下，以右键运行detect.py文件，默认检测的是目录\"\"/yolov5-master/data/images\"下的两张图片。\n\n通过上述代码的命令行指令，可以给detect.py的输入指定图片、视频或者某一个路径下的所有文件或某一个路径下的所有图片，当然还有视频或直接调用电脑或手机的摄像头实现实时检测。\n\n### 运行detect.py需要输入的参数\n\n示例代码：\n\n```python\npython detect.pu --source data/images --weights yolov5s.pt --conf 0.25\n```\n\n|       参数       |   解释   |\n| :--------------: | :------: |\n|      source      | 输入来源 |\n|     weights      |   权重   |\n| conf(confidence) |  失信度  |\n\n运行结果如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/detect文件运行截图.png)\n\n## detect.py相关代码学习\n\n### 参数&解释\n\n接上表继续学习该代码中的参数\n\n|           参数           |                 解释                 |\n| :----------------------: | :----------------------------------: |\n| --imgsz/--img/--img-size |            输入的图片尺寸            |\n|       --conf-thres       |                置信度                |\n|       --iou-thres        |                交并比                |\n|        --view-img        |             实时显示结果             |\n|        --save-txt        |             保存检测结果             |\n|       --save-conf        |              保存置信度              |\n|       --save-crop        |             保存预测结果             |\n|        --classes         |            可以检测的类别            |\n|      --agnostic-nms      |               数据增强               |\n|        --augment         |               增强检测               |\n|        --project         |            结果保存的位置            |\n|          --name          |            保存结果的名字            |\n|        --exist-ok        | 依然保存在默认文件夹，而不新增文件夹 |\n|                          |                                      |\n\n### 代码解析\n\n```python\nparser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n```\n\n虽然指定了图片尺寸，但比如输入1200*800的图片，输出依然为该尺寸，只是在检测过程中会对图片进行裁剪、分割。(代码后面会还原图片尺寸)。\n\n```python\nparser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n```\n\n默认值为0.25，表示只有当置信度大于0.25时，才会去标注它。值越小，检测标注的东西越多，但可信度就会降低，极容易出现误判。\n\n```python\nparser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n```\n\n有关IOU的知识在另一篇blog中有详细解释。**default=1表示框与框完全重合才能合并，结果中会有多个框出现。default=0表示只要框与框有交集部分就可以合并，故结果中没有重合的框。**\n\n设置默认值default=1，表示只有当检测框与标注框完全重合时才会合并，因此运行后检测的效果会看到许多框框，这种完全重合的情况是很难满足的。在不同框下，一个物体被多次重复检测。如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/3.png)\n\n当默认值default=0时，检测效果如下：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/IOU=0.png)\n\n```python\nparser.add_argument('--view-img', action='store_true', help='show results')\n```\n\n在运行detect.py时，若加上上行代码参数，则表示在运行此代码时会在检测的同时显示检测的效果。\n\n一般执行时是默认没有的，若需要显示，有两种方法：一种通过命令行，输入如下代码：\n\n```python\nconda activate pytorch\npython detect.py --view-img\n```\n\n我们不喜欢在命令行写指令，那么可以编辑该文件的运行配置，在[形参Parameters]位置上添加参数，如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/4.png)\n\n```python\nparser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n```\n\n保存检测的结果，内容为检测标注的值。\n\n```\nparser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n```\n\n例如设置classes=0并运行代码，则表示之间**人**这个类别。\n\n### 查看detect.py参数的默认值\n\n通过在调试可以看到所有参数的默认值\n\n![](https://cdn.jsdelivr.net/gh/PengJoy1106/peng_picgo/img/5.png)\n\n\n\n\n","tags":["python","计算机视觉","yolov5"],"categories":["计算机视觉"]},{"title":"博客编写发布指南","url":"/2022/03/14/博客编写发布指南/","content":"\n## 使用的软件\n### WebStorm\n用来管理博客相关源代码以及博客的部署和文章的提交\n### Typora\n用于博客写作\nTypora常用快捷键和用法：\n标题：Ctrl+1、2、3...对应一、二、三...级标题（光标定位到需要设置为标题的行，按快捷键）\n加粗：Ctrl+B（选中要加粗的文本，按快捷键）\n斜体：Ctrl+I（选中要设置斜体的文本，按快捷键）\n下划线：Ctrl+U（选中要加下划线的文本，按快捷键）\n删除线：Alt+Shift+5（选中要加删除线的文本，按快捷键）\n代码片段：Ctrl+Shift+`（选中要设置为代码片段的文本，按快捷键）\n代码块：Ctrl+Shift+K（任意位置按快捷键，选择编程语言然后在代码块中输入代码）\n切换到下一行：Ctrl+Enter（任意位置按快捷键，在代码块中可以跳出代码块另起一行）\n链接：Ctrl+K（先复制链接，然后选中要加链接的文本，按快捷键。Ctrl+左键点击文本可跳转到对应链接）\n取消格式：再次按相同的快捷键即可\n有序列表：数字+点+空格\n任务列表：加号或减号+空格 \n切换到列表下一行：Space+Enter\n嵌套列表：按Tab键\n退出列表：按 Shift+Tab\n插入表格：Ctrl+T\n引用：输入>后面加空格，或者Ctrl+Shift+Q\n\n## Hexo文章管理\n### 创建的命令\n\n``` -bash\n$ hexo new <title>\n$ hexo new \"我的第一篇文章\"\n```\n\n\n### 布局\n· 创建md文件时，我们可以指定布局\n``` -bash\n$ hexo new [layout] <title>\n$ hexo new page \"我的页面\"\n```\n· 布局有三种\n    ①post(文章)\n    ②draft(草稿)\n    ③page(页面)\n· 如果没有指定布局类型，则为默认布局post，可以在站点配置文件修改 default_layout 参数来修改默认布局。\n\n## 文章编写格式\n|         写法          |                             解释                             |\n| :-------------------: | :----------------------------------------------------------: |\n|         title         |                       【必需】文章标题                       |\n|         date          |                     【必需】文章创建日期                     |\n|        updated        |                     【可选】文章更新日期                     |\n|         tags          |                       【可选】文章标籤                       |\n|      categories       |                       【可选】文章分类                       |\n|       keywords        |                      【可选】文章关键字                      |\n|      description      |                       【可选】文章描述                       |\n|        top_img        |                     【可选】文章顶部图片                     |\n|         cover         | 【可选】文章缩略图 (如果没有设置 top_img, 文章页顶部将显示缩略图，可设为 false / 图片地址 / 留空) |\n|       comments        |             【可选】显示文章评论模块 (默认 true)             |\n|          toc          |    【可选】显示文章 TOC (默认为设置中 toc 的 enable 配置)    |\n|      toc_number       |  【可选】显示 toc_number (默认为设置中 toc 的 number 配置)   |\n|       copyright       | 【可选】显示文章版权模块 (默认为设置中 post_copyright 的 enable 配置) |\n|   copyright_author    |               【可选】文章版权模块的`文章作者`               |\n| copyright_author_href |             【可选】文章版权模块的`文章作者`链接             |\n|     copyright_url     |             【可选】文章版权模块的`文章连结`链接             |\n|    copyright_info     |             【可选】文章版权模块的`版权声明`文字             |\n|        mathjax        | 【可选】显示 mathjax (当设置 mathjax 的 per_page: false 时，才需要配置，默认 false) |\n|         katex         | 【可选】显示 katex (当设置 katex 的 per_page: false 时，才需要配置，默认 false) |\n|        aplayer        | 【可选】在需要的页面加载 aplayer 的 js 和 css, 请参考文章下面的`音乐` 配置 |\n|   highlight_shrink    | 【可选】配置代码框是否展开 (true/false)(默认为设置中 highlight_shrink 的配置) |\n|         aside         |                【可选】显示侧边栏 (默认 true)                |\n|         hide          |                       【可选】隐藏文章                       |\n|        sticky         |                【可选】文章置顶，值越大越靠上                |\n\n","categories":["生产力"]}]